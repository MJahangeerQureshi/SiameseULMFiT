{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ULMFiT + Siamese Network for Sentence Vectors\n",
    "## Part Two: Classifying\n",
    "\n",
    "The first notebook created a new language model from the SNLI dataset.\n",
    "This notebook will adapt that model to predicting the SNLI category for sentence pairs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text import *\n",
    "import html\n",
    "\n",
    "import json\n",
    "import html\n",
    "import re\n",
    "import pickle\n",
    "from collections import Counter\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import sklearn\n",
    "from sklearn import model_selection\n",
    "from functools import partial\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils \n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import dataset, dataloader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import time\n",
    "import math\n",
    "import sys\n",
    "import data\n",
    "\n",
    "data_root = './data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21434"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load the tokens\n",
    "itos = pickle.load(open(f'{data_root}itos.pkl', 'rb'))\n",
    "trn_lm = np.load(f'{data_root}trn_lm.npy')\n",
    "val_lm = np.load(f'{data_root}val_lm.npy')\n",
    "\n",
    "stoi = defaultdict(lambda:0, {v:k for k,v in enumerate(itos)})\n",
    "vocab_size = len(itos)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a new data loader to process the SNLI data into pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "class Entail(Enum):\n",
    "    entailment = 0\n",
    "    contradiction = 1\n",
    "    neutral = 2\n",
    "       \n",
    "class SiameseDataset(dataset.Dataset):\n",
    "    def __init__(self, json_file):\n",
    "        \n",
    "        content = None\n",
    "        with open(json_file) as fp:\n",
    "            content = json.load(fp)\n",
    "\n",
    "        self.items = []\n",
    "        for item in content:\n",
    "            s0 = item[0]\n",
    "            s1 = item[1]\n",
    "            label = Entail[item[2]].value\n",
    "            self.items.append((s0, s1, label))\n",
    "            \n",
    "    def shuffle(self):\n",
    "        random.shuffle(self.items)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.items[index]\n",
    "       \n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "    \n",
    "class SiameseDataLoader():\n",
    "    def __init__(self, dataset, stoi, pad_val, batch_size=32):\n",
    "        self.dataset = dataset\n",
    "        dataset.shuffle()\n",
    "        self.batch_size = batch_size\n",
    "        self.stoi = stoi\n",
    "        self.index = 0\n",
    "        self.pad_val = pad_val\n",
    "      \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def fill_tensor(self, sentences, max_len):\n",
    "        data = np.zeros((max_len, self.batch_size), dtype=np.long)\n",
    "        data.fill(self.pad_val)\n",
    "        \n",
    "        for i, s in enumerate(sentences): \n",
    "            start_idx = max_len - len(s)\n",
    "            for j, p in enumerate(s):\n",
    "                data[:,i][start_idx+j] = stoi[p]\n",
    "            \n",
    "        return torch.tensor(data, dtype=torch.long).cuda()\n",
    "     \n",
    "    def batch(self):\n",
    "        return self.index//self.batch_size\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)//self.batch_size\n",
    "    \n",
    "    def __next__(self):\n",
    "        #how many examples to ananlyise for this round\n",
    "        num = min(self.batch_size, len(self.dataset) - self.index)\n",
    "        \n",
    "        if num < 1:\n",
    "            raise StopIteration  # signals \"the end\"\n",
    "            \n",
    "        #collect the sentences\n",
    "        max_len = 0\n",
    "        first = []\n",
    "        second = []\n",
    "        labels = np.zeros((self.batch_size), dtype=np.long)\n",
    "        \n",
    "        for i in range(self.index, self.index+num):\n",
    "            a, b, l = self.dataset[i]\n",
    "            \n",
    "            if len(a) > max_len:\n",
    "                max_len = len(a)\n",
    "            \n",
    "            if len(b) > max_len:\n",
    "                max_len = len(b)\n",
    "            \n",
    "            first.append(a)\n",
    "            second.append(b)\n",
    "            labels[i - self.index] = l\n",
    "            \n",
    "        self.index += num\n",
    "             \n",
    "        return (self.fill_tensor(first, max_len),\n",
    "                self.fill_tensor(second, max_len),\n",
    "                torch.tensor(labels, dtype=torch.long).cuda())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Siamese network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, encoder, classifier):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.classifier = classifier\n",
    "    \n",
    "    def pool(self, x, bs, is_max):\n",
    "        f = F.adaptive_max_pool1d if is_max else F.adaptive_avg_pool1d\n",
    "        return f(x.permute(1,2,0), (1,)).view(bs,-1)\n",
    "\n",
    "    def pool_outputs(self, outputs):\n",
    "        output = outputs[-1]\n",
    "        sl, bs,_ = output.size()\n",
    "        avgpool = self.pool(output, bs, False)\n",
    "        maxpool = self.pool(output, bs, True)\n",
    "        return torch.cat([output[-1], maxpool, avgpool], 1)\n",
    "        \n",
    "    def forward(self, input1, input2):\n",
    "        raw_outputs1, outputs1 = self.encoder(input1)\n",
    "        raw_outputs2, outputs2 = self.encoder(input2)\n",
    "        \n",
    "        out1 = self.pool_outputs(outputs1)\n",
    "        out2 = self.pool_outputs(outputs2)\n",
    "        \n",
    "        out = torch.cat([out1, out2], 1)\n",
    "        return self.classifier(out)\n",
    "        \n",
    "    def reset(self):\n",
    "        for c in self.children():\n",
    "            if hasattr(c, 'reset'): c.reset()\n",
    "                \n",
    "class LinearClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden, num_categories, dropout):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            LinearBlock(input_size, hidden, dropout),\n",
    "            LinearBlock(hidden, hidden, dropout),\n",
    "            LinearBlock(hidden, num_categories, dropout)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for l in self.layers:\n",
    "            x = F.relu(l(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load our pretrained model then build the Siamese network from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#these are the values used for the original LM\n",
    "em_sz,nh,nl = 400,1150,3\n",
    "\n",
    "language_model = torch.load(f'{data_root}language_model.pt\")\n",
    "\n",
    "children = []\n",
    "for child in language_model.children():\n",
    "    children.append(child)\n",
    "\n",
    "#2 pooled vectors, of 3 times the embedding size\n",
    "classifier = LinearClassifier(2*3*em_sz, em_sz,  3, dropout=0.05)\n",
    "siamese_model = SiameseClassifier(children[0], classifier).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a new dataloader to create sentence pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "class Entail(Enum):\n",
    "    entailment = 0\n",
    "    contradiction = 1\n",
    "    neutral = 2\n",
    "       \n",
    "class SiameseDataset(dataset.Dataset):\n",
    "    def __init__(self, json_file):\n",
    "        \n",
    "        content = None\n",
    "        with open(json_file) as fp:\n",
    "            content = json.load(fp)\n",
    "\n",
    "        self.items = []\n",
    "        for item in content:\n",
    "            s0 = item[0]\n",
    "            s1 = item[1]\n",
    "            label = Entail[item[2]].value\n",
    "            self.items.append((s0, s1, label))\n",
    "            \n",
    "    def shuffle(self):\n",
    "        random.shuffle(self.items)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.items[index]\n",
    "       \n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "    \n",
    "class SiameseDataLoader():\n",
    "    def __init__(self, dataset, stoi, pad_val, batch_size=32):\n",
    "        self.dataset = dataset\n",
    "        dataset.shuffle()\n",
    "        self.batch_size = batch_size\n",
    "        self.stoi = stoi\n",
    "        self.index = 0\n",
    "        self.pad_val = pad_val\n",
    "      \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def fill_tensor(self, sentences, max_len):\n",
    "        data = np.zeros((max_len, self.batch_size), dtype=np.long)\n",
    "        data.fill(self.pad_val)\n",
    "        \n",
    "        for i, s in enumerate(sentences): \n",
    "            start_idx = max_len - len(s)\n",
    "            for j, p in enumerate(s):\n",
    "                data[:,i][start_idx+j] = stoi[p]\n",
    "            \n",
    "        return torch.LongTensor([data.tolist()]).cuda()\n",
    "     \n",
    "    def batch(self):\n",
    "        return self.index//self.batch_size\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)//self.batch_size\n",
    "    \n",
    "    def __next__(self):\n",
    "        #how many examples to ananlyise for this round\n",
    "        num = min(self.batch_size, len(self.dataset) - self.index)\n",
    "        \n",
    "        if num < 1:\n",
    "            raise StopIteration  # signals \"the end\"\n",
    "            \n",
    "        #collect the sentences\n",
    "        max_len = 0\n",
    "        first = []\n",
    "        second = []\n",
    "        labels = np.zeros((self.batch_size), dtype=np.long)\n",
    "        \n",
    "        for i in range(self.index, self.index+num):\n",
    "            a, b, l = self.dataset[i]\n",
    "            \n",
    "            if len(a) > max_len:\n",
    "                max_len = len(a)\n",
    "            \n",
    "            if len(b) > max_len:\n",
    "                max_len = len(b)\n",
    "            \n",
    "            first.append(a)\n",
    "            second.append(b)\n",
    "            labels[i - self.index] = l\n",
    "            \n",
    "        self.index += num\n",
    "             \n",
    "        return (self.fill_tensor(first, max_len),\n",
    "                self.fill_tensor(second, max_len),\n",
    "                torch.LongTensor([labels.tolist()]).cuda()\n",
    "               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the new network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_interval = 50\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "def evaluate(model, data_loader):\n",
    "    # Turn on evaluation mode which disables dropout.\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    num_correct = 0\n",
    "    total = 0 \n",
    "    for a, b, l in data_loader:\n",
    "\n",
    "        a, b, l = Variable(a), Variable(b), Variable(l)\n",
    "        a.requires_grad = False\n",
    "        b.requires_grad = False\n",
    "        l.requires_grad = False\n",
    "        out = model(a.squeeze(), b.squeeze())\n",
    "        num_correct += np.sum(l.data.cpu().numpy() == np.argmax(out.data.cpu().numpy(), 1))\n",
    "        total += out.shape[0]\n",
    "        loss = criterion(out, l.squeeze())\n",
    "        total_loss += out.shape[0] * loss.data.cpu()[0]\n",
    "\n",
    "    return (total_loss / total, num_correct / total)\n",
    "\n",
    "def train(model, data_loader, optimizer):\n",
    "    # Turn on training mode which enables dropout.\n",
    "    model.train()\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    \n",
    "    num_correct = 0\n",
    "    total = 0 \n",
    "        \n",
    "    for a, b, l in data_loader:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        a, b, l = Variable(a), Variable(b), Variable(l)\n",
    "\n",
    "        out = model(a.squeeze(), b.squeeze())\n",
    "        loss = criterion(out, l.squeeze())\n",
    "        total_loss += out.shape[0] * loss.data.cpu()[0]\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        num_correct += np.sum(l.data.cpu().numpy() == np.argmax(out.data.cpu().numpy(), 1))\n",
    "        total += out.shape[0]\n",
    "\n",
    "        batch = data_loader.batch()\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / total\n",
    "            elapsed = time.time() - start_time\n",
    "            batches = len(data_loader)\n",
    "            ms = elapsed * 1000 / log_interval\n",
    "            print(f'| epoch {epoch:3d} | {batch:5d}/{batches:5d} batches', end=\" \")\n",
    "            print(f'| ms/batch {ms:5.2f} | loss {cur_loss:5.4f} acc {num_correct / total}')\n",
    "            total_loss = 0\n",
    "            total = 0\n",
    "            num_correct = 0\n",
    "            start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_dataset_dev = SiameseDataset(f'{data_root}/snli_dev.json')\n",
    "siamese_dataset_test = SiameseDataset(f'{data_root}snli_test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training with lr 0.0001\n",
      "| epoch   0 |    50/  307 batches | ms/batch 260.02 | loss 1.1093 acc 0.35375\n",
      "| epoch   0 |   100/  307 batches | ms/batch 248.83 | loss 1.1055 acc 0.344375\n",
      "| epoch   0 |   150/  307 batches | ms/batch 261.89 | loss 1.1023 acc 0.36625\n",
      "| epoch   0 |   200/  307 batches | ms/batch 249.55 | loss 1.1076 acc 0.34375\n",
      "| epoch   0 |   250/  307 batches | ms/batch 269.93 | loss 1.1001 acc 0.35625\n",
      "| epoch   0 |   300/  307 batches | ms/batch 266.70 | loss 1.0951 acc 0.3825\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   0 | time: 110.51s | valid loss  1.08 accuracy 0.40848941368078173 learning rate 0.0001\n",
      "-----------------------------------------------------------------------------------------\n",
      "training with lr 0.0005\n",
      "| epoch   1 |    50/  307 batches | ms/batch 271.31 | loss 1.1076 acc 0.391875\n",
      "| epoch   1 |   100/  307 batches | ms/batch 258.45 | loss 1.1022 acc 0.376875\n",
      "| epoch   1 |   150/  307 batches | ms/batch 275.36 | loss 1.1112 acc 0.361875\n",
      "| epoch   1 |   200/  307 batches | ms/batch 238.92 | loss 1.0943 acc 0.3775\n",
      "| epoch   1 |   250/  307 batches | ms/batch 258.58 | loss 1.0969 acc 0.383125\n",
      "| epoch   1 |   300/  307 batches | ms/batch 262.03 | loss 1.0946 acc 0.38625\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 110.78s | valid loss  1.08 accuracy 0.40075325732899025 learning rate 0.0005\n",
      "-----------------------------------------------------------------------------------------\n",
      "training with lr 0.0001\n",
      "| epoch   2 |    50/  307 batches | ms/batch 272.87 | loss 1.0983 acc 0.375\n",
      "| epoch   2 |   100/  307 batches | ms/batch 258.62 | loss 1.0886 acc 0.39125\n",
      "| epoch   2 |   150/  307 batches | ms/batch 290.76 | loss 1.0826 acc 0.396875\n",
      "| epoch   2 |   200/  307 batches | ms/batch 255.12 | loss 1.0911 acc 0.38375\n",
      "| epoch   2 |   250/  307 batches | ms/batch 261.89 | loss 1.0804 acc 0.3875\n",
      "| epoch   2 |   300/  307 batches | ms/batch 252.68 | loss 1.0781 acc 0.4175\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 112.46s | valid loss  1.08 accuracy 0.4126628664495114 learning rate 0.0001\n",
      "-----------------------------------------------------------------------------------------\n",
      "training with lr 0.0001\n",
      "| epoch   3 |    50/  307 batches | ms/batch 251.66 | loss 1.0782 acc 0.413125\n",
      "| epoch   3 |   100/  307 batches | ms/batch 270.94 | loss 1.0887 acc 0.39375\n",
      "| epoch   3 |   150/  307 batches | ms/batch 268.70 | loss 1.0821 acc 0.3975\n",
      "| epoch   3 |   200/  307 batches | ms/batch 279.97 | loss 1.0897 acc 0.37375\n",
      "| epoch   3 |   250/  307 batches | ms/batch 249.85 | loss 1.0833 acc 0.390625\n",
      "| epoch   3 |   300/  307 batches | ms/batch 252.79 | loss 1.0753 acc 0.39\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 111.77s | valid loss  1.08 accuracy 0.3947475570032573 learning rate 0.0001\n",
      "-----------------------------------------------------------------------------------------\n",
      "training with lr 0.0001\n",
      "| epoch   4 |    50/  307 batches | ms/batch 260.30 | loss 1.0851 acc 0.39\n",
      "| epoch   4 |   100/  307 batches | ms/batch 269.10 | loss 1.0744 acc 0.405\n",
      "| epoch   4 |   150/  307 batches | ms/batch 243.97 | loss 1.0828 acc 0.401875\n",
      "| epoch   4 |   200/  307 batches | ms/batch 259.73 | loss 1.0825 acc 0.39125\n",
      "| epoch   4 |   250/  307 batches | ms/batch 267.23 | loss 1.0767 acc 0.396875\n",
      "| epoch   4 |   300/  307 batches | ms/batch 260.97 | loss 1.0775 acc 0.390625\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 111.06s | valid loss  1.07 accuracy 0.39932817589576547 learning rate 0.0001\n",
      "-----------------------------------------------------------------------------------------\n",
      "training with lr 5e-05\n",
      "| epoch   5 |    50/  307 batches | ms/batch 260.63 | loss 1.0774 acc 0.393125\n",
      "| epoch   5 |   100/  307 batches | ms/batch 262.05 | loss 1.0863 acc 0.38\n",
      "| epoch   5 |   150/  307 batches | ms/batch 269.55 | loss 1.0845 acc 0.383125\n",
      "| epoch   5 |   200/  307 batches | ms/batch 261.20 | loss 1.0798 acc 0.38125\n",
      "| epoch   5 |   250/  307 batches | ms/batch 260.62 | loss 1.0748 acc 0.39\n",
      "| epoch   5 |   300/  307 batches | ms/batch 267.06 | loss 1.0712 acc 0.419375\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 112.16s | valid loss  1.08 accuracy 0.3965798045602606 learning rate 5e-05\n",
      "-----------------------------------------------------------------------------------------\n",
      "training with lr 1e-05\n",
      "| epoch   6 |    50/  307 batches | ms/batch 254.54 | loss 1.0842 acc 0.396875\n",
      "| epoch   6 |   100/  307 batches | ms/batch 262.83 | loss 1.0777 acc 0.391875\n",
      "| epoch   6 |   150/  307 batches | ms/batch 256.78 | loss 1.0699 acc 0.42125\n",
      "| epoch   6 |   200/  307 batches | ms/batch 260.67 | loss 1.0682 acc 0.414375\n",
      "| epoch   6 |   250/  307 batches | ms/batch 267.88 | loss 1.0711 acc 0.40375\n",
      "| epoch   6 |   300/  307 batches | ms/batch 260.70 | loss 1.0736 acc 0.413125\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time: 111.27s | valid loss  1.07 accuracy 0.40818403908794787 learning rate 1e-05\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "lrs = [0.0001, 0.0005, 0.0001, 0.0001, 0.0001, 0.00005, 0.00001]\n",
    "\n",
    "for epoch, lr in enumerate(lrs):\n",
    "\n",
    "    print(f'training with lr {lr}')\n",
    "    optimizer = optim.Adam(siamese_model.parameters(), lr=lr)\n",
    "\n",
    "    training_data = SiameseDataLoader(siamese_dataset_dev, stoi, stoi[\"_pad_\"], batch_size=32)\n",
    "\n",
    "    epoch_start_time = time.time()\n",
    "    train(siamese_model, training_data, optimizer)\n",
    "\n",
    "    validation_data = SiameseDataLoader(siamese_dataset_test , stoi, stoi[\"_pad_\"], batch_size=32)\n",
    "    val_loss, accuracy = evaluate(siamese_model, validation_data)\n",
    "\n",
    "    delta_t = (time.time() - epoch_start_time)\n",
    "    print('-' * 89)\n",
    "    print(f'| end of epoch {epoch:3d} | time: {delta_t:5.2f}s | valid loss {val_loss:5.2f} accuracy {accuracy} learning rate {lr}')\n",
    "    print('-' * 89)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{data_root}siamese_model.pt', 'wb') as f:\n",
    "    torch.save(siamese_model, f)"
   ]
  }
 ],
 "metadata": {
  "_draft": {
   "nbviewer_url": "https://gist.github.com/0dd0df21cf404cf2bb51d0148c8b7d8b"
  },
  "gist": {
   "data": {
    "description": "fastai.text imdb example",
    "public": true
   },
   "id": "0dd0df21cf404cf2bb51d0148c8b7d8b"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "86px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
