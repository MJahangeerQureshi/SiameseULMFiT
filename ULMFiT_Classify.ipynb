{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ULMFiT + Siamese Network for Sentence Vectors\n",
    "## Part Three: Classifying\n",
    "\n",
    "The second notebook created a new language model from the SNLI dataset.\n",
    "This notebook will adapt that model to predicting the SNLI category for sentence pairs.\n",
    "The model will be used as a sentence encoder for a Siamese Network that builds sentence vectors that are feed into a classifier network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text import *\n",
    "import html\n",
    "\n",
    "import json\n",
    "import html\n",
    "import re\n",
    "import pickle\n",
    "from collections import Counter\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import sklearn\n",
    "from sklearn import model_selection\n",
    "from functools import partial\n",
    "from collections import Counter, defaultdict\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils \n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import dataset, dataloader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import time\n",
    "import math\n",
    "import sys\n",
    "import data\n",
    "\n",
    "snli_root = './data/snli_1.0/'\n",
    "token_files = './data/tokens/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a new dataloader to create sentence pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseDataLoader():\n",
    "    def __init__(self, sentence_pairs, pad_val, batch_size=32):\n",
    "        self.sentence_pairs = sentence_pairs\n",
    "        self.batch_size = batch_size\n",
    "        self.index = 0\n",
    "        self.pad_val = pad_val\n",
    "     \n",
    "    def shuffle(self):\n",
    "        def srtfn(x):\n",
    "            return x[:, -1] + random.randint(-5, 5)\n",
    "        \n",
    "        order = np.argsort(srtfn(self.sentence_pairs))\n",
    "        self.sentence_pairs = self.sentence_pairs[order]\n",
    "        \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def fill_tensor(self, sentences, max_len):\n",
    "        data = np.zeros((max_len, len(sentences)), dtype=np.long)\n",
    "        data.fill(self.pad_val)\n",
    "        \n",
    "        for i, s in enumerate(sentences): \n",
    "            start_idx = max_len - len(s)\n",
    "            for j, p in enumerate(s):\n",
    "                data[:,i][start_idx+j] = p\n",
    "            \n",
    "        return torch.LongTensor([data.tolist()]).cuda()\n",
    "     \n",
    "    def batch(self):\n",
    "        return self.index//self.batch_size\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sentence_pairs)//self.batch_size\n",
    "    \n",
    "    def __next__(self):\n",
    "        #how many examples to ananlyise for this round\n",
    "        num = min(self.batch_size, len(self.sentence_pairs) - self.index)\n",
    "        \n",
    "        if num < 1:\n",
    "            raise StopIteration  # signals \"the end\"\n",
    "            \n",
    "        #collect the sentences\n",
    "        max_len = 0\n",
    "        first = []\n",
    "        second = []\n",
    "        labels = torch.LongTensor(num)\n",
    "        \n",
    "        for i in range(num):\n",
    "            a, b, l, _ = self.sentence_pairs[self.index + i]\n",
    "            \n",
    "            if len(a) > max_len:\n",
    "                max_len = len(a)\n",
    "            \n",
    "            if len(b) > max_len:\n",
    "                max_len = len(b)\n",
    "            \n",
    "            first.append(a)\n",
    "            second.append(b)\n",
    "            labels[i] = l\n",
    "            \n",
    "        self.index += num\n",
    "             \n",
    "        return (self.fill_tensor(first, max_len).cuda(),\n",
    "                self.fill_tensor(second, max_len).cuda(),\n",
    "                labels.cuda()\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " x_bos a person on a horse jumps over a broken down airplane .\n",
      " x_bos a person is training his horse for a competition .\n",
      " x_bos two women are embracing while holding to go packages .\n",
      " x_bos the sisters are hugging goodbye while holding to go packages after just eating lunch .\n",
      " x_bos this church choir sings to the masses as they sing joyous songs from the book at a church .\n",
      " x_bos the church has cracks in the ceiling .\n"
     ]
    }
   ],
   "source": [
    "itos = pickle.load(open(f'{token_files}itos.pkl', 'rb'))\n",
    "stoi = defaultdict(lambda:0, {v:k for k,v in enumerate(itos)})\n",
    "vocab_size = len(itos)\n",
    "pad_tok = stoi['_pad_']\n",
    "\n",
    "sentence_pairs_train = np.load(f'{token_files}snli_tok_train.npy')\n",
    "sentence_pairs_dev = np.load(f'{token_files}snli_tok_dev.npy')\n",
    "sentence_pairs_test = np.load(f'{token_files}snli_tok_test.npy')\n",
    "\n",
    "def print_sentence(s):\n",
    "    sentence = \"\"\n",
    "    for tok in s:\n",
    "        sentence += \" \"+itos[tok]\n",
    "    print(sentence)\n",
    "\n",
    "print_sentence(sentence_pairs_train[0][0])\n",
    "print_sentence(sentence_pairs_train[0][1])\n",
    "\n",
    "print_sentence(sentence_pairs_dev[0][0])\n",
    "print_sentence(sentence_pairs_dev[0][1])\n",
    "\n",
    "print_sentence(sentence_pairs_test[0][0])\n",
    "print_sentence(sentence_pairs_test[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check the dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_bos a person on a horse jumps over a broken down airplane . "
     ]
    }
   ],
   "source": [
    "training_data = SiameseDataLoader(sentence_pairs_train, pad_tok)\n",
    "s = training_data.sentence_pairs[0][0]\n",
    "t = training_data.fill_tensor([s], len(s))\n",
    "for s in t[0]:\n",
    "    print(itos[int(s[0])], end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Siamese network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, encoder, linear):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.linear = linear\n",
    "    \n",
    "    def pool(self, x, bs, is_max):\n",
    "        f = F.adaptive_max_pool1d if is_max else F.adaptive_avg_pool1d\n",
    "        return f(x.permute(1,2,0), (1,)).view(bs,-1)\n",
    "\n",
    "    def pool_outputs(self, output):\n",
    "        sl, bs,_ = output.size()\n",
    "        avgpool = self.pool(output, bs, False)\n",
    "        maxpool = self.pool(output, bs, True)\n",
    "        return torch.cat([output[-1], maxpool, avgpool], 1)\n",
    "        \n",
    "    def forward_once(self, input):\n",
    "        raw_outputs, outputs = self.encoder(input)\n",
    "        out = self.pool_outputs(outputs[-1])\n",
    "        return out\n",
    "    \n",
    "    def forward(self, in1, in2):\n",
    "        u = self.forward_once(in1)\n",
    "        v = self.forward_once(in2)\n",
    "        features = torch.cat((u, v, torch.abs(u-v), u*v), 1)\n",
    "        out = self.linear(features)\n",
    "        return out \n",
    "        \n",
    "    def reset(self):\n",
    "        for c in self.children():\n",
    "            if hasattr(c, 'reset'): c.reset()\n",
    "\n",
    "class LinearClassifier(nn.Module):\n",
    "    def __init__(self, layers, dropout):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([LinearBlock(layers[i], layers[i + 1], dropout) for i in range(len(layers) - 1)])\n",
    "        \n",
    "    def forward(self, input):\n",
    "        x = input\n",
    "        for l in self.layers:\n",
    "            l_x = l(x)\n",
    "            x = F.relu(l_x)\n",
    "        return l_x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#these are the values used for the original LM\n",
    "em_sz, nh, nl = 400, 1150, 3\n",
    "bptt = 70\n",
    "max_seq = bptt * 20\n",
    "cats = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load our pretrained model then build the Siamese network from it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop\n",
    "This should be converted over to the fast.ai learner but I'm not sure how to do that yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_interval = 1000\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#criterion = nn.CosineEmbeddingLoss()\n",
    "\n",
    "def evaluate(model, data_loader):\n",
    "    # Turn on evaluation mode which disables dropout.\n",
    "    model.eval()\n",
    "    \n",
    "    total_loss = 0.\n",
    "    num_correct = 0\n",
    "    total = 0 \n",
    "    \n",
    "    for a, b, l in data_loader:\n",
    "        \n",
    "        model.reset()\n",
    "        a, b, l = Variable(a), Variable(b), Variable(l)\n",
    "        out = model(a.squeeze(), b.squeeze())\n",
    "        loss = criterion(out, l.squeeze())\n",
    "        total += l.size(0)\n",
    "        total_loss += l.size(0) * loss.item()\n",
    "        num_correct += np.sum(l.data.cpu().numpy() == np.argmax(out.data.cpu().numpy(), 1))\n",
    "        \n",
    "    return (total_loss / total, num_correct / total)\n",
    "\n",
    "def train(model, data_loader, optimizer):\n",
    "    # Turn on training mode which enables dropout.\n",
    "    start_time = time.time()\n",
    "    model.train() \n",
    "    \n",
    "    total_loss = 0.\n",
    "    num_correct = 0\n",
    "    total = 0 \n",
    "        \n",
    "    for a, b, l in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        model.reset()\n",
    "        a, b, l = Variable(a), Variable(b), Variable(l)\n",
    "        out = model(a.squeeze(), b.squeeze())\n",
    "        loss = criterion(out, l.squeeze())\n",
    "        total += l.size(0)\n",
    "        total_loss += l.size(0) * loss.item()\n",
    "        num_correct += np.sum(l.data.cpu().numpy() == np.argmax(out.data.cpu().numpy(), 1))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch = data_loader.batch()\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / total\n",
    "            elapsed = time.time() - start_time\n",
    "            batches = len(data_loader)\n",
    "            ms = elapsed * 1000 / log_interval\n",
    "            print(f'| {batch:5d}/{batches:5d} batches', end=\" \")\n",
    "            print(f'| ms/batch {ms:5.2f} | loss {cur_loss:5.4f} acc {num_correct / total}')\n",
    "            #print(f'| ms/batch {ms:5.2f} | loss {cur_loss:5.4f}')\n",
    "            total_loss = 0\n",
    "            total = 0\n",
    "            num_correct = 0\n",
    "            start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_loss = 100\n",
    "def training_loop(model, epochs, optimizer, scheduler = None):\n",
    "    \n",
    "    global best_loss\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        print(f'Start epoch {epoch:3d} training with lr ', end=\"\")\n",
    "        for g in optimizer.param_groups:\n",
    "            print(g['lr'], end=\" \")\n",
    "        print(\"\")\n",
    "        \n",
    "        training_data = SiameseDataLoader(sentence_pairs_train, pad_tok)\n",
    "        training_data.shuffle()\n",
    "\n",
    "        epoch_start_time = time.time()\n",
    "        \n",
    "        train(siamese_model, training_data, optimizer)\n",
    "        if scheduler != None:\n",
    "            scheduler.step()\n",
    "\n",
    "        dev_data = SiameseDataLoader(sentence_pairs_dev, pad_tok)\n",
    "        val_loss, accuracy = evaluate(siamese_model, dev_data)\n",
    "\n",
    "        delta_t = (time.time() - epoch_start_time)\n",
    "        print('-' * 89)\n",
    "        print(f'| end of epoch {epoch:3d} | time: {delta_t:5.2f}s | valid loss {val_loss:5.2f} accuracy {accuracy} learning rates')\n",
    "        for g in optimizer.param_groups:\n",
    "            print(g['lr'])\n",
    "        print('-' * 89)\n",
    "\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            with open(f'./siamese_model{val_loss:0.2f}{accuracy:0.2f}.pt', 'wb') as f:\n",
    "                torch.save(siamese_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import butter, filtfilt\n",
    "def butter_lowpass(cutoff, fs, order=5):\n",
    "    nyq = 0.5 * fs\n",
    "    normal_cutoff = cutoff / nyq\n",
    "    b, a = butter(order, normal_cutoff, btype='low', analog=False)\n",
    "    return b, a\n",
    "\n",
    "def butter_lowpass_filtfilt(data, cutoff, fs, order=5):\n",
    "    b, a = butter_lowpass(cutoff, fs, order=order)\n",
    "    y = filtfilt(b, a, data)\n",
    "    return y\n",
    "\n",
    "def plot_loss(losses):\n",
    "    plt.semilogx(losses[:,0], losses[:,1])\n",
    "    plt.semilogx(losses[:,0], butter_lowpass_filtfilt(losses[:,1], 300, 5000))\n",
    "    plt.show()\n",
    "\n",
    "def find_lr(model, model_to_optim, data_loader):\n",
    "    losses = []\n",
    "    model.train() \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    lr = 0.00001\n",
    "    for a, b, l in data_loader:\n",
    "        optimizer = optim.SGD(model_to_optim.parameters(), lr=lr)\n",
    "        #optimizer = optim.Adam(model_to_optim.parameters(), lr=lr)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        model.reset()\n",
    "        a, b, l = Variable(a), Variable(b), Variable(l)\n",
    "        out = model(a.squeeze(), b.squeeze())\n",
    "        loss = criterion(out, l.squeeze())\n",
    "        \n",
    "        los_val = loss.item()\n",
    "        losses.append((lr, los_val))\n",
    "        if los_val > 5:\n",
    "            break\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        lr *= 1.05\n",
    "    losses = np.array(losses)\n",
    "    #plot_loss(losses)\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "SNLI_LM = torch.load(\"snli_language_model.pt\")\n",
    "\n",
    "dps = np.array([0.4,0.5,0.05,0.3,0.4])*0.4\n",
    "SNLI_encoder = MultiBatchRNN(bptt, max_seq, vocab_size, em_sz, nh, nl, pad_tok, dropouti=dps[0], wdrop=dps[2], dropoute=dps[3], dropouth=dps[4])\n",
    "\n",
    "SNLI_encoder.load_state_dict(SNLI_LM[0].state_dict())\n",
    "\n",
    "#2 pooled vectors, of 3 times the embedding size\n",
    "siamese_model = SiameseClassifier(SNLI_encoder, LinearClassifier(layers=[em_sz*3*4, nh, 3], dropout=0.4)).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dev_data = SiameseDataLoader(sentence_pairs_dev, pad_tok)\n",
    "losses = find_lr(siamese_model, siamese_model, dev_data)\n",
    "plot_loss(np.array(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start epoch   0 training with lr 0.001 \n",
      "|  1000/17167 batches | ms/batch 14.81 | loss 0.9315 acc 0.57040625\n",
      "|  2000/17167 batches | ms/batch 15.86 | loss 0.9426 acc 0.5574375\n",
      "|  3000/17167 batches | ms/batch 16.50 | loss 0.9429 acc 0.5551875\n",
      "|  4000/17167 batches | ms/batch 17.09 | loss 0.9569 acc 0.54521875\n",
      "|  5000/17167 batches | ms/batch 17.51 | loss 0.9535 acc 0.54403125\n",
      "|  6000/17167 batches | ms/batch 17.96 | loss 0.9579 acc 0.54275\n",
      "|  7000/17167 batches | ms/batch 18.41 | loss 0.9613 acc 0.5358125\n",
      "|  8000/17167 batches | ms/batch 18.98 | loss 0.9587 acc 0.53903125\n",
      "|  9000/17167 batches | ms/batch 19.42 | loss 0.9612 acc 0.53721875\n",
      "| 10000/17167 batches | ms/batch 19.99 | loss 0.9547 acc 0.54028125\n",
      "| 11000/17167 batches | ms/batch 20.63 | loss 0.9585 acc 0.53871875\n",
      "| 12000/17167 batches | ms/batch 21.44 | loss 0.9607 acc 0.53503125\n",
      "| 13000/17167 batches | ms/batch 22.21 | loss 0.9616 acc 0.53978125\n",
      "| 14000/17167 batches | ms/batch 23.14 | loss 0.9601 acc 0.53853125\n",
      "| 15000/17167 batches | ms/batch 24.29 | loss 0.9554 acc 0.53709375\n",
      "| 16000/17167 batches | ms/batch 25.99 | loss 0.9650 acc 0.5325625\n",
      "| 17000/17167 batches | ms/batch 29.40 | loss 0.9599 acc 0.53584375\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   0 | time: 356.54s | valid loss  0.97 accuracy 0.5001016053647632 learning rates\n",
      "0.001\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for param in siamese_model.encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "optimizer = optim.SGD(siamese_model.parameters(), lr=0.001)\n",
    "training_loop(siamese_model.linear, 1, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start epoch   0 training with lr 0.005 \n",
      "|  1000/17167 batches | ms/batch 31.86 | loss 0.9271 acc 0.574125\n",
      "|  2000/17167 batches | ms/batch 34.50 | loss 0.9278 acc 0.56896875\n",
      "|  3000/17167 batches | ms/batch 36.01 | loss 0.9226 acc 0.5688125\n",
      "|  4000/17167 batches | ms/batch 37.56 | loss 0.9332 acc 0.56015625\n",
      "|  5000/17167 batches | ms/batch 38.61 | loss 0.9257 acc 0.5644375\n",
      "|  6000/17167 batches | ms/batch 39.72 | loss 0.9317 acc 0.55896875\n",
      "|  7000/17167 batches | ms/batch 40.74 | loss 0.9342 acc 0.5561875\n",
      "|  8000/17167 batches | ms/batch 42.09 | loss 0.9351 acc 0.558875\n",
      "|  9000/17167 batches | ms/batch 43.18 | loss 0.9357 acc 0.5564375\n",
      "| 10000/17167 batches | ms/batch 44.56 | loss 0.9309 acc 0.55953125\n",
      "| 11000/17167 batches | ms/batch 46.15 | loss 0.9315 acc 0.5585\n",
      "| 12000/17167 batches | ms/batch 48.03 | loss 0.9303 acc 0.55859375\n",
      "| 13000/17167 batches | ms/batch 49.87 | loss 0.9339 acc 0.55453125\n",
      "| 14000/17167 batches | ms/batch 52.01 | loss 0.9315 acc 0.55578125\n",
      "| 15000/17167 batches | ms/batch 54.67 | loss 0.9302 acc 0.5595\n",
      "| 16000/17167 batches | ms/batch 58.85 | loss 0.9374 acc 0.5525625\n",
      "| 17000/17167 batches | ms/batch 67.17 | loss 0.9308 acc 0.5603125\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   0 | time: 786.95s | valid loss  0.88 accuracy 0.5835196098353993 learning rates\n",
      "0.005\n",
      "-----------------------------------------------------------------------------------------\n",
      "Start epoch   0 training with lr 0.01 \n",
      "|  1000/17167 batches | ms/batch 31.81 | loss 0.8921 acc 0.60359375\n",
      "|  2000/17167 batches | ms/batch 34.45 | loss 0.8996 acc 0.5915\n",
      "|  3000/17167 batches | ms/batch 35.98 | loss 0.9030 acc 0.5851875\n",
      "|  4000/17167 batches | ms/batch 37.46 | loss 0.9118 acc 0.576\n",
      "|  5000/17167 batches | ms/batch 38.60 | loss 0.9029 acc 0.58096875\n",
      "|  6000/17167 batches | ms/batch 39.77 | loss 0.9153 acc 0.57228125\n",
      "|  7000/17167 batches | ms/batch 40.82 | loss 0.9143 acc 0.5735\n",
      "|  8000/17167 batches | ms/batch 42.18 | loss 0.9172 acc 0.56840625\n",
      "|  9000/17167 batches | ms/batch 43.24 | loss 0.9192 acc 0.57090625\n",
      "| 10000/17167 batches | ms/batch 44.59 | loss 0.9115 acc 0.5766875\n",
      "| 11000/17167 batches | ms/batch 46.15 | loss 0.9145 acc 0.57209375\n",
      "| 12000/17167 batches | ms/batch 48.09 | loss 0.9138 acc 0.57\n",
      "| 13000/17167 batches | ms/batch 49.92 | loss 0.9138 acc 0.57278125\n",
      "| 14000/17167 batches | ms/batch 52.00 | loss 0.9170 acc 0.57240625\n",
      "| 15000/17167 batches | ms/batch 54.69 | loss 0.9176 acc 0.56821875\n",
      "| 16000/17167 batches | ms/batch 58.93 | loss 0.9218 acc 0.56209375\n",
      "| 17000/17167 batches | ms/batch 67.12 | loss 0.9175 acc 0.5645625\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   0 | time: 787.17s | valid loss  0.83 accuracy 0.6275147327778907 learning rates\n",
      "0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "Start epoch   0 training with lr 0.015 \n",
      "|  1000/17167 batches | ms/batch 31.88 | loss 0.8650 acc 0.6198125\n",
      "|  2000/17167 batches | ms/batch 34.48 | loss 0.8790 acc 0.6045\n",
      "|  3000/17167 batches | ms/batch 36.04 | loss 0.8815 acc 0.5986875\n",
      "|  4000/17167 batches | ms/batch 37.57 | loss 0.8874 acc 0.59421875\n",
      "|  5000/17167 batches | ms/batch 38.69 | loss 0.8882 acc 0.59428125\n",
      "|  6000/17167 batches | ms/batch 39.77 | loss 0.8933 acc 0.589375\n",
      "|  7000/17167 batches | ms/batch 40.77 | loss 0.9008 acc 0.5839375\n",
      "|  8000/17167 batches | ms/batch 42.15 | loss 0.9002 acc 0.58421875\n",
      "|  9000/17167 batches | ms/batch 43.16 | loss 0.9033 acc 0.58084375\n",
      "| 10000/17167 batches | ms/batch 44.61 | loss 0.8930 acc 0.58834375\n",
      "| 11000/17167 batches | ms/batch 46.18 | loss 0.8989 acc 0.58553125\n",
      "| 12000/17167 batches | ms/batch 48.02 | loss 0.8963 acc 0.58321875\n",
      "| 13000/17167 batches | ms/batch 49.85 | loss 0.8976 acc 0.583\n",
      "| 14000/17167 batches | ms/batch 52.00 | loss 0.9038 acc 0.58171875\n",
      "| 15000/17167 batches | ms/batch 54.66 | loss 0.9022 acc 0.58309375\n",
      "| 16000/17167 batches | ms/batch 58.87 | loss 0.9090 acc 0.57575\n",
      "| 17000/17167 batches | ms/batch 67.14 | loss 0.8987 acc 0.58275\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   0 | time: 787.21s | valid loss  0.81 accuracy 0.6369640317008738 learning rates\n",
      "0.015\n",
      "-----------------------------------------------------------------------------------------\n",
      "Start epoch   0 training with lr 0.02 \n",
      "|  1000/17167 batches | ms/batch 31.85 | loss 0.8344 acc 0.639\n",
      "|  2000/17167 batches | ms/batch 34.50 | loss 0.8577 acc 0.61984375\n",
      "|  3000/17167 batches | ms/batch 36.02 | loss 0.8550 acc 0.6154375\n",
      "|  4000/17167 batches | ms/batch 37.56 | loss 0.8644 acc 0.6096875\n",
      "|  5000/17167 batches | ms/batch 38.67 | loss 0.8638 acc 0.6076875\n",
      "|  6000/17167 batches | ms/batch 39.78 | loss 0.8691 acc 0.60834375\n",
      "|  7000/17167 batches | ms/batch 40.85 | loss 0.8743 acc 0.59825\n",
      "|  8000/17167 batches | ms/batch 42.16 | loss 0.8671 acc 0.60775\n",
      "|  9000/17167 batches | ms/batch 43.22 | loss 0.8704 acc 0.6035\n",
      "| 10000/17167 batches | ms/batch 44.61 | loss 0.8676 acc 0.6064375\n",
      "| 11000/17167 batches | ms/batch 46.25 | loss 0.8741 acc 0.596875\n",
      "| 12000/17167 batches | ms/batch 48.16 | loss 0.8702 acc 0.60178125\n",
      "| 13000/17167 batches | ms/batch 49.99 | loss 0.8765 acc 0.5983125\n",
      "| 14000/17167 batches | ms/batch 52.03 | loss 0.8779 acc 0.59875\n",
      "| 15000/17167 batches | ms/batch 54.66 | loss 0.8763 acc 0.59784375\n",
      "| 16000/17167 batches | ms/batch 58.88 | loss 0.8862 acc 0.59003125\n",
      "| 17000/17167 batches | ms/batch 67.17 | loss 0.8792 acc 0.596375\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   0 | time: 787.67s | valid loss  0.80 accuracy 0.640926640926641 learning rates\n",
      "0.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "Start epoch   0 training with lr 0.025 \n",
      "|  1000/17167 batches | ms/batch 31.89 | loss 0.7986 acc 0.65746875\n",
      "|  2000/17167 batches | ms/batch 34.53 | loss 0.8253 acc 0.6409375\n",
      "|  3000/17167 batches | ms/batch 36.03 | loss 0.8294 acc 0.63121875\n",
      "|  4000/17167 batches | ms/batch 37.44 | loss 0.8359 acc 0.62928125\n",
      "|  5000/17167 batches | ms/batch 38.54 | loss 0.8349 acc 0.625875\n",
      "|  6000/17167 batches | ms/batch 39.62 | loss 0.8406 acc 0.62428125\n",
      "|  7000/17167 batches | ms/batch 40.70 | loss 0.8429 acc 0.62128125\n",
      "|  8000/17167 batches | ms/batch 41.99 | loss 0.8473 acc 0.615125\n",
      "|  9000/17167 batches | ms/batch 43.08 | loss 0.8484 acc 0.61809375\n",
      "| 10000/17167 batches | ms/batch 44.54 | loss 0.8440 acc 0.6204375\n",
      "| 11000/17167 batches | ms/batch 46.12 | loss 0.8456 acc 0.61815625\n",
      "| 12000/17167 batches | ms/batch 48.03 | loss 0.8444 acc 0.619625\n",
      "| 13000/17167 batches | ms/batch 49.82 | loss 0.8481 acc 0.61575\n",
      "| 14000/17167 batches | ms/batch 51.92 | loss 0.8472 acc 0.61571875\n",
      "| 15000/17167 batches | ms/batch 54.60 | loss 0.8525 acc 0.6141875\n",
      "| 16000/17167 batches | ms/batch 58.78 | loss 0.8627 acc 0.6075625\n",
      "| 17000/17167 batches | ms/batch 67.13 | loss 0.8527 acc 0.61415625\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   0 | time: 786.11s | valid loss  0.71 accuracy 0.692237350132087 learning rates\n",
      "0.025\n",
      "-----------------------------------------------------------------------------------------\n",
      "Start epoch   0 training with lr 0.030000000000000002 \n",
      "|  1000/17167 batches | ms/batch 31.94 | loss 0.7721 acc 0.67228125\n",
      "|  2000/17167 batches | ms/batch 34.58 | loss 0.7954 acc 0.65353125\n",
      "|  3000/17167 batches | ms/batch 36.11 | loss 0.8017 acc 0.64671875\n",
      "|  4000/17167 batches | ms/batch 37.59 | loss 0.8075 acc 0.64534375\n",
      "|  5000/17167 batches | ms/batch 38.72 | loss 0.8035 acc 0.64675\n",
      "|  6000/17167 batches | ms/batch 39.81 | loss 0.8052 acc 0.64390625\n",
      "|  7000/17167 batches | ms/batch 40.82 | loss 0.8126 acc 0.635875\n",
      "|  8000/17167 batches | ms/batch 42.16 | loss 0.8191 acc 0.63484375\n",
      "|  9000/17167 batches | ms/batch 43.24 | loss 0.8196 acc 0.63328125\n",
      "| 10000/17167 batches | ms/batch 44.60 | loss 0.8147 acc 0.63521875\n",
      "| 11000/17167 batches | ms/batch 46.20 | loss 0.8151 acc 0.635375\n",
      "| 12000/17167 batches | ms/batch 47.98 | loss 0.8181 acc 0.6339375\n",
      "| 13000/17167 batches | ms/batch 49.87 | loss 0.8219 acc 0.62965625\n",
      "| 14000/17167 batches | ms/batch 51.95 | loss 0.8212 acc 0.63275\n",
      "| 15000/17167 batches | ms/batch 54.60 | loss 0.8231 acc 0.6288125\n",
      "| 16000/17167 batches | ms/batch 58.90 | loss 0.8323 acc 0.62346875\n",
      "| 17000/17167 batches | ms/batch 67.21 | loss 0.8321 acc 0.622875\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   0 | time: 787.63s | valid loss  0.73 accuracy 0.6850233692338955 learning rates\n",
      "0.030000000000000002\n",
      "-----------------------------------------------------------------------------------------\n",
      "Start epoch   0 training with lr 0.034999999999999996 \n",
      "|  1000/17167 batches | ms/batch 31.87 | loss 0.7375 acc 0.6865\n",
      "|  2000/17167 batches | ms/batch 34.50 | loss 0.7638 acc 0.6718125\n",
      "|  3000/17167 batches | ms/batch 36.03 | loss 0.7721 acc 0.660125\n",
      "|  4000/17167 batches | ms/batch 37.52 | loss 0.7770 acc 0.65759375\n",
      "|  5000/17167 batches | ms/batch 38.63 | loss 0.7789 acc 0.6570625\n",
      "|  6000/17167 batches | ms/batch 39.74 | loss 0.7784 acc 0.65715625\n",
      "|  7000/17167 batches | ms/batch 40.78 | loss 0.7856 acc 0.65228125\n",
      "|  8000/17167 batches | ms/batch 42.10 | loss 0.7851 acc 0.651\n",
      "|  9000/17167 batches | ms/batch 43.15 | loss 0.7939 acc 0.6465625\n",
      "| 10000/17167 batches | ms/batch 44.57 | loss 0.7903 acc 0.6475625\n",
      "| 11000/17167 batches | ms/batch 46.01 | loss 0.7892 acc 0.65046875\n",
      "| 12000/17167 batches | ms/batch 48.00 | loss 0.7863 acc 0.650375\n",
      "| 13000/17167 batches | ms/batch 49.90 | loss 0.7995 acc 0.64171875\n",
      "| 14000/17167 batches | ms/batch 51.93 | loss 0.8020 acc 0.643125\n",
      "| 15000/17167 batches | ms/batch 54.63 | loss 0.8021 acc 0.641\n",
      "| 16000/17167 batches | ms/batch 58.84 | loss 0.8129 acc 0.6344375\n",
      "| 17000/17167 batches | ms/batch 67.15 | loss 0.8046 acc 0.6404375\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   0 | time: 786.68s | valid loss  0.70 accuracy 0.6998577524893315 learning rates\n",
      "0.034999999999999996\n",
      "-----------------------------------------------------------------------------------------\n",
      "Start epoch   0 training with lr 0.04 \n",
      "|  1000/17167 batches | ms/batch 31.89 | loss 0.7166 acc 0.69828125\n",
      "|  2000/17167 batches | ms/batch 34.50 | loss 0.7355 acc 0.68440625\n",
      "|  3000/17167 batches | ms/batch 36.00 | loss 0.7485 acc 0.677\n",
      "|  4000/17167 batches | ms/batch 37.49 | loss 0.7492 acc 0.67365625\n",
      "|  5000/17167 batches | ms/batch 38.54 | loss 0.7488 acc 0.67184375\n",
      "|  6000/17167 batches | ms/batch 39.63 | loss 0.7608 acc 0.665875\n",
      "|  7000/17167 batches | ms/batch 40.70 | loss 0.7646 acc 0.6638125\n",
      "|  8000/17167 batches | ms/batch 42.01 | loss 0.7656 acc 0.66409375\n",
      "|  9000/17167 batches | ms/batch 43.02 | loss 0.7694 acc 0.6628125\n",
      "| 10000/17167 batches | ms/batch 44.50 | loss 0.7664 acc 0.6613125\n",
      "| 11000/17167 batches | ms/batch 45.98 | loss 0.7682 acc 0.6633125\n",
      "| 12000/17167 batches | ms/batch 48.05 | loss 0.7692 acc 0.65875\n",
      "| 13000/17167 batches | ms/batch 49.81 | loss 0.7714 acc 0.657\n",
      "| 14000/17167 batches | ms/batch 51.91 | loss 0.7749 acc 0.65828125\n",
      "| 15000/17167 batches | ms/batch 54.55 | loss 0.7785 acc 0.65546875\n",
      "| 16000/17167 batches | ms/batch 58.80 | loss 0.7892 acc 0.65078125\n",
      "| 17000/17167 batches | ms/batch 67.12 | loss 0.7857 acc 0.65071875\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   0 | time: 785.84s | valid loss  0.68 accuracy 0.7112375533428165 learning rates\n",
      "0.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "Start epoch   0 training with lr 0.045 \n",
      "|  1000/17167 batches | ms/batch 31.87 | loss 0.6935 acc 0.7096875\n",
      "|  2000/17167 batches | ms/batch 34.49 | loss 0.7137 acc 0.69878125\n",
      "|  3000/17167 batches | ms/batch 36.02 | loss 0.7241 acc 0.69\n",
      "|  4000/17167 batches | ms/batch 37.54 | loss 0.7333 acc 0.6835625\n",
      "|  5000/17167 batches | ms/batch 38.63 | loss 0.7313 acc 0.6830625\n",
      "|  6000/17167 batches | ms/batch 39.72 | loss 0.7316 acc 0.6825\n",
      "|  7000/17167 batches | ms/batch 40.75 | loss 0.7414 acc 0.67659375\n",
      "|  8000/17167 batches | ms/batch 42.09 | loss 0.7446 acc 0.6746875\n",
      "|  9000/17167 batches | ms/batch 43.15 | loss 0.7530 acc 0.66875\n",
      "| 10000/17167 batches | ms/batch 44.53 | loss 0.7454 acc 0.6763125\n",
      "| 11000/17167 batches | ms/batch 46.13 | loss 0.7484 acc 0.67121875\n",
      "| 12000/17167 batches | ms/batch 48.15 | loss 0.7469 acc 0.6703125\n",
      "| 13000/17167 batches | ms/batch 49.90 | loss 0.7531 acc 0.67059375\n",
      "| 14000/17167 batches | ms/batch 52.01 | loss 0.7618 acc 0.66509375\n",
      "| 15000/17167 batches | ms/batch 54.68 | loss 0.7596 acc 0.66478125\n",
      "| 16000/17167 batches | ms/batch 58.88 | loss 0.7714 acc 0.654875\n",
      "| 17000/17167 batches | ms/batch 67.22 | loss 0.7677 acc 0.66090625\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   0 | time: 787.17s | valid loss  0.65 accuracy 0.7262751473277789 learning rates\n",
      "0.045\n",
      "-----------------------------------------------------------------------------------------\n",
      "Start epoch   0 training with lr 0.049999999999999996 \n",
      "|  1000/17167 batches | ms/batch 31.76 | loss 0.6802 acc 0.7174375\n",
      "|  2000/17167 batches | ms/batch 34.42 | loss 0.6997 acc 0.7038125\n",
      "|  3000/17167 batches | ms/batch 35.93 | loss 0.7087 acc 0.69840625\n",
      "|  4000/17167 batches | ms/batch 37.44 | loss 0.7125 acc 0.69275\n",
      "|  5000/17167 batches | ms/batch 38.56 | loss 0.7130 acc 0.691875\n",
      "|  6000/17167 batches | ms/batch 39.69 | loss 0.7225 acc 0.68878125\n",
      "|  7000/17167 batches | ms/batch 40.69 | loss 0.7278 acc 0.68365625\n",
      "|  8000/17167 batches | ms/batch 42.02 | loss 0.7293 acc 0.68040625\n",
      "|  9000/17167 batches | ms/batch 43.12 | loss 0.7389 acc 0.67646875\n",
      "| 10000/17167 batches | ms/batch 44.50 | loss 0.7340 acc 0.68065625\n",
      "| 11000/17167 batches | ms/batch 46.02 | loss 0.7348 acc 0.67940625\n",
      "| 12000/17167 batches | ms/batch 47.95 | loss 0.7365 acc 0.67809375\n",
      "| 13000/17167 batches | ms/batch 49.79 | loss 0.7397 acc 0.67603125\n",
      "| 14000/17167 batches | ms/batch 51.88 | loss 0.7475 acc 0.67878125\n",
      "| 15000/17167 batches | ms/batch 54.58 | loss 0.7503 acc 0.67225\n",
      "| 16000/17167 batches | ms/batch 58.77 | loss 0.7602 acc 0.66528125\n",
      "| 17000/17167 batches | ms/batch 67.02 | loss 0.7591 acc 0.66609375\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   0 | time: 785.50s | valid loss  0.64 accuracy 0.7291200975411501 learning rates\n",
      "0.049999999999999996\n",
      "-----------------------------------------------------------------------------------------\n",
      "Start epoch   0 training with lr 0.055 \n",
      "|  1000/17167 batches | ms/batch 31.86 | loss 0.6701 acc 0.72259375\n",
      "|  2000/17167 batches | ms/batch 34.48 | loss 0.6826 acc 0.71184375\n",
      "|  3000/17167 batches | ms/batch 35.99 | loss 0.6921 acc 0.70875\n",
      "|  4000/17167 batches | ms/batch 37.50 | loss 0.7042 acc 0.70159375\n",
      "|  5000/17167 batches | ms/batch 38.65 | loss 0.6985 acc 0.699625\n",
      "|  6000/17167 batches | ms/batch 39.76 | loss 0.7077 acc 0.6959375\n",
      "|  7000/17167 batches | ms/batch 40.75 | loss 0.7136 acc 0.68815625\n",
      "|  8000/17167 batches | ms/batch 42.09 | loss 0.7199 acc 0.68640625\n",
      "|  9000/17167 batches | ms/batch 43.18 | loss 0.7246 acc 0.685625\n",
      "| 10000/17167 batches | ms/batch 44.58 | loss 0.7212 acc 0.6901875\n",
      "| 11000/17167 batches | ms/batch 46.10 | loss 0.7170 acc 0.68725\n",
      "| 12000/17167 batches | ms/batch 48.05 | loss 0.7168 acc 0.6886875\n",
      "| 13000/17167 batches | ms/batch 49.86 | loss 0.7306 acc 0.6794375\n",
      "| 14000/17167 batches | ms/batch 51.98 | loss 0.7376 acc 0.67778125\n",
      "| 15000/17167 batches | ms/batch 54.65 | loss 0.7359 acc 0.676625\n",
      "| 16000/17167 batches | ms/batch 58.92 | loss 0.7482 acc 0.6683125\n",
      "| 17000/17167 batches | ms/batch 67.12 | loss 0.7431 acc 0.67615625\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   0 | time: 786.88s | valid loss  0.61 accuracy 0.7461897988213778 learning rates\n",
      "0.055\n",
      "-----------------------------------------------------------------------------------------\n",
      "Start epoch   0 training with lr 0.06 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brian/.conda/envs/pytorch/lib/python3.6/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type SiameseClassifier. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/brian/.conda/envs/pytorch/lib/python3.6/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type LinearClassifier. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|  1000/17167 batches | ms/batch 31.78 | loss 0.6576 acc 0.7301875\n",
      "|  2000/17167 batches | ms/batch 34.42 | loss 0.6743 acc 0.7169375\n",
      "|  3000/17167 batches | ms/batch 35.94 | loss 0.6866 acc 0.70765625\n",
      "|  4000/17167 batches | ms/batch 37.44 | loss 0.6910 acc 0.706125\n",
      "|  5000/17167 batches | ms/batch 38.56 | loss 0.6902 acc 0.7039375\n",
      "|  6000/17167 batches | ms/batch 39.67 | loss 0.6962 acc 0.70353125\n",
      "|  7000/17167 batches | ms/batch 40.70 | loss 0.7029 acc 0.6985625\n",
      "|  8000/17167 batches | ms/batch 42.04 | loss 0.7046 acc 0.69346875\n",
      "|  9000/17167 batches | ms/batch 43.08 | loss 0.7168 acc 0.68959375\n",
      "| 10000/17167 batches | ms/batch 44.51 | loss 0.7083 acc 0.6946875\n",
      "| 11000/17167 batches | ms/batch 46.05 | loss 0.7096 acc 0.69225\n",
      "| 12000/17167 batches | ms/batch 47.94 | loss 0.7130 acc 0.6936875\n",
      "| 13000/17167 batches | ms/batch 49.74 | loss 0.7231 acc 0.68546875\n",
      "| 14000/17167 batches | ms/batch 51.86 | loss 0.7250 acc 0.68784375\n",
      "| 15000/17167 batches | ms/batch 54.55 | loss 0.7252 acc 0.68484375\n",
      "| 16000/17167 batches | ms/batch 58.75 | loss 0.7413 acc 0.676625\n",
      "| 17000/17167 batches | ms/batch 67.02 | loss 0.7387 acc 0.679375\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   0 | time: 785.42s | valid loss  0.61 accuracy 0.7496443812233285 learning rates\n",
      "0.06\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brian/.conda/envs/pytorch/lib/python3.6/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type SiameseClassifier. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/brian/.conda/envs/pytorch/lib/python3.6/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type LinearClassifier. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start epoch   0 training with lr 0.065 \n",
      "|  1000/17167 batches | ms/batch 31.82 | loss 0.6457 acc 0.7345625\n",
      "|  2000/17167 batches | ms/batch 34.41 | loss 0.6629 acc 0.72140625\n",
      "|  3000/17167 batches | ms/batch 35.88 | loss 0.6707 acc 0.71659375\n",
      "|  4000/17167 batches | ms/batch 37.41 | loss 0.6787 acc 0.7128125\n",
      "|  5000/17167 batches | ms/batch 38.56 | loss 0.6826 acc 0.7079375\n",
      "|  6000/17167 batches | ms/batch 39.70 | loss 0.6815 acc 0.71325\n",
      "|  7000/17167 batches | ms/batch 40.67 | loss 0.6960 acc 0.69884375\n",
      "|  8000/17167 batches | ms/batch 42.00 | loss 0.6944 acc 0.700375\n",
      "|  9000/17167 batches | ms/batch 43.08 | loss 0.7055 acc 0.69815625\n",
      "| 10000/17167 batches | ms/batch 44.46 | loss 0.6943 acc 0.70165625\n",
      "| 11000/17167 batches | ms/batch 46.05 | loss 0.7027 acc 0.6974375\n",
      "| 12000/17167 batches | ms/batch 47.94 | loss 0.7037 acc 0.69446875\n",
      "| 13000/17167 batches | ms/batch 49.74 | loss 0.7114 acc 0.69228125\n",
      "| 14000/17167 batches | ms/batch 51.85 | loss 0.7164 acc 0.6899375\n",
      "| 15000/17167 batches | ms/batch 54.55 | loss 0.7178 acc 0.68825\n",
      "| 16000/17167 batches | ms/batch 58.73 | loss 0.7336 acc 0.68159375\n",
      "| 17000/17167 batches | ms/batch 66.99 | loss 0.7313 acc 0.6830625\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   0 | time: 785.19s | valid loss  0.59 accuracy 0.7567567567567568 learning rates\n",
      "0.065\n",
      "-----------------------------------------------------------------------------------------\n",
      "Start epoch   0 training with lr 0.07 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brian/.conda/envs/pytorch/lib/python3.6/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type SiameseClassifier. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/brian/.conda/envs/pytorch/lib/python3.6/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type LinearClassifier. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|  1000/17167 batches | ms/batch 31.80 | loss 0.6433 acc 0.734125\n",
      "|  2000/17167 batches | ms/batch 34.43 | loss 0.6571 acc 0.7263125\n",
      "|  3000/17167 batches | ms/batch 35.93 | loss 0.6639 acc 0.7214375\n",
      "|  4000/17167 batches | ms/batch 37.48 | loss 0.6701 acc 0.7153125\n",
      "|  5000/17167 batches | ms/batch 38.58 | loss 0.6756 acc 0.71190625\n",
      "|  6000/17167 batches | ms/batch 39.67 | loss 0.6731 acc 0.71428125\n",
      "|  7000/17167 batches | ms/batch 40.76 | loss 0.6840 acc 0.7065625\n",
      "|  8000/17167 batches | ms/batch 42.11 | loss 0.6906 acc 0.70228125\n",
      "|  9000/17167 batches | ms/batch 43.17 | loss 0.7019 acc 0.698125\n",
      "| 10000/17167 batches | ms/batch 44.54 | loss 0.6953 acc 0.70228125\n",
      "| 11000/17167 batches | ms/batch 46.15 | loss 0.6930 acc 0.70128125\n",
      "| 12000/17167 batches | ms/batch 48.06 | loss 0.6954 acc 0.70153125\n",
      "| 13000/17167 batches | ms/batch 49.90 | loss 0.7024 acc 0.69659375\n",
      "| 14000/17167 batches | ms/batch 52.01 | loss 0.7072 acc 0.69478125\n",
      "| 15000/17167 batches | ms/batch 54.64 | loss 0.7147 acc 0.69215625\n",
      "| 16000/17167 batches | ms/batch 58.89 | loss 0.7209 acc 0.68596875\n",
      "| 17000/17167 batches | ms/batch 67.19 | loss 0.7231 acc 0.68609375\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   0 | time: 786.71s | valid loss  0.58 accuracy 0.7582808372282056 learning rates\n",
      "0.07\n",
      "-----------------------------------------------------------------------------------------\n",
      "Start epoch   0 training with lr 0.07500000000000001 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brian/.conda/envs/pytorch/lib/python3.6/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type SiameseClassifier. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/brian/.conda/envs/pytorch/lib/python3.6/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type LinearClassifier. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|  1000/17167 batches | ms/batch 31.88 | loss 0.6316 acc 0.74125\n",
      "|  2000/17167 batches | ms/batch 34.50 | loss 0.6469 acc 0.72909375\n",
      "|  3000/17167 batches | ms/batch 36.04 | loss 0.6581 acc 0.72325\n",
      "|  4000/17167 batches | ms/batch 37.50 | loss 0.6633 acc 0.71803125\n",
      "|  5000/17167 batches | ms/batch 38.59 | loss 0.6666 acc 0.71690625\n",
      "|  6000/17167 batches | ms/batch 39.66 | loss 0.6653 acc 0.71853125\n",
      "|  7000/17167 batches | ms/batch 40.73 | loss 0.6753 acc 0.7138125\n",
      "|  8000/17167 batches | ms/batch 42.08 | loss 0.6800 acc 0.70803125\n",
      "|  9000/17167 batches | ms/batch 43.17 | loss 0.6906 acc 0.70459375\n",
      "| 10000/17167 batches | ms/batch 44.55 | loss 0.6820 acc 0.70953125\n",
      "| 11000/17167 batches | ms/batch 46.17 | loss 0.6868 acc 0.70571875\n",
      "| 12000/17167 batches | ms/batch 48.08 | loss 0.6880 acc 0.70290625\n",
      "| 13000/17167 batches | ms/batch 49.82 | loss 0.6910 acc 0.700625\n",
      "| 14000/17167 batches | ms/batch 51.93 | loss 0.7071 acc 0.69203125\n",
      "| 15000/17167 batches | ms/batch 54.62 | loss 0.7034 acc 0.6981875\n",
      "| 16000/17167 batches | ms/batch 58.82 | loss 0.7138 acc 0.68984375\n",
      "| 17000/17167 batches | ms/batch 67.08 | loss 0.7146 acc 0.692125\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   0 | time: 786.58s | valid loss  0.58 accuracy 0.7654948181263971 learning rates\n",
      "0.07500000000000001\n",
      "-----------------------------------------------------------------------------------------\n",
      "Start epoch   0 training with lr 0.08 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brian/.conda/envs/pytorch/lib/python3.6/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type SiameseClassifier. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/brian/.conda/envs/pytorch/lib/python3.6/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type LinearClassifier. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|  1000/17167 batches | ms/batch 31.78 | loss 0.6249 acc 0.7436875\n",
      "|  2000/17167 batches | ms/batch 34.40 | loss 0.6366 acc 0.73615625\n",
      "|  3000/17167 batches | ms/batch 35.90 | loss 0.6475 acc 0.72853125\n",
      "|  4000/17167 batches | ms/batch 37.42 | loss 0.6575 acc 0.72421875\n",
      "|  5000/17167 batches | ms/batch 38.58 | loss 0.6573 acc 0.723875\n",
      "|  6000/17167 batches | ms/batch 39.71 | loss 0.6624 acc 0.7208125\n",
      "|  7000/17167 batches | ms/batch 40.74 | loss 0.6696 acc 0.7155\n",
      "|  8000/17167 batches | ms/batch 42.08 | loss 0.6777 acc 0.71203125\n",
      "|  9000/17167 batches | ms/batch 43.16 | loss 0.6805 acc 0.70803125\n",
      "| 10000/17167 batches | ms/batch 44.53 | loss 0.6761 acc 0.71084375\n",
      "| 11000/17167 batches | ms/batch 46.15 | loss 0.6816 acc 0.70621875\n",
      "| 12000/17167 batches | ms/batch 48.02 | loss 0.6791 acc 0.70628125\n",
      "| 13000/17167 batches | ms/batch 49.86 | loss 0.6898 acc 0.70384375\n",
      "| 14000/17167 batches | ms/batch 51.96 | loss 0.6902 acc 0.70471875\n",
      "| 15000/17167 batches | ms/batch 54.63 | loss 0.6989 acc 0.69775\n",
      "| 16000/17167 batches | ms/batch 58.87 | loss 0.7086 acc 0.69375\n",
      "| 17000/17167 batches | ms/batch 67.19 | loss 0.7070 acc 0.6953125\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   0 | time: 786.35s | valid loss  0.57 accuracy 0.7685429790692948 learning rates\n",
      "0.08\n",
      "-----------------------------------------------------------------------------------------\n",
      "Start epoch   0 training with lr 0.085 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brian/.conda/envs/pytorch/lib/python3.6/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type SiameseClassifier. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/brian/.conda/envs/pytorch/lib/python3.6/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type LinearClassifier. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|  1000/17167 batches | ms/batch 31.84 | loss 0.6176 acc 0.75025\n",
      "|  2000/17167 batches | ms/batch 34.43 | loss 0.6326 acc 0.73675\n",
      "|  3000/17167 batches | ms/batch 35.96 | loss 0.6454 acc 0.72890625\n",
      "|  4000/17167 batches | ms/batch 37.53 | loss 0.6525 acc 0.72409375\n",
      "|  5000/17167 batches | ms/batch 38.63 | loss 0.6476 acc 0.72646875\n",
      "|  6000/17167 batches | ms/batch 39.68 | loss 0.6534 acc 0.72621875\n",
      "|  7000/17167 batches | ms/batch 40.71 | loss 0.6629 acc 0.718125\n",
      "|  8000/17167 batches | ms/batch 42.04 | loss 0.6608 acc 0.7195625\n",
      "|  9000/17167 batches | ms/batch 43.09 | loss 0.6790 acc 0.70990625\n",
      "| 10000/17167 batches | ms/batch 44.48 | loss 0.6713 acc 0.7156875\n",
      "| 11000/17167 batches | ms/batch 46.04 | loss 0.6711 acc 0.7154375\n",
      "| 12000/17167 batches | ms/batch 47.95 | loss 0.6802 acc 0.7075625\n",
      "| 13000/17167 batches | ms/batch 49.76 | loss 0.6846 acc 0.7038125\n",
      "| 14000/17167 batches | ms/batch 51.86 | loss 0.6894 acc 0.70615625\n",
      "| 15000/17167 batches | ms/batch 54.56 | loss 0.6898 acc 0.7060625\n",
      "| 16000/17167 batches | ms/batch 58.75 | loss 0.7057 acc 0.69325\n",
      "| 17000/17167 batches | ms/batch 67.02 | loss 0.6980 acc 0.698625\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   0 | time: 785.66s | valid loss  0.56 accuracy 0.7698638488112173 learning rates\n",
      "0.085\n",
      "-----------------------------------------------------------------------------------------\n",
      "Start epoch   0 training with lr 0.09000000000000001 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brian/.conda/envs/pytorch/lib/python3.6/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type SiameseClassifier. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/brian/.conda/envs/pytorch/lib/python3.6/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type LinearClassifier. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|  1000/17167 batches | ms/batch 31.84 | loss 0.6093 acc 0.751625\n",
      "|  2000/17167 batches | ms/batch 34.45 | loss 0.6250 acc 0.7429375\n",
      "|  3000/17167 batches | ms/batch 35.92 | loss 0.6318 acc 0.7381875\n",
      "|  4000/17167 batches | ms/batch 37.40 | loss 0.6436 acc 0.72890625\n",
      "|  5000/17167 batches | ms/batch 38.61 | loss 0.6439 acc 0.72984375\n",
      "|  6000/17167 batches | ms/batch 39.67 | loss 0.6505 acc 0.725375\n",
      "|  7000/17167 batches | ms/batch 40.68 | loss 0.6518 acc 0.7213125\n",
      "|  8000/17167 batches | ms/batch 42.03 | loss 0.6568 acc 0.722\n",
      "|  9000/17167 batches | ms/batch 43.06 | loss 0.6683 acc 0.71659375\n",
      "| 10000/17167 batches | ms/batch 44.49 | loss 0.6661 acc 0.71665625\n",
      "| 11000/17167 batches | ms/batch 46.10 | loss 0.6668 acc 0.7134375\n",
      "| 12000/17167 batches | ms/batch 47.99 | loss 0.6685 acc 0.71315625\n",
      "| 13000/17167 batches | ms/batch 49.79 | loss 0.6764 acc 0.7089375\n",
      "| 14000/17167 batches | ms/batch 51.87 | loss 0.6822 acc 0.71028125\n",
      "| 15000/17167 batches | ms/batch 54.54 | loss 0.6800 acc 0.71171875\n",
      "| 16000/17167 batches | ms/batch 58.80 | loss 0.6988 acc 0.6991875\n",
      "| 17000/17167 batches | ms/batch 66.97 | loss 0.6948 acc 0.70034375\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   0 | time: 785.53s | valid loss  0.56 accuracy 0.7731152204836416 learning rates\n",
      "0.09000000000000001\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brian/.conda/envs/pytorch/lib/python3.6/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type SiameseClassifier. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/brian/.conda/envs/pytorch/lib/python3.6/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type LinearClassifier. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start epoch   0 training with lr 0.095 \n",
      "|  1000/17167 batches | ms/batch 31.84 | loss 0.6019 acc 0.7564375\n",
      "|  2000/17167 batches | ms/batch 34.48 | loss 0.6176 acc 0.74528125\n",
      "|  3000/17167 batches | ms/batch 35.99 | loss 0.6280 acc 0.73975\n",
      "|  4000/17167 batches | ms/batch 37.50 | loss 0.6356 acc 0.73359375\n",
      "|  5000/17167 batches | ms/batch 38.64 | loss 0.6400 acc 0.7308125\n",
      "|  6000/17167 batches | ms/batch 39.74 | loss 0.6421 acc 0.72890625\n",
      "|  7000/17167 batches | ms/batch 40.78 | loss 0.6529 acc 0.72290625\n",
      "|  8000/17167 batches | ms/batch 42.06 | loss 0.6518 acc 0.723875\n",
      "|  9000/17167 batches | ms/batch 43.15 | loss 0.6606 acc 0.7213125\n",
      "| 10000/17167 batches | ms/batch 44.51 | loss 0.6594 acc 0.720375\n",
      "| 11000/17167 batches | ms/batch 46.12 | loss 0.6646 acc 0.71621875\n",
      "| 12000/17167 batches | ms/batch 48.06 | loss 0.6632 acc 0.71684375\n",
      "| 13000/17167 batches | ms/batch 49.90 | loss 0.6731 acc 0.7095\n",
      "| 14000/17167 batches | ms/batch 51.97 | loss 0.6766 acc 0.7086875\n",
      "| 15000/17167 batches | ms/batch 54.66 | loss 0.6795 acc 0.7088125\n",
      "| 16000/17167 batches | ms/batch 58.85 | loss 0.6908 acc 0.70271875\n",
      "| 17000/17167 batches | ms/batch 67.13 | loss 0.6890 acc 0.70340625\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   0 | time: 786.72s | valid loss  0.55 accuracy 0.7795163584637269 learning rates\n",
      "0.095\n",
      "-----------------------------------------------------------------------------------------\n",
      "Start epoch   0 training with lr 0.1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brian/.conda/envs/pytorch/lib/python3.6/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type SiameseClassifier. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/brian/.conda/envs/pytorch/lib/python3.6/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type LinearClassifier. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|  1000/17167 batches | ms/batch 31.80 | loss 0.5994 acc 0.7574375\n",
      "|  2000/17167 batches | ms/batch 34.46 | loss 0.6143 acc 0.746875\n",
      "|  3000/17167 batches | ms/batch 35.99 | loss 0.6263 acc 0.736875\n",
      "|  4000/17167 batches | ms/batch 37.49 | loss 0.6307 acc 0.734625\n",
      "|  5000/17167 batches | ms/batch 38.63 | loss 0.6293 acc 0.73475\n",
      "|  6000/17167 batches | ms/batch 39.75 | loss 0.6369 acc 0.73346875\n",
      "|  7000/17167 batches | ms/batch 40.78 | loss 0.6405 acc 0.73003125\n",
      "|  8000/17167 batches | ms/batch 42.11 | loss 0.6458 acc 0.728\n",
      "|  9000/17167 batches | ms/batch 43.19 | loss 0.6592 acc 0.71953125\n",
      "| 10000/17167 batches | ms/batch 44.57 | loss 0.6534 acc 0.72303125\n",
      "| 11000/17167 batches | ms/batch 46.10 | loss 0.6585 acc 0.7195\n",
      "| 12000/17167 batches | ms/batch 48.06 | loss 0.6584 acc 0.72015625\n",
      "| 13000/17167 batches | ms/batch 49.90 | loss 0.6634 acc 0.7163125\n",
      "| 14000/17167 batches | ms/batch 52.01 | loss 0.6740 acc 0.71321875\n",
      "| 15000/17167 batches | ms/batch 54.68 | loss 0.6721 acc 0.71421875\n",
      "| 16000/17167 batches | ms/batch 58.94 | loss 0.6846 acc 0.706375\n",
      "| 17000/17167 batches | ms/batch 67.21 | loss 0.6857 acc 0.7043125\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   0 | time: 787.08s | valid loss  0.54 accuracy 0.7832757569599675 learning rates\n",
      "0.1\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brian/.conda/envs/pytorch/lib/python3.6/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type SiameseClassifier. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/brian/.conda/envs/pytorch/lib/python3.6/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type LinearClassifier. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "for param in siamese_model.encoder.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "for lr in [x/200+0.005 for x in range(20)]:\n",
    "    optimizer = optim.SGD(siamese_model.parameters(), lr=lr)\n",
    "    training_loop(siamese_model, 1, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start epoch   0 training with lr 0.1 \n",
      "|  1000/17167 batches | ms/batch 31.62 | loss 0.5024 acc 0.80209375\n",
      "|  2000/17167 batches | ms/batch 34.31 | loss 0.5148 acc 0.79321875\n",
      "|  3000/17167 batches | ms/batch 35.82 | loss 0.5347 acc 0.78534375\n",
      "|  4000/17167 batches | ms/batch 37.30 | loss 0.5352 acc 0.78334375\n",
      "|  5000/17167 batches | ms/batch 38.46 | loss 0.5359 acc 0.78396875\n",
      "|  6000/17167 batches | ms/batch 39.57 | loss 0.5396 acc 0.78053125\n",
      "|  7000/17167 batches | ms/batch 40.60 | loss 0.5543 acc 0.7726875\n",
      "|  8000/17167 batches | ms/batch 41.96 | loss 0.5542 acc 0.77225\n",
      "|  9000/17167 batches | ms/batch 43.04 | loss 0.5666 acc 0.767\n",
      "| 10000/17167 batches | ms/batch 44.45 | loss 0.5545 acc 0.77190625\n",
      "| 11000/17167 batches | ms/batch 46.04 | loss 0.5641 acc 0.7680625\n",
      "| 12000/17167 batches | ms/batch 47.98 | loss 0.5672 acc 0.7654375\n",
      "| 13000/17167 batches | ms/batch 49.82 | loss 0.5715 acc 0.764375\n",
      "| 14000/17167 batches | ms/batch 51.94 | loss 0.5792 acc 0.76028125\n",
      "| 15000/17167 batches | ms/batch 54.63 | loss 0.5800 acc 0.76203125\n",
      "| 16000/17167 batches | ms/batch 58.83 | loss 0.5850 acc 0.7575625\n",
      "| 17000/17167 batches | ms/batch 67.04 | loss 0.5867 acc 0.75721875\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   0 | time: 784.77s | valid loss  0.50 accuracy 0.8068482015850437 learning rates\n",
      "0.1\n",
      "-----------------------------------------------------------------------------------------\n",
      "Start epoch   1 training with lr 0.1 \n",
      "|  1000/17167 batches | ms/batch 31.86 | loss 0.5083 acc 0.8019375\n",
      "|  2000/17167 batches | ms/batch 34.51 | loss 0.5288 acc 0.78878125\n",
      "|  3000/17167 batches | ms/batch 36.02 | loss 0.5295 acc 0.786375\n",
      "|  4000/17167 batches | ms/batch 37.52 | loss 0.5366 acc 0.7811875\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "optimizer = optim.SGD(siamese_model.parameters(), lr=0.1)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs, eta_min=0.001)\n",
    "training_loop(siamese_model, epochs, optimizer, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "entailed_a = []\n",
    "entailed_b = []\n",
    "contra_a = []\n",
    "contra_b = []\n",
    "netural_a = []\n",
    "netural_b = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def list2arr(l):\n",
    "    \"Convert list into pytorch Variable.\"\n",
    "    return V(np.expand_dims(np.array(l), -1))\n",
    "\n",
    "def make_prediction_from_list(model, l):\n",
    "    \"\"\"\n",
    "    Encode a list of integers that represent a sequence of tokens.  The\n",
    "    purpose is to encode a sentence or phrase.\n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "    model : fastai language model\n",
    "    l : list\n",
    "        list of integers, representing a sequence of tokens that you want to encode`\n",
    "\n",
    "    \"\"\"\n",
    "    arr = list2arr(l)# turn list into pytorch Variable with bs=1\n",
    "    model.reset()  # language model is stateful, so you must reset upon each prediction\n",
    "    hidden_states = model(arr)[-1][-1] # RNN Hidden Layer output is last output, and only need the last layer\n",
    "\n",
    "    #return avg-pooling, max-pooling, and last hidden state\n",
    "    return hidden_states.mean(0), hidden_states.max(0)[0], hidden_states[-1]\n",
    "\n",
    "\n",
    "def get_embeddings(encoder, list_list_int):\n",
    "    \"\"\"\n",
    "    Vectorize a list of sequences List[List[int]] using a fast.ai language model.\n",
    "\n",
    "    Paramters\n",
    "    ---------\n",
    "    encoder : sentence_encoder\n",
    "    list_list_int : List[List[int]]\n",
    "        A list of sequences to encode\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple: (avg, mean, last)\n",
    "        A tuple that returns the average-pooling, max-pooling over time steps as well as the last time step.\n",
    "    \"\"\"\n",
    "    n_rows = len(list_list_int)\n",
    "    n_dim = encoder.nhid\n",
    "    avgarr = np.empty((n_rows, n_dim))\n",
    "    maxarr = np.empty((n_rows, n_dim))\n",
    "    lastarr = np.empty((n_rows, n_dim))\n",
    "\n",
    "    for i in tqdm_notebook(range(len(list_list_int))):\n",
    "        avg_, max_, last_ = make_prediction_from_list(encoder, list_list_int[i])\n",
    "        avgarr[i,:] = avg_.data.cpu().numpy()\n",
    "        maxarr[i,:] = max_.data.cpu().numpy()\n",
    "        lastarr[i,:] = last_.data.cpu().numpy()\n",
    "\n",
    "    return avgarr, maxarr, lastarr\n",
    "\n",
    "def encode_sentences(sentences, encoding_dict):\n",
    "    sentences_enc = []\n",
    "    for sent in sentences:\n",
    "        sent_enc = []\n",
    "        for word in f'{BOS} ' +fixup(sent):\n",
    "            sent_enc.append(encoding_dict[word])\n",
    "        sentences_enc.append(sent_enc)\n",
    "        \n",
    "    return sentences_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_model = torch.load('siamese_model0.73.pt')\n",
    "siamese_model.encoder.nhid = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entailed_a_vec = get_embeddings(siamese_model.encoder, encode_sentences(entailed_a, stoi))\n",
    "entailed_b_vec = get_embeddings(siamese_model.encoder, encode_sentences(entailed_b, stoi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nmslib\n",
    "\n",
    "def create_nmslib_search_index(numpy_vectors):\n",
    "    \"\"\"Create search index using nmslib.\n",
    "    Parameters\n",
    "    ==========\n",
    "    numpy_vectors : numpy.array\n",
    "        The matrix of vectors\n",
    "    Returns\n",
    "    =======\n",
    "    nmslib object that has index of numpy_vectors\n",
    "    \"\"\"\n",
    "\n",
    "    search_index = nmslib.init(method='hnsw', space='cosinesimil')\n",
    "    search_index.addDataPointBatch(numpy_vectors)\n",
    "    search_index.createIndex({'post': 2}, print_progress=True)\n",
    "    return search_index\n",
    "\n",
    "def percent_matching(query_vec, searchindex, k=10):\n",
    "    num_found = 0\n",
    "    num_total = len(query_vec)\n",
    "    for i in range(num_total):\n",
    "        query = query_vec[i]\n",
    "        idxs, dists = searchindex.knnQuery(query, k=k)\n",
    "        if i in idxs:\n",
    "            num_found += 1\n",
    "\n",
    "    return 100 * num_found/num_total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entailed_b_max_searchindex = create_nmslib_search_index(entailed_b_vec[0])\n",
    "percent_matching(entailed_a_vec[0], entailed_b_max_searchindex, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entailed_b_max_searchindex = create_nmslib_search_index(entailed_b_vec[1])\n",
    "percent_matching(entailed_a_vec[1], entailed_b_max_searchindex, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entailed_b_last_searchindex = create_nmslib_search_index(entailed_b_vec[2])\n",
    "percent_matching(entailed_a_vec[2], entailed_b_last_searchindex, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entailed_a_concat = np.concatenate((entailed_a_vec[0], entailed_a_vec[1], entailed_a_vec[2]), axis=1)\n",
    "entailed_b_concat = np.concatenate((entailed_b_vec[0], entailed_b_vec[1], entailed_b_vec[2]), axis=1)\n",
    "entailed_b_concat_searchindex = create_nmslib_search_index(entailed_b_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_matching(entailed_a_concat, entailed_b_concat_searchindex, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_similar(query, query_idx, searchindex):\n",
    "    idx, _ = searchindex.knnQuery(query[query_idx], k=10)\n",
    "    matched = []\n",
    "    for i in idx:\n",
    "        matched.append(entailed_b[i])\n",
    "    \n",
    "    match = query_idx in idx\n",
    "    \n",
    "    return match, entailed_a[query_idx], entailed_b[query_idx], matched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    results = show_similar(entailed_a_vec[1], i, entailed_b_max_searchindex)\n",
    "    if results[0]:\n",
    "        print(results[1])\n",
    "        print(results[2])\n",
    "        for result in results[3]:\n",
    "            print(\"\\t\"+result)\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "_draft": {
   "nbviewer_url": "https://gist.github.com/0dd0df21cf404cf2bb51d0148c8b7d8b"
  },
  "gist": {
   "data": {
    "description": "fastai.text imdb example",
    "public": true
   },
   "id": "0dd0df21cf404cf2bb51d0148c8b7d8b"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "86px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
