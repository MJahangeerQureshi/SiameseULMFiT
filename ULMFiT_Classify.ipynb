{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ULMFiT + Siamese Network for Sentence Vectors\n",
    "## Part Three: Classifying\n",
    "\n",
    "The second notebook created a new language model from the SNLI dataset.\n",
    "This notebook will adapt that model to predicting the SNLI category for sentence pairs.\n",
    "The model will be used as a sentence encoder for a Siamese Network that builds sentence vectors that are feed into a classifier network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text import *\n",
    "import html\n",
    "\n",
    "import json\n",
    "import html\n",
    "import re\n",
    "import pickle\n",
    "from collections import Counter\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import sklearn\n",
    "from sklearn import model_selection\n",
    "from functools import partial\n",
    "from collections import Counter, defaultdict\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils \n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import dataset, dataloader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import time\n",
    "import math\n",
    "import sys\n",
    "import data\n",
    "\n",
    "snli_root = './data/snli_1.0/'\n",
    "token_files = './data/tokens/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34155"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load the tokens\n",
    "itos = pickle.load(open(f'{token_files}itos.pkl', 'rb'))\n",
    "\n",
    "stoi = defaultdict(lambda:0, {v:k for k,v in enumerate(itos)})\n",
    "vocab_size = len(itos)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a new dataloader to create sentence pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "class Entail(Enum):\n",
    "    entailment = 0\n",
    "    contradiction = 1\n",
    "    neutral = 2\n",
    "       \n",
    "class SiameseDataset(dataset.Dataset):\n",
    "    def __init__(self, json_file):\n",
    "        \n",
    "        content = []\n",
    "        with open(json_file) as fp:\n",
    "            while True:\n",
    "                line = fp.readline()\n",
    "                if line:\n",
    "                    content.append(json.loads(line))\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "        self.items = []\n",
    "        for item in content:\n",
    "            l = item['gold_label']\n",
    "            s0 = item['sentence1']\n",
    "            s1 = item['sentence2']\n",
    "            \n",
    "            average_len = (len(s0)+len(s1))/2\n",
    "            try:\n",
    "                label = Entail[l].value\n",
    "                self.items.append((s0, s1, label, average_len))\n",
    "            except KeyError:\n",
    "                pass\n",
    "            \n",
    "    def shuffle(self):\n",
    "        self.items.sort(key=lambda x: x[3]+random.randint(-5, 5))\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.items[index]\n",
    "       \n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "\n",
    "    \n",
    "class SiameseDataLoader():\n",
    "    def __init__(self, dataset, stoi, pad_val, batch_size=32):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.stoi = stoi\n",
    "        self.index = 0\n",
    "        self.pad_val = pad_val\n",
    "      \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def fill_tensor(self, sentences, max_len):\n",
    "        data = np.zeros((max_len, len(sentences)), dtype=np.long)\n",
    "        data.fill(self.pad_val)\n",
    "        \n",
    "        for i, s in enumerate(sentences): \n",
    "            start_idx = max_len - len(s)\n",
    "            for j, p in enumerate(s):\n",
    "                data[:,i][start_idx+j] = stoi[p]\n",
    "            \n",
    "        return torch.LongTensor([data.tolist()]).cuda()\n",
    "     \n",
    "    def batch(self):\n",
    "        return self.index//self.batch_size\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)//self.batch_size\n",
    "    \n",
    "    def __next__(self):\n",
    "        #how many examples to ananlyise for this round\n",
    "        num = min(self.batch_size, len(self.dataset) - self.index)\n",
    "        \n",
    "        if num < 1:\n",
    "            raise StopIteration  # signals \"the end\"\n",
    "            \n",
    "        #collect the sentences\n",
    "        max_len = 0\n",
    "        first = []\n",
    "        second = []\n",
    "        labels = torch.LongTensor(num)\n",
    "        \n",
    "        for i in range(num):\n",
    "            a, b, l, _ = self.dataset[self.index + i]\n",
    "            \n",
    "            if len(a) > max_len:\n",
    "                max_len = len(a)\n",
    "            \n",
    "            if len(b) > max_len:\n",
    "                max_len = len(b)\n",
    "            \n",
    "            first.append(a)\n",
    "            second.append(b)\n",
    "            labels[i] = l\n",
    "            \n",
    "        self.index += num\n",
    "             \n",
    "        return (self.fill_tensor(first, max_len).cuda(),\n",
    "                self.fill_tensor(second, max_len).cuda(),\n",
    "                labels.cuda()\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_dataset_train = SiameseDataset(f'{snli_root}/snli_1.0_train.jsonl')\n",
    "siamese_dataset_dev = SiameseDataset(f'{snli_root}snli_1.0_dev.jsonl')\n",
    "siamese_dataset_test = SiameseDataset(f'{snli_root}snli_1.0_test.jsonl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Siamese network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, encoder, linear):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.linear = linear\n",
    "    \n",
    "    def pool(self, x, bs, is_max):\n",
    "        f = F.adaptive_max_pool1d if is_max else F.adaptive_avg_pool1d\n",
    "        return f(x.permute(1,2,0), (1,)).view(bs,-1)\n",
    "\n",
    "    def pool_outputs(self, output):\n",
    "        sl, bs,_ = output.size()\n",
    "        avgpool = self.pool(output, bs, False)\n",
    "        maxpool = self.pool(output, bs, True)\n",
    "        return torch.cat([output[-1], maxpool, avgpool], 1)\n",
    "        \n",
    "    def forward_once(self, input):\n",
    "        raw_outputs, outputs = self.encoder(input)\n",
    "        out = self.pool_outputs(outputs[-1])\n",
    "        return out\n",
    "    \n",
    "    def forward(self, in1, in2):\n",
    "        u = self.forward_once(in1)\n",
    "        v = self.forward_once(in2)\n",
    "        features = torch.cat((u, v, torch.abs(u-v), u*v), 1)\n",
    "        out = self.linear(features)\n",
    "        return out \n",
    "        \n",
    "    def reset(self):\n",
    "        for c in self.children():\n",
    "            if hasattr(c, 'reset'): c.reset()\n",
    "\n",
    "class LinearClassifier(nn.Module):\n",
    "    def __init__(self, layers, dropout):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([LinearBlock(layers[i], layers[i + 1], dropout) for i in range(len(layers) - 1)])\n",
    "        \n",
    "    def forward(self, input):\n",
    "        x = input\n",
    "        for l in self.layers:\n",
    "            l_x = l(x)\n",
    "            x = F.relu(l_x)\n",
    "        return l_x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#these are the values used for the original LM\n",
    "em_sz, nh, nl = 400, 1150, 3\n",
    "bptt = 70\n",
    "max_seq = bptt * 20\n",
    "cats = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load our pretrained model then build the Siamese network from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SNLI_LM = torch.load(\"SNLI_LM.pt\")\n",
    "\n",
    "dps = np.array([0.4,0.5,0.05,0.3,0.4])*0.1\n",
    "SNLI_encoder = MultiBatchRNN(bptt, max_seq, vocab_size, em_sz, nh, nl, stoi[\"_pad_\"], dropouti=dps[0], wdrop=dps[2], dropoute=dps[3], dropouth=dps[4])\n",
    "\n",
    "SNLI_encoder.load_state_dict(SNLI_LM[0].state_dict())\n",
    "\n",
    "#2 pooled vectors, of 3 times the embedding size\n",
    "siamese_model = SiameseClassifier(SNLI_encoder, LinearClassifier(layers=[em_sz*3*4, nh, em_sz], dropout=0.1)).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop\n",
    "This should be converted over to the fast.ai learner but I'm not sure how to do that yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_interval = 1000\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#criterion = nn.CosineEmbeddingLoss()\n",
    "\n",
    "def evaluate(model, data_loader):\n",
    "    # Turn on evaluation mode which disables dropout.\n",
    "    model.eval()\n",
    "    \n",
    "    total_loss = 0.\n",
    "    num_correct = 0\n",
    "    total = 0 \n",
    "    \n",
    "    for a, b, l in data_loader:\n",
    "        \n",
    "        model.reset()\n",
    "        a, b, l = Variable(a), Variable(b), Variable(l)\n",
    "        out = model(a.squeeze(), b.squeeze())\n",
    "        loss = criterion(out, l.squeeze())\n",
    "        total += l.size(0)\n",
    "        total_loss += l.size(0) * loss.item()\n",
    "        num_correct += np.sum(l.data.cpu().numpy() == np.argmax(out.data.cpu().numpy(), 1))\n",
    "        \n",
    "    return (total_loss / total, num_correct / total)\n",
    "\n",
    "def train(model, data_loader, optimizer):\n",
    "    # Turn on training mode which enables dropout.\n",
    "    start_time = time.time()\n",
    "    model.train() \n",
    "    \n",
    "    total_loss = 0.\n",
    "    num_correct = 0\n",
    "    total = 0 \n",
    "        \n",
    "    for a, b, l in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        model.reset()\n",
    "        a, b, l = Variable(a), Variable(b), Variable(l)\n",
    "        out = model(a.squeeze(), b.squeeze())\n",
    "        loss = criterion(out, l.squeeze())\n",
    "        total += l.size(0)\n",
    "        total_loss += l.size(0) * loss.item()\n",
    "        num_correct += np.sum(l.data.cpu().numpy() == np.argmax(out.data.cpu().numpy(), 1))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch = data_loader.batch()\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / total\n",
    "            elapsed = time.time() - start_time\n",
    "            batches = len(data_loader)\n",
    "            ms = elapsed * 1000 / log_interval\n",
    "            print(f'| {batch:5d}/{batches:5d} batches', end=\" \")\n",
    "            print(f'| ms/batch {ms:5.2f} | loss {cur_loss:5.4f} acc {num_correct / total}')\n",
    "            #print(f'| ms/batch {ms:5.2f} | loss {cur_loss:5.4f}')\n",
    "            total_loss = 0\n",
    "            total = 0\n",
    "            num_correct = 0\n",
    "            start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_accuracy = 0.72\n",
    "def training_loop(lrs, model):\n",
    "    global best_accuracy\n",
    "    for epoch, lr in enumerate(lrs):\n",
    "\n",
    "        print(f'Start epoch {epoch:3d} training with lr {lr}')\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "        siamese_dataset_train.shuffle()\n",
    "        training_data = SiameseDataLoader(siamese_dataset_train, stoi, stoi[\"_pad_\"], batch_size=24)\n",
    "\n",
    "        epoch_start_time = time.time()\n",
    "        train(siamese_model, training_data, optimizer)\n",
    "\n",
    "        validation_data = SiameseDataLoader(siamese_dataset_test , stoi, stoi[\"_pad_\"], batch_size=24)\n",
    "        val_loss, accuracy = evaluate(siamese_model, validation_data)\n",
    "\n",
    "        delta_t = (time.time() - epoch_start_time)\n",
    "        print('-' * 89)\n",
    "        print(f'| end of epoch {epoch:3d} | time: {delta_t:5.2f}s | valid loss {val_loss:5.2f} accuracy {accuracy} learning rate {lr}')\n",
    "        print('-' * 89)\n",
    "\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            with open(f'./siamese_model{best_accuracy:0.2f}.pt', 'wb') as f:\n",
    "                torch.save(siamese_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in siamese_model.encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "training_loop([0.05], siamese_model.linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start epoch   0 training with lr 0.08\n",
      "|  1000/22890 batches | ms/batch 68.93 | loss 0.5526 acc 0.77625\n",
      "|  2000/22890 batches | ms/batch 78.50 | loss 0.5852 acc 0.7637083333333333\n",
      "|  3000/22890 batches | ms/batch 84.69 | loss 0.6163 acc 0.7449583333333333\n",
      "|  4000/22890 batches | ms/batch 89.33 | loss 0.6140 acc 0.74525\n",
      "|  5000/22890 batches | ms/batch 93.95 | loss 0.6248 acc 0.7397916666666666\n",
      "|  6000/22890 batches | ms/batch 97.44 | loss 0.6275 acc 0.7377083333333333\n",
      "|  7000/22890 batches | ms/batch 104.01 | loss 0.6337 acc 0.7340833333333333\n",
      "|  8000/22890 batches | ms/batch 109.99 | loss 0.6450 acc 0.7255416666666666\n",
      "|  9000/22890 batches | ms/batch 115.36 | loss 0.6507 acc 0.726625\n",
      "| 10000/22890 batches | ms/batch 121.93 | loss 0.6562 acc 0.723\n",
      "| 11000/22890 batches | ms/batch 128.66 | loss 0.6601 acc 0.7204583333333333\n",
      "| 12000/22890 batches | ms/batch 133.05 | loss 0.6695 acc 0.7187083333333333\n",
      "| 13000/22890 batches | ms/batch 139.09 | loss 0.6720 acc 0.7139166666666666\n",
      "| 14000/22890 batches | ms/batch 144.54 | loss 0.6760 acc 0.7112916666666667\n",
      "| 15000/22890 batches | ms/batch 149.62 | loss 0.6666 acc 0.7172083333333333\n",
      "| 16000/22890 batches | ms/batch 154.72 | loss 0.6826 acc 0.7096666666666667\n",
      "| 17000/22890 batches | ms/batch 161.53 | loss 0.6817 acc 0.70975\n",
      "| 18000/22890 batches | ms/batch 167.89 | loss 0.6856 acc 0.707625\n",
      "| 19000/22890 batches | ms/batch 176.54 | loss 0.6974 acc 0.70175\n",
      "| 20000/22890 batches | ms/batch 186.50 | loss 0.6955 acc 0.702\n",
      "| 21000/22890 batches | ms/batch 200.87 | loss 0.7116 acc 0.6940833333333334\n",
      "| 22000/22890 batches | ms/batch 230.85 | loss 0.7041 acc 0.7016666666666667\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   0 | time: 3232.12s | valid loss  0.75 accuracy 0.7292345276872965 learning rate 0.08\n",
      "-----------------------------------------------------------------------------------------\n",
      "Start epoch   1 training with lr 0.075\n",
      "|  1000/22890 batches | ms/batch 69.19 | loss 0.5746 acc 0.7680416666666666\n",
      "|  2000/22890 batches | ms/batch 78.49 | loss 0.5918 acc 0.7594166666666666\n",
      "|  3000/22890 batches | ms/batch 84.36 | loss 0.6121 acc 0.7474583333333333\n",
      "|  4000/22890 batches | ms/batch 88.61 | loss 0.6079 acc 0.7505833333333334\n",
      "|  5000/22890 batches | ms/batch 93.81 | loss 0.6170 acc 0.7450833333333333\n",
      "|  6000/22890 batches | ms/batch 97.50 | loss 0.6287 acc 0.7407083333333333\n",
      "|  7000/22890 batches | ms/batch 104.15 | loss 0.6313 acc 0.7364166666666667\n",
      "|  8000/22890 batches | ms/batch 109.89 | loss 0.6353 acc 0.7332916666666667\n",
      "|  9000/22890 batches | ms/batch 114.63 | loss 0.6479 acc 0.7269583333333334\n",
      "| 10000/22890 batches | ms/batch 121.54 | loss 0.6500 acc 0.72625\n",
      "| 11000/22890 batches | ms/batch 128.55 | loss 0.6491 acc 0.72525\n",
      "| 12000/22890 batches | ms/batch 133.00 | loss 0.6646 acc 0.7227083333333333\n",
      "| 13000/22890 batches | ms/batch 139.49 | loss 0.6584 acc 0.7182916666666667\n",
      "| 14000/22890 batches | ms/batch 144.33 | loss 0.6605 acc 0.719\n",
      "| 15000/22890 batches | ms/batch 149.86 | loss 0.6633 acc 0.7195833333333334\n",
      "| 16000/22890 batches | ms/batch 155.47 | loss 0.6661 acc 0.7157083333333333\n",
      "| 17000/22890 batches | ms/batch 161.55 | loss 0.6690 acc 0.7141666666666666\n",
      "| 18000/22890 batches | ms/batch 168.18 | loss 0.6764 acc 0.7145\n",
      "| 19000/22890 batches | ms/batch 176.57 | loss 0.6874 acc 0.7045416666666666\n",
      "| 20000/22890 batches | ms/batch 186.32 | loss 0.6859 acc 0.7062083333333333\n",
      "| 21000/22890 batches | ms/batch 200.45 | loss 0.6945 acc 0.7022916666666666\n",
      "| 22000/22890 batches | ms/batch 231.16 | loss 0.7065 acc 0.6940833333333334\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 3232.50s | valid loss  0.76 accuracy 0.7278094462540716 learning rate 0.075\n",
      "-----------------------------------------------------------------------------------------\n",
      "Start epoch   2 training with lr 0.07\n",
      "|  1000/22890 batches | ms/batch 69.37 | loss 0.5646 acc 0.7755416666666667\n",
      "|  2000/22890 batches | ms/batch 78.61 | loss 0.5814 acc 0.7652916666666667\n",
      "|  3000/22890 batches | ms/batch 84.54 | loss 0.6038 acc 0.7512083333333334\n",
      "|  4000/22890 batches | ms/batch 88.89 | loss 0.6056 acc 0.7480416666666667\n",
      "|  5000/22890 batches | ms/batch 93.90 | loss 0.6234 acc 0.740375\n",
      "|  6000/22890 batches | ms/batch 97.14 | loss 0.6189 acc 0.7428333333333333\n",
      "|  7000/22890 batches | ms/batch 104.26 | loss 0.6284 acc 0.7409583333333334\n",
      "|  8000/22890 batches | ms/batch 109.52 | loss 0.6215 acc 0.7397916666666666\n",
      "|  9000/22890 batches | ms/batch 115.14 | loss 0.6378 acc 0.7320833333333333\n",
      "| 10000/22890 batches | ms/batch 121.54 | loss 0.6282 acc 0.7374583333333333\n",
      "| 11000/22890 batches | ms/batch 128.20 | loss 0.6486 acc 0.7262916666666667\n",
      "| 12000/22890 batches | ms/batch 133.09 | loss 0.6511 acc 0.7238333333333333\n",
      "| 13000/22890 batches | ms/batch 139.04 | loss 0.6424 acc 0.7272916666666667\n",
      "| 14000/22890 batches | ms/batch 143.76 | loss 0.6551 acc 0.718625\n",
      "| 15000/22890 batches | ms/batch 149.74 | loss 0.6614 acc 0.71875\n",
      "| 16000/22890 batches | ms/batch 154.93 | loss 0.6502 acc 0.7275833333333334\n",
      "| 17000/22890 batches | ms/batch 161.72 | loss 0.6683 acc 0.715625\n",
      "| 18000/22890 batches | ms/batch 167.98 | loss 0.6627 acc 0.7210833333333333\n",
      "| 19000/22890 batches | ms/batch 175.99 | loss 0.6763 acc 0.71225\n",
      "| 20000/22890 batches | ms/batch 186.38 | loss 0.6813 acc 0.7071666666666667\n",
      "| 21000/22890 batches | ms/batch 200.12 | loss 0.6910 acc 0.7040416666666667\n",
      "| 22000/22890 batches | ms/batch 230.76 | loss 0.6953 acc 0.7\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 3229.50s | valid loss  1.21 accuracy 0.7150855048859935 learning rate 0.07\n",
      "-----------------------------------------------------------------------------------------\n",
      "Start epoch   3 training with lr 0.065\n",
      "|  1000/22890 batches | ms/batch 69.39 | loss 0.5642 acc 0.7742083333333334\n",
      "|  2000/22890 batches | ms/batch 78.63 | loss 0.5834 acc 0.7607083333333333\n",
      "|  3000/22890 batches | ms/batch 84.78 | loss 0.5884 acc 0.7568333333333334\n",
      "|  4000/22890 batches | ms/batch 88.83 | loss 0.6021 acc 0.7497083333333333\n",
      "|  5000/22890 batches | ms/batch 94.19 | loss 0.6136 acc 0.74675\n",
      "|  6000/22890 batches | ms/batch 97.12 | loss 0.6086 acc 0.7468333333333333\n",
      "|  7000/22890 batches | ms/batch 103.73 | loss 0.6129 acc 0.74175\n",
      "|  8000/22890 batches | ms/batch 109.61 | loss 0.6198 acc 0.7419583333333334\n",
      "|  9000/22890 batches | ms/batch 114.68 | loss 0.6359 acc 0.7310416666666667\n",
      "| 10000/22890 batches | ms/batch 122.08 | loss 0.6298 acc 0.7373333333333333\n",
      "| 11000/22890 batches | ms/batch 128.07 | loss 0.6330 acc 0.735875\n",
      "| 12000/22890 batches | ms/batch 132.65 | loss 0.6340 acc 0.7343333333333333\n",
      "| 13000/22890 batches | ms/batch 138.66 | loss 0.6435 acc 0.72675\n",
      "| 14000/22890 batches | ms/batch 144.22 | loss 0.6384 acc 0.7313333333333333\n",
      "| 15000/22890 batches | ms/batch 149.40 | loss 0.6515 acc 0.7265833333333334\n",
      "| 16000/22890 batches | ms/batch 155.62 | loss 0.6568 acc 0.7209583333333334\n",
      "| 17000/22890 batches | ms/batch 161.89 | loss 0.6528 acc 0.7264166666666667\n",
      "| 18000/22890 batches | ms/batch 168.43 | loss 0.6628 acc 0.7185\n",
      "| 19000/22890 batches | ms/batch 176.70 | loss 0.6575 acc 0.7216666666666667\n",
      "| 20000/22890 batches | ms/batch 186.71 | loss 0.6726 acc 0.7135833333333333\n",
      "| 21000/22890 batches | ms/batch 200.59 | loss 0.6680 acc 0.714625\n",
      "| 22000/22890 batches | ms/batch 230.52 | loss 0.6830 acc 0.707875\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 3231.05s | valid loss  1.22 accuracy 0.720378664495114 learning rate 0.065\n",
      "-----------------------------------------------------------------------------------------\n",
      "Start epoch   4 training with lr 0.06\n",
      "|  1000/22890 batches | ms/batch 69.49 | loss 0.5574 acc 0.7765833333333333\n",
      "|  2000/22890 batches | ms/batch 78.86 | loss 0.5771 acc 0.7651666666666667\n",
      "|  3000/22890 batches | ms/batch 84.75 | loss 0.5906 acc 0.7597916666666666\n",
      "|  4000/22890 batches | ms/batch 89.03 | loss 0.5827 acc 0.7629583333333333\n",
      "|  5000/22890 batches | ms/batch 93.83 | loss 0.5991 acc 0.7546666666666667\n",
      "|  6000/22890 batches | ms/batch 97.36 | loss 0.5956 acc 0.75025\n",
      "|  7000/22890 batches | ms/batch 103.63 | loss 0.6079 acc 0.7505\n",
      "|  8000/22890 batches | ms/batch 110.02 | loss 0.6187 acc 0.74425\n",
      "|  9000/22890 batches | ms/batch 114.76 | loss 0.6150 acc 0.7452083333333334\n",
      "| 10000/22890 batches | ms/batch 121.77 | loss 0.6192 acc 0.7404166666666666\n",
      "| 11000/22890 batches | ms/batch 127.94 | loss 0.6286 acc 0.7355416666666666\n",
      "| 12000/22890 batches | ms/batch 133.04 | loss 0.6306 acc 0.737\n",
      "| 13000/22890 batches | ms/batch 138.69 | loss 0.6418 acc 0.73\n",
      "| 14000/22890 batches | ms/batch 144.29 | loss 0.6378 acc 0.7336666666666667\n",
      "| 15000/22890 batches | ms/batch 149.64 | loss 0.6352 acc 0.7279166666666667\n",
      "| 16000/22890 batches | ms/batch 154.80 | loss 0.6395 acc 0.7305833333333334\n",
      "| 17000/22890 batches | ms/batch 161.30 | loss 0.6415 acc 0.73375\n",
      "| 18000/22890 batches | ms/batch 167.51 | loss 0.6439 acc 0.7279583333333334\n",
      "| 19000/22890 batches | ms/batch 176.10 | loss 0.6615 acc 0.7192083333333333\n",
      "| 20000/22890 batches | ms/batch 186.18 | loss 0.6621 acc 0.7201666666666666\n",
      "| 21000/22890 batches | ms/batch 200.76 | loss 0.6626 acc 0.7172083333333333\n",
      "| 22000/22890 batches | ms/batch 230.47 | loss 0.6709 acc 0.713375\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 3228.74s | valid loss  1.46 accuracy 0.7178338762214984 learning rate 0.06\n",
      "-----------------------------------------------------------------------------------------\n",
      "Start epoch   5 training with lr 0.055\n",
      "|  1000/22890 batches | ms/batch 69.29 | loss 0.5472 acc 0.7802916666666667\n",
      "|  2000/22890 batches | ms/batch 78.66 | loss 0.5634 acc 0.7725\n",
      "|  3000/22890 batches | ms/batch 84.45 | loss 0.5775 acc 0.7656666666666667\n",
      "|  4000/22890 batches | ms/batch 88.96 | loss 0.5812 acc 0.763625\n",
      "|  5000/22890 batches | ms/batch 93.52 | loss 0.5877 acc 0.7597083333333333\n",
      "|  6000/22890 batches | ms/batch 96.95 | loss 0.5933 acc 0.7551666666666667\n",
      "|  7000/22890 batches | ms/batch 103.81 | loss 0.5978 acc 0.754375\n",
      "|  8000/22890 batches | ms/batch 109.26 | loss 0.6037 acc 0.7477083333333333\n",
      "|  9000/22890 batches | ms/batch 114.92 | loss 0.6084 acc 0.747\n",
      "| 10000/22890 batches | ms/batch 121.91 | loss 0.6127 acc 0.7483333333333333\n",
      "| 11000/22890 batches | ms/batch 127.93 | loss 0.6198 acc 0.739375\n",
      "| 12000/22890 batches | ms/batch 133.08 | loss 0.6164 acc 0.742125\n",
      "| 13000/22890 batches | ms/batch 139.49 | loss 0.6243 acc 0.740625\n",
      "| 14000/22890 batches | ms/batch 144.29 | loss 0.6362 acc 0.73325\n",
      "| 15000/22890 batches | ms/batch 149.45 | loss 0.6245 acc 0.735625\n",
      "| 16000/22890 batches | ms/batch 155.19 | loss 0.6362 acc 0.7304583333333333\n",
      "| 17000/22890 batches | ms/batch 161.14 | loss 0.6362 acc 0.731\n",
      "| 18000/22890 batches | ms/batch 167.69 | loss 0.6319 acc 0.7347083333333333\n",
      "| 19000/22890 batches | ms/batch 176.17 | loss 0.6486 acc 0.725625\n",
      "| 20000/22890 batches | ms/batch 186.09 | loss 0.6516 acc 0.7229166666666667\n",
      "| 21000/22890 batches | ms/batch 200.40 | loss 0.6558 acc 0.7216666666666667\n",
      "| 22000/22890 batches | ms/batch 229.81 | loss 0.6733 acc 0.712875\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 3227.22s | valid loss  1.42 accuracy 0.7127442996742671 learning rate 0.055\n",
      "-----------------------------------------------------------------------------------------\n",
      "Start epoch   6 training with lr 0.05\n",
      "|  1000/22890 batches | ms/batch 69.53 | loss 0.5456 acc 0.7828333333333334\n",
      "|  2000/22890 batches | ms/batch 78.79 | loss 0.5640 acc 0.7725833333333333\n",
      "|  3000/22890 batches | ms/batch 84.82 | loss 0.5555 acc 0.7742083333333334\n",
      "|  4000/22890 batches | ms/batch 89.43 | loss 0.5683 acc 0.7668333333333334\n",
      "|  5000/22890 batches | ms/batch 93.72 | loss 0.5827 acc 0.76175\n",
      "|  6000/22890 batches | ms/batch 97.76 | loss 0.5938 acc 0.75275\n",
      "|  7000/22890 batches | ms/batch 103.71 | loss 0.5895 acc 0.75225\n",
      "|  8000/22890 batches | ms/batch 109.86 | loss 0.5998 acc 0.7498333333333334\n",
      "|  9000/22890 batches | ms/batch 114.42 | loss 0.5983 acc 0.7494583333333333\n",
      "| 10000/22890 batches | ms/batch 121.15 | loss 0.5997 acc 0.7499166666666667\n",
      "| 11000/22890 batches | ms/batch 128.62 | loss 0.6093 acc 0.7485\n",
      "| 12000/22890 batches | ms/batch 132.56 | loss 0.6241 acc 0.7390833333333333\n",
      "| 13000/22890 batches | ms/batch 139.42 | loss 0.6199 acc 0.7390833333333333\n",
      "| 14000/22890 batches | ms/batch 144.13 | loss 0.6267 acc 0.734625\n",
      "| 15000/22890 batches | ms/batch 149.70 | loss 0.6285 acc 0.735\n",
      "| 16000/22890 batches | ms/batch 155.55 | loss 0.6303 acc 0.7367916666666666\n",
      "| 17000/22890 batches | ms/batch 161.69 | loss 0.6242 acc 0.7379583333333334\n",
      "| 18000/22890 batches | ms/batch 168.30 | loss 0.6225 acc 0.7375833333333334\n",
      "| 19000/22890 batches | ms/batch 176.43 | loss 0.6367 acc 0.731875\n",
      "| 20000/22890 batches | ms/batch 186.57 | loss 0.6477 acc 0.72225\n",
      "| 21000/22890 batches | ms/batch 200.58 | loss 0.6488 acc 0.7243333333333334\n",
      "| 22000/22890 batches | ms/batch 230.81 | loss 0.6551 acc 0.721\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time: 3231.87s | valid loss  1.40 accuracy 0.7099959283387622 learning rate 0.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "Start epoch   7 training with lr 0.045\n",
      "|  1000/22890 batches | ms/batch 69.39 | loss 0.5315 acc 0.7889583333333333\n",
      "|  2000/22890 batches | ms/batch 78.59 | loss 0.5504 acc 0.7796666666666666\n",
      "|  3000/22890 batches | ms/batch 84.49 | loss 0.5587 acc 0.774\n",
      "|  4000/22890 batches | ms/batch 89.06 | loss 0.5629 acc 0.767875\n",
      "|  5000/22890 batches | ms/batch 93.55 | loss 0.5747 acc 0.764625\n",
      "|  6000/22890 batches | ms/batch 96.74 | loss 0.5695 acc 0.764875\n",
      "|  7000/22890 batches | ms/batch 103.37 | loss 0.5789 acc 0.7606666666666667\n",
      "|  8000/22890 batches | ms/batch 109.34 | loss 0.5906 acc 0.7545\n",
      "|  9000/22890 batches | ms/batch 114.50 | loss 0.5945 acc 0.7534166666666666\n",
      "| 10000/22890 batches | ms/batch 121.21 | loss 0.6028 acc 0.750375\n",
      "| 11000/22890 batches | ms/batch 127.65 | loss 0.5968 acc 0.7534166666666666\n",
      "| 12000/22890 batches | ms/batch 132.49 | loss 0.6085 acc 0.747\n",
      "| 13000/22890 batches | ms/batch 138.89 | loss 0.6058 acc 0.745875\n",
      "| 14000/22890 batches | ms/batch 144.03 | loss 0.6114 acc 0.7453333333333333\n",
      "| 15000/22890 batches | ms/batch 149.97 | loss 0.6164 acc 0.7420833333333333\n",
      "| 16000/22890 batches | ms/batch 155.42 | loss 0.6087 acc 0.7420833333333333\n",
      "| 17000/22890 batches | ms/batch 161.86 | loss 0.6143 acc 0.7443333333333333\n",
      "| 18000/22890 batches | ms/batch 167.82 | loss 0.6239 acc 0.7392083333333334\n",
      "| 19000/22890 batches | ms/batch 176.37 | loss 0.6381 acc 0.7315\n",
      "| 20000/22890 batches | ms/batch 186.57 | loss 0.6300 acc 0.7312083333333333\n",
      "| 21000/22890 batches | ms/batch 200.48 | loss 0.6418 acc 0.7309583333333334\n",
      "| 22000/22890 batches | ms/batch 230.39 | loss 0.6494 acc 0.7250416666666667\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time: 3226.85s | valid loss  1.39 accuracy 0.7128460912052117 learning rate 0.045\n",
      "-----------------------------------------------------------------------------------------\n",
      "Start epoch   8 training with lr 0.04\n",
      "|  1000/22890 batches | ms/batch 69.33 | loss 0.5329 acc 0.789625\n",
      "|  2000/22890 batches | ms/batch 78.51 | loss 0.5456 acc 0.7788333333333334\n",
      "|  3000/22890 batches | ms/batch 84.61 | loss 0.5550 acc 0.772125\n",
      "|  4000/22890 batches | ms/batch 88.84 | loss 0.5563 acc 0.7707083333333333\n",
      "|  5000/22890 batches | ms/batch 94.04 | loss 0.5584 acc 0.7695\n",
      "|  6000/22890 batches | ms/batch 97.12 | loss 0.5626 acc 0.7708333333333334\n",
      "|  7000/22890 batches | ms/batch 103.82 | loss 0.5711 acc 0.7635833333333333\n",
      "|  8000/22890 batches | ms/batch 109.40 | loss 0.5793 acc 0.7636666666666667\n",
      "|  9000/22890 batches | ms/batch 114.73 | loss 0.5944 acc 0.7565833333333334\n",
      "| 10000/22890 batches | ms/batch 121.92 | loss 0.5890 acc 0.754\n",
      "| 11000/22890 batches | ms/batch 128.25 | loss 0.5848 acc 0.7570416666666666\n",
      "| 12000/22890 batches | ms/batch 133.13 | loss 0.5962 acc 0.7482083333333334\n",
      "| 13000/22890 batches | ms/batch 139.17 | loss 0.5980 acc 0.752625\n",
      "| 14000/22890 batches | ms/batch 144.22 | loss 0.6037 acc 0.74775\n",
      "| 15000/22890 batches | ms/batch 149.31 | loss 0.6041 acc 0.743875\n",
      "| 16000/22890 batches | ms/batch 155.38 | loss 0.6067 acc 0.7436666666666667\n",
      "| 17000/22890 batches | ms/batch 161.84 | loss 0.6010 acc 0.7515833333333334\n",
      "| 18000/22890 batches | ms/batch 168.36 | loss 0.6136 acc 0.7427916666666666\n",
      "| 19000/22890 batches | ms/batch 176.20 | loss 0.6142 acc 0.7412916666666667\n",
      "| 20000/22890 batches | ms/batch 186.28 | loss 0.6286 acc 0.7354166666666667\n",
      "| 21000/22890 batches | ms/batch 200.40 | loss 0.6378 acc 0.7325416666666666\n",
      "| 22000/22890 batches | ms/batch 230.25 | loss 0.6366 acc 0.732625\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time: 3230.09s | valid loss  1.50 accuracy 0.7132532573289903 learning rate 0.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "Start epoch   9 training with lr 0.035\n",
      "|  1000/22890 batches | ms/batch 69.53 | loss 0.5141 acc 0.7975416666666667\n",
      "|  2000/22890 batches | ms/batch 78.86 | loss 0.5387 acc 0.7792916666666667\n",
      "|  3000/22890 batches | ms/batch 84.32 | loss 0.5413 acc 0.7814166666666666\n",
      "|  4000/22890 batches | ms/batch 88.92 | loss 0.5455 acc 0.7792083333333333\n",
      "|  5000/22890 batches | ms/batch 93.80 | loss 0.5559 acc 0.7712916666666667\n",
      "|  6000/22890 batches | ms/batch 97.77 | loss 0.5592 acc 0.7714583333333334\n",
      "|  7000/22890 batches | ms/batch 103.80 | loss 0.5677 acc 0.767875\n",
      "|  8000/22890 batches | ms/batch 109.68 | loss 0.5711 acc 0.7630833333333333\n",
      "|  9000/22890 batches | ms/batch 115.11 | loss 0.5780 acc 0.7595833333333334\n",
      "| 10000/22890 batches | ms/batch 121.87 | loss 0.5784 acc 0.7635\n",
      "| 11000/22890 batches | ms/batch 128.36 | loss 0.5825 acc 0.7582083333333334\n",
      "| 12000/22890 batches | ms/batch 133.25 | loss 0.5926 acc 0.754125\n",
      "| 13000/22890 batches | ms/batch 138.84 | loss 0.5923 acc 0.7536666666666667\n",
      "| 14000/22890 batches | ms/batch 143.94 | loss 0.5925 acc 0.7509166666666667\n",
      "| 15000/22890 batches | ms/batch 149.84 | loss 0.5988 acc 0.749625\n",
      "| 16000/22890 batches | ms/batch 155.75 | loss 0.6010 acc 0.7453333333333333\n",
      "| 17000/22890 batches | ms/batch 161.89 | loss 0.6055 acc 0.7480833333333333\n",
      "| 18000/22890 batches | ms/batch 167.89 | loss 0.6041 acc 0.7502916666666667\n",
      "| 19000/22890 batches | ms/batch 176.56 | loss 0.6069 acc 0.7485\n",
      "| 20000/22890 batches | ms/batch 186.79 | loss 0.6280 acc 0.7371666666666666\n",
      "| 21000/22890 batches | ms/batch 200.66 | loss 0.6238 acc 0.736125\n",
      "| 22000/22890 batches | ms/batch 230.48 | loss 0.6317 acc 0.7323333333333333\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time: 3232.16s | valid loss  1.54 accuracy 0.7088762214983714 learning rate 0.035\n",
      "-----------------------------------------------------------------------------------------\n",
      "Start epoch  10 training with lr 0.03\n",
      "|  1000/22890 batches | ms/batch 69.40 | loss 0.5131 acc 0.79775\n",
      "|  2000/22890 batches | ms/batch 78.63 | loss 0.5282 acc 0.7868333333333334\n",
      "|  3000/22890 batches | ms/batch 84.64 | loss 0.5358 acc 0.7855\n",
      "|  4000/22890 batches | ms/batch 88.68 | loss 0.5413 acc 0.780375\n",
      "|  5000/22890 batches | ms/batch 93.78 | loss 0.5446 acc 0.778625\n",
      "|  6000/22890 batches | ms/batch 96.82 | loss 0.5529 acc 0.7732916666666667\n",
      "|  7000/22890 batches | ms/batch 103.25 | loss 0.5533 acc 0.7735416666666667\n",
      "|  8000/22890 batches | ms/batch 110.04 | loss 0.5623 acc 0.7684583333333334\n",
      "|  9000/22890 batches | ms/batch 114.04 | loss 0.5663 acc 0.7677083333333333\n",
      "| 10000/22890 batches | ms/batch 121.48 | loss 0.5696 acc 0.7661666666666667\n",
      "| 11000/22890 batches | ms/batch 128.49 | loss 0.5762 acc 0.7610416666666666\n",
      "| 12000/22890 batches | ms/batch 132.97 | loss 0.5723 acc 0.7628333333333334\n",
      "| 13000/22890 batches | ms/batch 138.70 | loss 0.5845 acc 0.7570833333333333\n",
      "| 14000/22890 batches | ms/batch 144.23 | loss 0.5723 acc 0.766\n",
      "| 15000/22890 batches | ms/batch 149.30 | loss 0.5941 acc 0.7545\n",
      "| 16000/22890 batches | ms/batch 154.63 | loss 0.5868 acc 0.7537916666666666\n",
      "| 17000/22890 batches | ms/batch 161.28 | loss 0.5868 acc 0.7555416666666667\n",
      "| 18000/22890 batches | ms/batch 167.60 | loss 0.6019 acc 0.7500833333333333\n",
      "| 19000/22890 batches | ms/batch 176.17 | loss 0.6040 acc 0.746125\n",
      "| 20000/22890 batches | ms/batch 185.50 | loss 0.6157 acc 0.74375\n",
      "| 21000/22890 batches | ms/batch 200.72 | loss 0.6132 acc 0.742\n",
      "| 22000/22890 batches | ms/batch 229.89 | loss 0.6276 acc 0.7359583333333334\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | time: 3224.74s | valid loss  1.60 accuracy 0.7047027687296417 learning rate 0.03\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for param in siamese_model.encoder.parameters():\n",
    "    param.requires_grad = True\n",
    "    \n",
    "lrs = [x/200 for x in reversed(range(6, 17))]\n",
    "training_loop(lrs, siamese_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(siamese_model, './siamese_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "_draft": {
   "nbviewer_url": "https://gist.github.com/0dd0df21cf404cf2bb51d0148c8b7d8b"
  },
  "gist": {
   "data": {
    "description": "fastai.text imdb example",
    "public": true
   },
   "id": "0dd0df21cf404cf2bb51d0148c8b7d8b"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "86px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
