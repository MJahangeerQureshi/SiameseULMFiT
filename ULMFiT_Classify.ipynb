{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ULMFiT + Siamese Network for Sentence Vectors\n",
    "## Part Three: Classifying\n",
    "\n",
    "The second notebook created a new language model from the SNLI dataset.\n",
    "This notebook will adapt that model to predicting the SNLI category for sentence pairs.\n",
    "The model will be used as a sentence encoder for a Siamese Network that builds sentence vectors that are feed into a classifier network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text import *\n",
    "import html\n",
    "\n",
    "import json\n",
    "import html\n",
    "import re\n",
    "import pickle\n",
    "from collections import Counter\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import sklearn\n",
    "from sklearn import model_selection\n",
    "from functools import partial\n",
    "from collections import Counter, defaultdict\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils \n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import dataset, dataloader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import time\n",
    "import math\n",
    "import sys\n",
    "import data\n",
    "\n",
    "snli_root = './data/SNLI/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34155"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load the tokens\n",
    "itos = pickle.load(open(f'{snli_root}itos.pkl', 'rb'))\n",
    "\n",
    "stoi = defaultdict(lambda:0, {v:k for k,v in enumerate(itos)})\n",
    "vocab_size = len(itos)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a new dataloader to create sentence pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "class Entail(Enum):\n",
    "    entailment = 0\n",
    "    contradiction = 1\n",
    "    neutral = 2\n",
    "       \n",
    "class SiameseDataset(dataset.Dataset):\n",
    "    def __init__(self, json_file):\n",
    "        \n",
    "        content = None\n",
    "        with open(json_file) as fp:\n",
    "            content = json.load(fp)\n",
    "\n",
    "        self.items = []\n",
    "        for item in content:\n",
    "            s0 = item[0]\n",
    "            s1 = item[1]\n",
    "            average_len = (len(s0)+len(s1))/2\n",
    "            try:\n",
    "                label = Entail[item[2]].value\n",
    "                self.items.append((s0, s1, label, average_len))\n",
    "            except KeyError:\n",
    "                pass\n",
    "            \n",
    "    def shuffle(self):\n",
    "        self.items.sort(key=lambda x: x[3]+random.randint(-5, 5))\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.items[index]\n",
    "       \n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "    \n",
    "class SiameseDataLoader():\n",
    "    def __init__(self, dataset, stoi, pad_val, batch_size=32):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.stoi = stoi\n",
    "        self.index = 0\n",
    "        self.pad_val = pad_val\n",
    "      \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def fill_tensor(self, sentences, max_len):\n",
    "        data = np.zeros((max_len, len(sentences)), dtype=np.long)\n",
    "        data.fill(self.pad_val)\n",
    "        \n",
    "        for i, s in enumerate(sentences): \n",
    "            start_idx = max_len - len(s)\n",
    "            for j, p in enumerate(s):\n",
    "                data[:,i][start_idx+j] = stoi[p]\n",
    "            \n",
    "        return torch.LongTensor([data.tolist()]).cuda()\n",
    "     \n",
    "    def batch(self):\n",
    "        return self.index//self.batch_size\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)//self.batch_size\n",
    "    \n",
    "    def __next__(self):\n",
    "        #how many examples to ananlyise for this round\n",
    "        num = min(self.batch_size, len(self.dataset) - self.index)\n",
    "        \n",
    "        if num < 1:\n",
    "            raise StopIteration  # signals \"the end\"\n",
    "            \n",
    "        #collect the sentences\n",
    "        max_len = 0\n",
    "        first = []\n",
    "        second = []\n",
    "        labels = np.zeros((num), dtype=np.long)\n",
    "        \n",
    "        for i in range(num):\n",
    "            a, b, l, _ = self.dataset[self.index + i]\n",
    "            \n",
    "            if len(a) > max_len:\n",
    "                max_len = len(a)\n",
    "            \n",
    "            if len(b) > max_len:\n",
    "                max_len = len(b)\n",
    "            \n",
    "            first.append(a)\n",
    "            second.append(b)\n",
    "            labels[i] = l\n",
    "            \n",
    "        self.index += num\n",
    "             \n",
    "        return (self.fill_tensor(first, max_len).cuda(),\n",
    "                self.fill_tensor(second, max_len).cuda(),\n",
    "                torch.LongTensor([labels.tolist()]).cuda()\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_dataset_train = SiameseDataset(f'{snli_root}/snli_train.json')\n",
    "siamese_dataset_dev = SiameseDataset(f'{snli_root}snli_dev.json')\n",
    "siamese_dataset_test = SiameseDataset(f'{snli_root}snli_test.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Siamese network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, encoder, classifier):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.classifier = classifier\n",
    "    \n",
    "    def pool(self, x, bs, is_max):\n",
    "        f = F.adaptive_max_pool1d if is_max else F.adaptive_avg_pool1d\n",
    "        return f(x.permute(1,2,0), (1,)).view(bs,-1)\n",
    "\n",
    "    def pool_outputs(self, output):\n",
    "        sl, bs,_ = output.size()\n",
    "        avgpool = self.pool(output, bs, False)\n",
    "        maxpool = self.pool(output, bs, True)\n",
    "        return torch.cat([output[-1], maxpool, avgpool], 1)\n",
    "        \n",
    "    def forward(self, input1, input2):\n",
    "\n",
    "        raw_outputs1, outputs1 = self.encoder(input1)\n",
    "        raw_outputs2, outputs2 = self.encoder(input2)\n",
    "        \n",
    "        out1 = self.pool_outputs(outputs1[-1])\n",
    "        out2 = self.pool_outputs(outputs2[-1])\n",
    "        \n",
    "        out = torch.cat([out1, out2], 1)\n",
    "        \n",
    "        return self.classifier(out)\n",
    "        \n",
    "    def reset(self):\n",
    "        for c in self.children():\n",
    "            if hasattr(c, 'reset'): c.reset()\n",
    "\n",
    "class LinearClassifier(nn.Module):\n",
    "    def __init__(self, layers, dropout):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([LinearBlock(layers[i], layers[i + 1], dropout) for i in range(len(layers) - 1)])\n",
    "        \n",
    "    def forward(self, input):\n",
    "        x = input\n",
    "        for l in self.layers:\n",
    "            l_x = l(x)\n",
    "            x = F.relu(l_x)\n",
    "        return l_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load our pretrained model then build the Siamese network from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#these are the values used for the original LM\n",
    "em_sz, nh, nl = 400,1150,3\n",
    "bptt = 70\n",
    "max_seq = bptt * 20\n",
    "cats = 3\n",
    "\n",
    "SNLI_LM = torch.load(\"SNLI_LM.pt\")\n",
    "\n",
    "dps = np.array([0.4,0.5,0.05,0.3,0.4])*0.1\n",
    "SNLI_encoder = MultiBatchRNN(bptt, max_seq, vocab_size, em_sz, nh, nl, stoi[\"_pad_\"], dropouti=dps[0], wdrop=dps[2], dropoute=dps[3], dropouth=dps[4])\n",
    "\n",
    "SNLI_encoder.load_state_dict(SNLI_LM[0].state_dict())\n",
    "\n",
    "#2 pooled vectors, of 3 times the embedding size\n",
    "siamese_model = SiameseClassifier(SNLI_encoder, LinearClassifier(layers=[2*em_sz*3, 50, cats], dropout=0.1)).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop\n",
    "This should be converted over to the fast.ai learner but I'm not sure how to do that yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_interval = 100\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "def evaluate(model, data_loader):\n",
    "    # Turn on evaluation mode which disables dropout.\n",
    "    model.eval()\n",
    "    model.reset()\n",
    "    total_loss = 0.\n",
    "    num_correct = 0\n",
    "    total = 0 \n",
    "    for a, b, l in data_loader:\n",
    "\n",
    "        a, b, l = Variable(a), Variable(b), Variable(l)\n",
    "        a.requires_grad = False\n",
    "        b.requires_grad = False\n",
    "        l.requires_grad = False\n",
    "        out = model(a.squeeze(), b.squeeze())\n",
    "        num_correct += np.sum(l.data.cpu().numpy() == np.argmax(out.data.cpu().numpy(), 1))\n",
    "        total += out.shape[0]\n",
    "        loss = criterion(out, l.squeeze())\n",
    "        total_loss += out.shape[0] * loss.data.cpu()[0]\n",
    "\n",
    "    return (total_loss / total, num_correct / total)\n",
    "\n",
    "def train(model, data_loader, optimizer):\n",
    "    # Turn on training mode which enables dropout.\n",
    "    model.train()\n",
    "    model.reset()\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    \n",
    "    num_correct = 0\n",
    "    total = 0 \n",
    "        \n",
    "    for a, b, l in data_loader:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        a, b, l = Variable(a), Variable(b), Variable(l)\n",
    "\n",
    "        out = model(a.squeeze(), b.squeeze())\n",
    "        loss = criterion(out, l.squeeze())\n",
    "        total_loss += out.shape[0] * loss.data.cpu()[0]\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        num_correct += np.sum(l.data.cpu().numpy() == np.argmax(out.data.cpu().numpy(), 1))\n",
    "        total += out.shape[0]\n",
    "\n",
    "        batch = data_loader.batch()\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / total\n",
    "            elapsed = time.time() - start_time\n",
    "            batches = len(data_loader)\n",
    "            ms = elapsed * 1000 / log_interval\n",
    "            print(f'| {batch:5d}/{batches:5d} batches', end=\" \")\n",
    "            print(f'| ms/batch {ms:5.2f} | loss {cur_loss:5.4f} acc {num_correct / total}')\n",
    "            total_loss = 0\n",
    "            total = 0\n",
    "            num_correct = 0\n",
    "            start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_acc = 0\n",
    "def training_loop(lrs, model):\n",
    "    global best_acc\n",
    "    for epoch, lr in enumerate(lrs):\n",
    "\n",
    "        print(f'Start epoch {epoch:3d} training with lr {lr}')\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        siamese_dataset_train.shuffle()\n",
    "        training_data = SiameseDataLoader(siamese_dataset_train, stoi, stoi[\"_pad_\"], batch_size=32)\n",
    "\n",
    "        epoch_start_time = time.time()\n",
    "        train(siamese_model, training_data, optimizer)\n",
    "\n",
    "        validation_data = SiameseDataLoader(siamese_dataset_test , stoi, stoi[\"_pad_\"], batch_size=32)\n",
    "        val_loss, accuracy = evaluate(siamese_model, validation_data)\n",
    "\n",
    "        delta_t = (time.time() - epoch_start_time)\n",
    "        print('-' * 89)\n",
    "        print(f'| end of epoch {epoch:3d} | time: {delta_t:5.2f}s | valid loss {val_loss:5.2f} accuracy {accuracy} learning rate {lr}')\n",
    "        print('-' * 89)\n",
    "\n",
    "        if accuracy > best_acc:\n",
    "            best_acc = accuracy\n",
    "            with open(f'./siamese_model.pt', 'wb') as f:\n",
    "                torch.save(siamese_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start epoch   0 training with lr 0.003\n",
      "|   100/  307 batches | ms/batch 99.59 | loss 1.0719 acc 0.4121875\n",
      "|   200/  307 batches | ms/batch 103.49 | loss 1.0639 acc 0.4246875\n",
      "|   300/  307 batches | ms/batch 100.20 | loss 1.0571 acc 0.4425\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   0 | time: 59.11s | valid loss  1.05 accuracy 0.44421824104234525 learning rate 0.003\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brian/.conda/envs/fastai/lib/python3.6/site-packages/torch/serialization.py:159: UserWarning: Couldn't retrieve source code for container of type SiameseClassifier. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/brian/.conda/envs/fastai/lib/python3.6/site-packages/torch/serialization.py:159: UserWarning: Couldn't retrieve source code for container of type LinearClassifier. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "for param in siamese_model.encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "training_loop([3e-3], siamese_model.classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start epoch   0 training with lr 0.0002\n",
      "|   100/  307 batches | ms/batch 238.94 | loss 1.0337 acc 0.4653125\n",
      "|   200/  307 batches | ms/batch 248.22 | loss 1.0380 acc 0.4534375\n",
      "|   300/  307 batches | ms/batch 240.44 | loss 1.0244 acc 0.479375\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   0 | time: 103.47s | valid loss  1.03 accuracy 0.4732288273615635 learning rate 0.0002\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brian/.conda/envs/fastai/lib/python3.6/site-packages/torch/serialization.py:159: UserWarning: Couldn't retrieve source code for container of type SiameseClassifier. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/brian/.conda/envs/fastai/lib/python3.6/site-packages/torch/serialization.py:159: UserWarning: Couldn't retrieve source code for container of type LinearClassifier. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start epoch   1 training with lr 0.0004\n",
      "|   100/  307 batches | ms/batch 239.21 | loss 1.0094 acc 0.4940625\n",
      "|   200/  307 batches | ms/batch 248.07 | loss 1.0079 acc 0.49375\n",
      "|   300/  307 batches | ms/batch 240.97 | loss 0.9967 acc 0.51625\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 103.52s | valid loss  1.01 accuracy 0.49297638436482083 learning rate 0.0004\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brian/.conda/envs/fastai/lib/python3.6/site-packages/torch/serialization.py:159: UserWarning: Couldn't retrieve source code for container of type SiameseClassifier. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/brian/.conda/envs/fastai/lib/python3.6/site-packages/torch/serialization.py:159: UserWarning: Couldn't retrieve source code for container of type LinearClassifier. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start epoch   2 training with lr 0.0008\n",
      "|   100/  307 batches | ms/batch 239.18 | loss 0.9849 acc 0.515\n",
      "|   200/  307 batches | ms/batch 247.99 | loss 0.9876 acc 0.5028125\n",
      "|   300/  307 batches | ms/batch 240.45 | loss 0.9702 acc 0.5115625\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 103.49s | valid loss  0.99 accuracy 0.5154723127035831 learning rate 0.0008\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brian/.conda/envs/fastai/lib/python3.6/site-packages/torch/serialization.py:159: UserWarning: Couldn't retrieve source code for container of type SiameseClassifier. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/brian/.conda/envs/fastai/lib/python3.6/site-packages/torch/serialization.py:159: UserWarning: Couldn't retrieve source code for container of type LinearClassifier. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start epoch   3 training with lr 0.001\n",
      "|   100/  307 batches | ms/batch 239.55 | loss 0.9362 acc 0.5584375\n",
      "|   200/  307 batches | ms/batch 248.44 | loss 0.9364 acc 0.548125\n",
      "|   300/  307 batches | ms/batch 240.76 | loss 0.9289 acc 0.56\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 103.63s | valid loss  0.99 accuracy 0.5261604234527687 learning rate 0.001\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brian/.conda/envs/fastai/lib/python3.6/site-packages/torch/serialization.py:159: UserWarning: Couldn't retrieve source code for container of type SiameseClassifier. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/brian/.conda/envs/fastai/lib/python3.6/site-packages/torch/serialization.py:159: UserWarning: Couldn't retrieve source code for container of type LinearClassifier. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start epoch   4 training with lr 0.0005\n",
      "|   100/  307 batches | ms/batch 239.76 | loss 0.8949 acc 0.5828125\n",
      "|   200/  307 batches | ms/batch 248.34 | loss 0.8913 acc 0.5778125\n",
      "|   300/  307 batches | ms/batch 240.89 | loss 0.8563 acc 0.6109375\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 103.64s | valid loss  0.98 accuracy 0.5414291530944625 learning rate 0.0005\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brian/.conda/envs/fastai/lib/python3.6/site-packages/torch/serialization.py:159: UserWarning: Couldn't retrieve source code for container of type SiameseClassifier. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/brian/.conda/envs/fastai/lib/python3.6/site-packages/torch/serialization.py:159: UserWarning: Couldn't retrieve source code for container of type LinearClassifier. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start epoch   5 training with lr 0.0003\n",
      "|   100/  307 batches | ms/batch 239.55 | loss 0.8431 acc 0.615\n",
      "|   200/  307 batches | ms/batch 248.75 | loss 0.8435 acc 0.60625\n",
      "|   300/  307 batches | ms/batch 240.81 | loss 0.8113 acc 0.6253125\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 103.65s | valid loss  0.99 accuracy 0.5453990228013029 learning rate 0.0003\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brian/.conda/envs/fastai/lib/python3.6/site-packages/torch/serialization.py:159: UserWarning: Couldn't retrieve source code for container of type SiameseClassifier. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/brian/.conda/envs/fastai/lib/python3.6/site-packages/torch/serialization.py:159: UserWarning: Couldn't retrieve source code for container of type LinearClassifier. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start epoch   6 training with lr 0.0001\n",
      "|   100/  307 batches | ms/batch 239.64 | loss 0.8161 acc 0.6284375\n",
      "|   200/  307 batches | ms/batch 248.73 | loss 0.8198 acc 0.6228125\n",
      "|   300/  307 batches | ms/batch 241.20 | loss 0.7870 acc 0.6559375\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time: 103.68s | valid loss  0.99 accuracy 0.5498778501628665 learning rate 0.0001\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brian/.conda/envs/fastai/lib/python3.6/site-packages/torch/serialization.py:159: UserWarning: Couldn't retrieve source code for container of type SiameseClassifier. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/brian/.conda/envs/fastai/lib/python3.6/site-packages/torch/serialization.py:159: UserWarning: Couldn't retrieve source code for container of type LinearClassifier. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start epoch   7 training with lr 8e-05\n",
      "|   100/  307 batches | ms/batch 239.86 | loss 0.8062 acc 0.6378125\n",
      "|   200/  307 batches | ms/batch 248.51 | loss 0.8072 acc 0.63875\n",
      "|   300/  307 batches | ms/batch 241.09 | loss 0.7672 acc 0.6578125\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time: 103.72s | valid loss  0.99 accuracy 0.5495724755700325 learning rate 8e-05\n",
      "-----------------------------------------------------------------------------------------\n",
      "Start epoch   8 training with lr 4e-05\n",
      "|   100/  307 batches | ms/batch 239.55 | loss 0.7913 acc 0.638125\n",
      "|   200/  307 batches | ms/batch 248.19 | loss 0.7972 acc 0.6353125\n",
      "|   300/  307 batches | ms/batch 240.70 | loss 0.7483 acc 0.6628125\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time: 103.58s | valid loss  0.98 accuracy 0.5501832247557004 learning rate 4e-05\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brian/.conda/envs/fastai/lib/python3.6/site-packages/torch/serialization.py:159: UserWarning: Couldn't retrieve source code for container of type SiameseClassifier. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/brian/.conda/envs/fastai/lib/python3.6/site-packages/torch/serialization.py:159: UserWarning: Couldn't retrieve source code for container of type LinearClassifier. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "for param in siamese_model.encoder.parameters():\n",
    "    param.requires_grad = True\n",
    "    \n",
    "training_loop([0.0002, 0.0004, 0.0008, 0.001, 0.0005, 0.0003, 0.0001, 0.00008, 0.00004], siamese_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start epoch   0 training with lr 0.0005\n",
      "|   100/17167 batches | ms/batch 71.78 | loss 0.9766 acc 0.56625\n",
      "|   200/17167 batches | ms/batch 78.40 | loss 0.9085 acc 0.59125\n",
      "|   300/17167 batches | ms/batch 80.94 | loss 0.9006 acc 0.58375\n",
      "|   400/17167 batches | ms/batch 83.87 | loss 0.8837 acc 0.5928125\n",
      "|   500/17167 batches | ms/batch 85.30 | loss 0.8903 acc 0.5859375\n",
      "|   600/17167 batches | ms/batch 87.14 | loss 0.8986 acc 0.5859375\n",
      "|   700/17167 batches | ms/batch 89.20 | loss 0.9035 acc 0.578125\n",
      "|   800/17167 batches | ms/batch 90.20 | loss 0.9032 acc 0.57125\n",
      "|   900/17167 batches | ms/batch 91.62 | loss 0.8920 acc 0.590625\n",
      "|  1000/17167 batches | ms/batch 92.51 | loss 0.9004 acc 0.57375\n"
     ]
    }
   ],
   "source": [
    "training_loop([0.0005, 0.0003, 0.0001, 0.00008], siamese_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brian/.conda/envs/fastai/lib/python3.6/site-packages/torch/serialization.py:159: UserWarning: Couldn't retrieve source code for container of type SiameseClassifier. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/brian/.conda/envs/fastai/lib/python3.6/site-packages/torch/serialization.py:159: UserWarning: Couldn't retrieve source code for container of type LinearClassifier. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "with open(f'./siamese_model0.50.pt', 'wb') as f:\n",
    "    torch.save(siamese_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "_draft": {
   "nbviewer_url": "https://gist.github.com/0dd0df21cf404cf2bb51d0148c8b7d8b"
  },
  "gist": {
   "data": {
    "description": "fastai.text imdb example",
    "public": true
   },
   "id": "0dd0df21cf404cf2bb51d0148c8b7d8b"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "86px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
