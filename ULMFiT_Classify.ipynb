{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ULMFiT + Siamese Network for Sentence Vectors\n",
    "## Part Two: Classifying\n",
    "\n",
    "The first notebook created a new language model from the SNLI dataset.\n",
    "This notebook will adapt that model to predicting the SNLI category for sentence pairs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text import *\n",
    "import html\n",
    "\n",
    "import json\n",
    "import html\n",
    "import re\n",
    "import pickle\n",
    "from collections import Counter\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import sklearn\n",
    "from sklearn import model_selection\n",
    "from functools import partial\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils \n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import dataset, dataloader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import time\n",
    "import math\n",
    "import sys\n",
    "import data\n",
    "\n",
    "snli_root = './data/SNLI/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34155"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load the tokens\n",
    "itos = pickle.load(open(f'{snli_root}itos.pkl', 'rb'))\n",
    "trn_lm = np.load(f'{snli_root}trn_lm.npy')\n",
    "val_lm = np.load(f'{snli_root}val_lm.npy')\n",
    "\n",
    "stoi = defaultdict(lambda:0, {v:k for k,v in enumerate(itos)})\n",
    "vocab_size = len(itos)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a new dataloader to create sentence pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "class Entail(Enum):\n",
    "    entailment = 0\n",
    "    contradiction = 1\n",
    "    neutral = 2\n",
    "       \n",
    "class SiameseDataset(dataset.Dataset):\n",
    "    def __init__(self, json_file):\n",
    "        \n",
    "        content = None\n",
    "        with open(json_file) as fp:\n",
    "            content = json.load(fp)\n",
    "\n",
    "        self.items = []\n",
    "        for item in content:\n",
    "            s0 = item[0]\n",
    "            s1 = item[1]\n",
    "            label = Entail[item[2]].value\n",
    "            self.items.append((s0, s1, label))\n",
    "            \n",
    "    def shuffle(self):\n",
    "        random.shuffle(self.items)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.items[index]\n",
    "       \n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "    \n",
    "class SiameseDataLoader():\n",
    "    def __init__(self, dataset, stoi, pad_val, batch_size=32):\n",
    "        self.dataset = dataset\n",
    "        dataset.shuffle()\n",
    "        self.batch_size = batch_size\n",
    "        self.stoi = stoi\n",
    "        self.index = 0\n",
    "        self.pad_val = pad_val\n",
    "      \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def fill_tensor(self, sentences, max_len):\n",
    "        data = np.zeros((max_len, self.batch_size), dtype=np.long)\n",
    "        data.fill(self.pad_val)\n",
    "        \n",
    "        for i, s in enumerate(sentences): \n",
    "            start_idx = max_len - len(s)\n",
    "            for j, p in enumerate(s):\n",
    "                data[:,i][start_idx+j] = stoi[p]\n",
    "            \n",
    "        return torch.LongTensor([data.tolist()]).cuda()\n",
    "     \n",
    "    def batch(self):\n",
    "        return self.index//self.batch_size\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)//self.batch_size\n",
    "    \n",
    "    def __next__(self):\n",
    "        #how many examples to ananlyise for this round\n",
    "        num = min(self.batch_size, len(self.dataset) - self.index)\n",
    "        \n",
    "        if num < 1:\n",
    "            raise StopIteration  # signals \"the end\"\n",
    "            \n",
    "        #collect the sentences\n",
    "        max_len = 0\n",
    "        first = []\n",
    "        second = []\n",
    "        labels = np.zeros((self.batch_size), dtype=np.long)\n",
    "        \n",
    "        for i in range(self.index, self.index+num):\n",
    "            a, b, l = self.dataset[i]\n",
    "            \n",
    "            if len(a) > max_len:\n",
    "                max_len = len(a)\n",
    "            \n",
    "            if len(b) > max_len:\n",
    "                max_len = len(b)\n",
    "            \n",
    "            first.append(a)\n",
    "            second.append(b)\n",
    "            labels[i - self.index] = l\n",
    "            \n",
    "        self.index += num\n",
    "             \n",
    "        return (self.fill_tensor(first, max_len),\n",
    "                self.fill_tensor(second, max_len),\n",
    "                torch.LongTensor([labels.tolist()]).cuda()\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_dataset_dev = SiameseDataset(f'{snli_root}/snli_dev.json')\n",
    "siamese_dataset_test = SiameseDataset(f'{snli_root}snli_test.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Siamese network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, encoder, classifier):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.classifier = classifier\n",
    "    \n",
    "    def pool(self, x, bs, is_max):\n",
    "        f = F.adaptive_max_pool1d if is_max else F.adaptive_avg_pool1d\n",
    "        return f(x.permute(1,2,0), (1,)).view(bs,-1)\n",
    "\n",
    "    def pool_outputs(self, output):\n",
    "        sl, bs,_ = output.size()\n",
    "        avgpool = self.pool(output, bs, False)\n",
    "        maxpool = self.pool(output, bs, True)\n",
    "        return torch.cat([output[-1], maxpool, avgpool], 1)\n",
    "        \n",
    "    def forward(self, input1, input2):\n",
    "\n",
    "        raw_outputs1, outputs1 = self.encoder(input1)\n",
    "        raw_outputs2, outputs2 = self.encoder(input2)\n",
    "        \n",
    "        out1 = self.pool_outputs(outputs1[-1])\n",
    "        out2 = self.pool_outputs(outputs2[-1])\n",
    "        \n",
    "        out = torch.cat([out1, out2], 1)\n",
    "        \n",
    "        return self.classifier(out)\n",
    "        \n",
    "    def reset(self):\n",
    "        for c in self.children():\n",
    "            if hasattr(c, 'reset'): c.reset()\n",
    "                \n",
    "class LinearClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden, num_categories, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layers = nn.ModuleList([\n",
    "            LinearBlock(input_size, hidden, dropout),\n",
    "            LinearBlock(hidden, hidden, dropout),\n",
    "            LinearBlock(hidden, num_categories, dropout)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for l in self.layers:\n",
    "            x = F.relu(l(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load our pretrained model then build the Siamese network from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#these are the values used for the original LM\n",
    "em_sz, nh = 400, 1150\n",
    "\n",
    "SNLI_encoder = torch.load(\"SNLI_Encoder.pt\")\n",
    "\n",
    "#2 pooled vectors, of 3 times the embedding size\n",
    "classifier = LinearClassifier(3*2*em_sz, nh,  3, dropout=0.05)\n",
    "siamese_model = SiameseClassifier(SNLI_encoder, classifier).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the new network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_interval = 50\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "def evaluate(model, data_loader):\n",
    "    # Turn on evaluation mode which disables dropout.\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    num_correct = 0\n",
    "    total = 0 \n",
    "    for a, b, l in data_loader:\n",
    "\n",
    "        a, b, l = Variable(a), Variable(b), Variable(l)\n",
    "        a.requires_grad = False\n",
    "        b.requires_grad = False\n",
    "        l.requires_grad = False\n",
    "        out = model(a.squeeze(), b.squeeze())\n",
    "        num_correct += np.sum(l.data.cpu().numpy() == np.argmax(out.data.cpu().numpy(), 1))\n",
    "        total += out.shape[0]\n",
    "        loss = criterion(out, l.squeeze())\n",
    "        total_loss += out.shape[0] * loss.data.cpu()[0]\n",
    "\n",
    "    return (total_loss / total, num_correct / total)\n",
    "\n",
    "def train(model, data_loader, optimizer):\n",
    "    # Turn on training mode which enables dropout.\n",
    "    model.train()\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    \n",
    "    num_correct = 0\n",
    "    total = 0 \n",
    "        \n",
    "    for a, b, l in data_loader:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        a, b, l = Variable(a), Variable(b), Variable(l)\n",
    "\n",
    "        out = model(a.squeeze(), b.squeeze())\n",
    "        loss = criterion(out, l.squeeze())\n",
    "        total_loss += out.shape[0] * loss.data.cpu()[0]\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        num_correct += np.sum(l.data.cpu().numpy() == np.argmax(out.data.cpu().numpy(), 1))\n",
    "        total += out.shape[0]\n",
    "\n",
    "        batch = data_loader.batch()\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / total\n",
    "            elapsed = time.time() - start_time\n",
    "            batches = len(data_loader)\n",
    "            ms = elapsed * 1000 / log_interval\n",
    "            print(f'| epoch {epoch:3d} | {batch:5d}/{batches:5d} batches', end=\" \")\n",
    "            print(f'| ms/batch {ms:5.2f} | loss {cur_loss:5.4f} acc {num_correct / total}')\n",
    "            total_loss = 0\n",
    "            total = 0\n",
    "            num_correct = 0\n",
    "            start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training with lr 0.001\n",
      "| epoch   0 |    50/  307 batches | ms/batch 285.29 | loss 1.2102 acc 0.363125\n",
      "| epoch   0 |   100/  307 batches | ms/batch 267.07 | loss 1.1121 acc 0.343125\n",
      "| epoch   0 |   150/  307 batches | ms/batch 273.61 | loss 1.1117 acc 0.344375\n",
      "| epoch   0 |   200/  307 batches | ms/batch 257.82 | loss 1.1106 acc 0.344375\n",
      "| epoch   0 |   250/  307 batches | ms/batch 244.26 | loss 1.1000 acc 0.351875\n",
      "| epoch   0 |   300/  307 batches | ms/batch 260.12 | loss 1.1108 acc 0.350625\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   0 | time: 112.19s | valid loss  1.09 accuracy 0.36624592833876224 learning rate 0.001\n",
      "-----------------------------------------------------------------------------------------\n",
      "training with lr 0.0005\n",
      "| epoch   1 |    50/  307 batches | ms/batch 268.46 | loss 1.1031 acc 0.35375\n"
     ]
    }
   ],
   "source": [
    "lrs = [0.001, 0.0005, 0.0001, 0.0001, 0.0001, 0.00005, 0.00001]\n",
    "\n",
    "for epoch, lr in enumerate(lrs):\n",
    "\n",
    "    print(f'training with lr {lr}')\n",
    "    optimizer = optim.Adam(siamese_model.parameters(), lr=lr)\n",
    "\n",
    "    training_data = SiameseDataLoader(siamese_dataset_dev, stoi, stoi[\"_pad_\"], batch_size=32)\n",
    "\n",
    "    epoch_start_time = time.time()\n",
    "    train(siamese_model, training_data, optimizer)\n",
    "\n",
    "    validation_data = SiameseDataLoader(siamese_dataset_test , stoi, stoi[\"_pad_\"], batch_size=32)\n",
    "    val_loss, accuracy = evaluate(siamese_model, validation_data)\n",
    "\n",
    "    delta_t = (time.time() - epoch_start_time)\n",
    "    print('-' * 89)\n",
    "    print(f'| end of epoch {epoch:3d} | time: {delta_t:5.2f}s | valid loss {val_loss:5.2f} accuracy {accuracy} learning rate {lr}')\n",
    "    print('-' * 89)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{data_root}siamese_model.pt', 'wb') as f:\n",
    "    torch.save(siamese_model, f)"
   ]
  }
 ],
 "metadata": {
  "_draft": {
   "nbviewer_url": "https://gist.github.com/0dd0df21cf404cf2bb51d0148c8b7d8b"
  },
  "gist": {
   "data": {
    "description": "fastai.text imdb example",
    "public": true
   },
   "id": "0dd0df21cf404cf2bb51d0148c8b7d8b"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "86px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
