{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ULMFiT + Siamese Network for Sentence Vectors\n",
    "## Part Three: Classifying\n",
    "\n",
    "The second notebook created a new language model from the SNLI dataset.\n",
    "This notebook will adapt that model to predicting the SNLI category for sentence pairs.\n",
    "The model will be used as a sentence encoder for a Siamese Network that builds sentence vectors that are feed into a classifier network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text import *\n",
    "import html\n",
    "\n",
    "import json\n",
    "import html\n",
    "import re\n",
    "import pickle\n",
    "from collections import Counter\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import sklearn\n",
    "from sklearn import model_selection\n",
    "from functools import partial\n",
    "from collections import Counter, defaultdict\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils \n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import dataset, dataloader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import time\n",
    "import math\n",
    "import sys\n",
    "import data\n",
    "\n",
    "snli_root = './data/snli_1.0/'\n",
    "token_files = './data/tokens/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a new dataloader to create sentence pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseDataLoader():\n",
    "    def __init__(self, sentence_pairs, pad_val, batch_size=32):\n",
    "        self.sentence_pairs = sentence_pairs\n",
    "        self.batch_size = batch_size\n",
    "        self.index = 0\n",
    "        self.pad_val = pad_val\n",
    "     \n",
    "    def shuffle(self):\n",
    "        def srtfn(x):\n",
    "            return x[:, -1] + random.randint(-5, 5)\n",
    "        \n",
    "        order = np.argsort(srtfn(self.sentence_pairs))\n",
    "        self.sentence_pairs = self.sentence_pairs[order]\n",
    "        \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def fill_tensor(self, sentences, max_len):\n",
    "        data = np.zeros((max_len, len(sentences)), dtype=np.long)\n",
    "        data.fill(self.pad_val)\n",
    "        \n",
    "        for i, s in enumerate(sentences): \n",
    "            start_idx = max_len - len(s)\n",
    "            for j, p in enumerate(s):\n",
    "                data[:,i][start_idx+j] = p\n",
    "            \n",
    "        return torch.LongTensor([data.tolist()]).cuda()\n",
    "     \n",
    "    def batch(self):\n",
    "        return self.index//self.batch_size\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sentence_pairs)//self.batch_size\n",
    "    \n",
    "    def __next__(self):\n",
    "        #how many examples to ananlyise for this round\n",
    "        num = min(self.batch_size, len(self.sentence_pairs) - self.index)\n",
    "        \n",
    "        if num < 1:\n",
    "            raise StopIteration  # signals \"the end\"\n",
    "            \n",
    "        #collect the sentences\n",
    "        max_len = 0\n",
    "        first = []\n",
    "        second = []\n",
    "        labels = torch.LongTensor(num)\n",
    "        \n",
    "        for i in range(num):\n",
    "            a, b, l, _ = self.sentence_pairs[self.index + i]\n",
    "            \n",
    "            if len(a) > max_len:\n",
    "                max_len = len(a)\n",
    "            \n",
    "            if len(b) > max_len:\n",
    "                max_len = len(b)\n",
    "            \n",
    "            first.append(a)\n",
    "            second.append(b)\n",
    "            labels[i] = l\n",
    "            \n",
    "        self.index += num\n",
    "             \n",
    "        return (self.fill_tensor(first, max_len).cuda(),\n",
    "                self.fill_tensor(second, max_len).cuda(),\n",
    "                labels.cuda()\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " x_bos a person on a horse jumps over a broken down airplane .\n",
      " x_bos a person is training his horse for a competition .\n",
      " x_bos two women are embracing while holding to go packages .\n",
      " x_bos the sisters are hugging goodbye while holding to go packages after just eating lunch .\n",
      " x_bos this church choir sings to the masses as they sing joyous songs from the book at a church .\n",
      " x_bos the church has cracks in the ceiling .\n"
     ]
    }
   ],
   "source": [
    "itos = pickle.load(open(f'{token_files}itos.pkl', 'rb'))\n",
    "stoi = defaultdict(lambda:0, {v:k for k,v in enumerate(itos)})\n",
    "vocab_size = len(itos)\n",
    "pad_tok = stoi['_pad_']\n",
    "\n",
    "sentence_pairs_train = np.load(f'{token_files}snli_tok_train.npy')\n",
    "sentence_pairs_dev = np.load(f'{token_files}snli_tok_dev.npy')\n",
    "sentence_pairs_test = np.load(f'{token_files}snli_tok_test.npy')\n",
    "\n",
    "def print_sentence(s):\n",
    "    sentence = \"\"\n",
    "    for tok in s:\n",
    "        sentence += \" \"+itos[tok]\n",
    "    print(sentence)\n",
    "\n",
    "print_sentence(sentence_pairs_train[0][0])\n",
    "print_sentence(sentence_pairs_train[0][1])\n",
    "\n",
    "print_sentence(sentence_pairs_dev[0][0])\n",
    "print_sentence(sentence_pairs_dev[0][1])\n",
    "\n",
    "print_sentence(sentence_pairs_test[0][0])\n",
    "print_sentence(sentence_pairs_test[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check the dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_bos a person on a horse jumps over a broken down airplane . "
     ]
    }
   ],
   "source": [
    "training_data = SiameseDataLoader(sentence_pairs_train, pad_tok)\n",
    "s = training_data.sentence_pairs[0][0]\n",
    "t = training_data.fill_tensor([s], len(s))\n",
    "for s in t[0]:\n",
    "    print(itos[int(s[0])], end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Siamese network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, encoder, linear):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.linear = linear\n",
    "    \n",
    "    def pool(self, x, bs, is_max):\n",
    "        f = F.adaptive_max_pool1d if is_max else F.adaptive_avg_pool1d\n",
    "        return f(x.permute(1,2,0), (1,)).view(bs,-1)\n",
    "\n",
    "    def pool_outputs(self, output):\n",
    "        sl, bs,_ = output.size()\n",
    "        #avgpool = self.pool(output, bs, False)\n",
    "        maxpool = self.pool(output, bs, True)\n",
    "        #return torch.cat([output[-1], maxpool, avgpool], 1)\n",
    "        return maxpool\n",
    "        \n",
    "    def forward_once(self, input):\n",
    "        raw_outputs, outputs = self.encoder(input)\n",
    "        out = self.pool_outputs(outputs[-1])\n",
    "        return out\n",
    "    \n",
    "    def forward(self, in1, in2):\n",
    "        u = self.forward_once(in1)\n",
    "        v = self.forward_once(in2)\n",
    "        features = torch.cat((u, v, torch.abs(u-v), u*v), 1)\n",
    "        out = self.linear(features)\n",
    "        return out \n",
    "        \n",
    "    def reset(self):\n",
    "        for c in self.children():\n",
    "            if hasattr(c, 'reset'): c.reset()\n",
    "\n",
    "class LinearClassifier(nn.Module):\n",
    "    def __init__(self, layers, dropout):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([LinearBlock(layers[i], layers[i + 1], dropout) for i in range(len(layers) - 1)])\n",
    "        \n",
    "    def forward(self, input):\n",
    "        x = input\n",
    "        for l in self.layers:\n",
    "            l_x = l(x)\n",
    "            x = F.relu(l_x)\n",
    "        return l_x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#these are the values used for the original LM\n",
    "em_sz, nh, nl = 400, 1150, 3\n",
    "bptt = 70\n",
    "max_seq = bptt * 20\n",
    "cats = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load our pretrained model then build the Siamese network from it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop\n",
    "This should be converted over to the fast.ai learner but I'm not sure how to do that yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_interval = 1000\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#criterion = nn.CosineEmbeddingLoss()\n",
    "\n",
    "def evaluate(model, data_loader):\n",
    "    # Turn on evaluation mode which disables dropout.\n",
    "    model.eval()\n",
    "    \n",
    "    total_loss = 0.\n",
    "    num_correct = 0\n",
    "    total = 0 \n",
    "    \n",
    "    for a, b, l in data_loader:\n",
    "        \n",
    "        model.reset()\n",
    "        a, b, l = Variable(a), Variable(b), Variable(l)\n",
    "        out = model(a.squeeze(), b.squeeze())\n",
    "        loss = criterion(out, l.squeeze())\n",
    "        total += l.size(0)\n",
    "        total_loss += l.size(0) * loss.item()\n",
    "        num_correct += np.sum(l.data.cpu().numpy() == np.argmax(out.data.cpu().numpy(), 1))\n",
    "        \n",
    "    return (total_loss / total, num_correct / total)\n",
    "\n",
    "def train(model, data_loader, optimizer):\n",
    "    # Turn on training mode which enables dropout.\n",
    "    start_time = time.time()\n",
    "    model.train() \n",
    "    \n",
    "    total_loss = 0.\n",
    "    num_correct = 0\n",
    "    total = 0 \n",
    "        \n",
    "    for a, b, l in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        model.reset()\n",
    "        a, b, l = Variable(a), Variable(b), Variable(l)\n",
    "        out = model(a.squeeze(), b.squeeze())\n",
    "        loss = criterion(out, l.squeeze())\n",
    "        total += l.size(0)\n",
    "        total_loss += l.size(0) * loss.item()\n",
    "        num_correct += np.sum(l.data.cpu().numpy() == np.argmax(out.data.cpu().numpy(), 1))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch = data_loader.batch()\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / total\n",
    "            elapsed = time.time() - start_time\n",
    "            batches = len(data_loader)\n",
    "            ms = elapsed * 1000 / log_interval\n",
    "            print(f'| {batch:5d}/{batches:5d} batches', end=\" \")\n",
    "            print(f'| ms/batch {ms:5.2f} | loss {cur_loss:5.4f} acc {num_correct / total}')\n",
    "            #print(f'| ms/batch {ms:5.2f} | loss {cur_loss:5.4f}')\n",
    "            total_loss = 0\n",
    "            total = 0\n",
    "            num_correct = 0\n",
    "            start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_loss = 100\n",
    "def training_loop(model, epochs, optimizer, scheduler = None):\n",
    "    \n",
    "    global best_loss\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        print(f'Start epoch {epoch:3d} training with lr ', end=\"\")\n",
    "        for g in optimizer.param_groups:\n",
    "            print(g['lr'], end=\" \")\n",
    "        print(\"\")\n",
    "        \n",
    "        training_data = SiameseDataLoader(sentence_pairs_train, pad_tok)\n",
    "        training_data.shuffle()\n",
    "\n",
    "        epoch_start_time = time.time()\n",
    "        \n",
    "        train(siamese_model, training_data, optimizer)\n",
    "        if scheduler != None:\n",
    "            scheduler.step()\n",
    "\n",
    "        dev_data = SiameseDataLoader(sentence_pairs_dev, pad_tok)\n",
    "        val_loss, accuracy = evaluate(siamese_model, dev_data)\n",
    "\n",
    "        delta_t = (time.time() - epoch_start_time)\n",
    "        print('-' * 89)\n",
    "        print(f'| end of epoch {epoch:3d} | time: {delta_t:5.2f}s | valid loss {val_loss:5.2f} accuracy {accuracy} learning rates')\n",
    "        for g in optimizer.param_groups:\n",
    "            print(g['lr'])\n",
    "        print('-' * 89)\n",
    "\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            with open(f'./siamese_model{val_loss:0.2f}{accuracy:0.2f}.pt', 'wb') as f:\n",
    "                torch.save(siamese_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import butter, filtfilt\n",
    "def butter_lowpass(cutoff, fs, order=5):\n",
    "    nyq = 0.5 * fs\n",
    "    normal_cutoff = cutoff / nyq\n",
    "    b, a = butter(order, normal_cutoff, btype='low', analog=False)\n",
    "    return b, a\n",
    "\n",
    "def butter_lowpass_filtfilt(data, cutoff, fs, order=5):\n",
    "    b, a = butter_lowpass(cutoff, fs, order=order)\n",
    "    y = filtfilt(b, a, data)\n",
    "    return y\n",
    "\n",
    "def plot_loss(losses):\n",
    "    plt.semilogx(losses[:,0], losses[:,1])\n",
    "    plt.semilogx(losses[:,0], butter_lowpass_filtfilt(losses[:,1], 300, 5000))\n",
    "    plt.show()\n",
    "\n",
    "def find_lr(model, model_to_optim, data_loader):\n",
    "    losses = []\n",
    "    model.train() \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    lr = 0.00001\n",
    "    for a, b, l in data_loader:\n",
    "        optimizer = optim.SGD(model_to_optim.parameters(), lr=lr)\n",
    "        #optimizer = optim.Adam(model_to_optim.parameters(), lr=lr)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        model.reset()\n",
    "        a, b, l = Variable(a), Variable(b), Variable(l)\n",
    "        out = model(a.squeeze(), b.squeeze())\n",
    "        loss = criterion(out, l.squeeze())\n",
    "        \n",
    "        los_val = loss.item()\n",
    "        losses.append((lr, los_val))\n",
    "        if los_val > 5:\n",
    "            break\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        lr *= 1.05\n",
    "    losses = np.array(losses)\n",
    "    #plot_loss(losses)\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "SNLI_LM = torch.load(\"snli_language_model.pt\")\n",
    "\n",
    "dps = np.array([0.4,0.5,0.05,0.3,0.4])*0.1\n",
    "SNLI_encoder = MultiBatchRNN(bptt, max_seq, vocab_size, em_sz, nh, nl, pad_tok, dropouti=dps[0], wdrop=dps[2], dropoute=dps[3], dropouth=dps[4])\n",
    "\n",
    "SNLI_encoder.load_state_dict(SNLI_LM[0].state_dict())\n",
    "\n",
    "#2 pooled vectors, of 3 times the embedding size\n",
    "siamese_model = SiameseClassifier(SNLI_encoder, LinearClassifier(layers=[em_sz*4, nh, 3], dropout=0.1)).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dev_data = SiameseDataLoader(sentence_pairs_dev, pad_tok)\n",
    "losses = find_lr(siamese_model, siamese_model, dev_data)\n",
    "plot_loss(np.array(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_model = SiameseClassifier(SNLI_encoder, LinearClassifier(layers=[em_sz*4, nh, 3], dropout=0.1)).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#siamese_model = torch.load(\"siamese_model0.73.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start epoch   0 training with lr 0.01 \n",
      "|  1000/17167 batches | ms/batch 15.29 | loss 0.8920 acc 0.59934375\n",
      "|  2000/17167 batches | ms/batch 16.33 | loss 0.8808 acc 0.60240625\n",
      "|  3000/17167 batches | ms/batch 16.95 | loss 0.8725 acc 0.6038125\n",
      "|  4000/17167 batches | ms/batch 17.54 | loss 0.8861 acc 0.5920625\n",
      "|  5000/17167 batches | ms/batch 17.97 | loss 0.8720 acc 0.60134375\n",
      "|  6000/17167 batches | ms/batch 18.41 | loss 0.8790 acc 0.60025\n",
      "|  7000/17167 batches | ms/batch 18.85 | loss 0.8780 acc 0.59765625\n",
      "|  8000/17167 batches | ms/batch 19.40 | loss 0.8821 acc 0.59428125\n",
      "|  9000/17167 batches | ms/batch 19.83 | loss 0.8848 acc 0.59696875\n",
      "| 10000/17167 batches | ms/batch 20.40 | loss 0.8744 acc 0.5954375\n",
      "| 11000/17167 batches | ms/batch 21.05 | loss 0.8753 acc 0.596875\n",
      "| 12000/17167 batches | ms/batch 21.85 | loss 0.8786 acc 0.5935625\n",
      "| 13000/17167 batches | ms/batch 22.62 | loss 0.8868 acc 0.590625\n",
      "| 14000/17167 batches | ms/batch 23.55 | loss 0.8830 acc 0.59175\n",
      "| 15000/17167 batches | ms/batch 24.68 | loss 0.8860 acc 0.591625\n",
      "| 16000/17167 batches | ms/batch 26.42 | loss 0.8958 acc 0.5845625\n",
      "| 17000/17167 batches | ms/batch 29.82 | loss 0.8907 acc 0.58740625\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   0 | time: 363.84s | valid loss  0.89 accuracy 0.595509042877464 learning rates\n",
      "0.01\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brian/.conda/envs/pytorch/lib/python3.6/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type SiameseClassifier. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/brian/.conda/envs/pytorch/lib/python3.6/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type LinearClassifier. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "for param in siamese_model.encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "optimizer = optim.SGD(siamese_model.parameters(), lr=0.01)\n",
    "training_loop(siamese_model.linear, 1, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start epoch   0 training with lr 0.01 \n",
      "|  1000/17167 batches | ms/batch 31.02 | loss 0.7931 acc 0.6626875\n",
      "|  2000/17167 batches | ms/batch 33.63 | loss 0.7997 acc 0.65134375\n",
      "|  3000/17167 batches | ms/batch 35.14 | loss 0.7985 acc 0.64725\n",
      "|  4000/17167 batches | ms/batch 36.65 | loss 0.8120 acc 0.640875\n",
      "|  5000/17167 batches | ms/batch 37.83 | loss 0.8079 acc 0.6416875\n",
      "|  6000/17167 batches | ms/batch 38.99 | loss 0.8127 acc 0.63871875\n",
      "|  7000/17167 batches | ms/batch 40.04 | loss 0.8189 acc 0.633\n",
      "|  8000/17167 batches | ms/batch 41.36 | loss 0.8203 acc 0.63290625\n",
      "|  9000/17167 batches | ms/batch 42.39 | loss 0.8270 acc 0.63040625\n",
      "| 10000/17167 batches | ms/batch 43.79 | loss 0.8158 acc 0.63325\n",
      "| 11000/17167 batches | ms/batch 45.36 | loss 0.8184 acc 0.6334375\n",
      "| 12000/17167 batches | ms/batch 47.25 | loss 0.8208 acc 0.631\n",
      "| 13000/17167 batches | ms/batch 49.08 | loss 0.8318 acc 0.625\n",
      "| 14000/17167 batches | ms/batch 51.23 | loss 0.8348 acc 0.62496875\n",
      "| 15000/17167 batches | ms/batch 53.97 | loss 0.8370 acc 0.62071875\n",
      "| 16000/17167 batches | ms/batch 58.20 | loss 0.8472 acc 0.614\n",
      "| 17000/17167 batches | ms/batch 66.46 | loss 0.8449 acc 0.61521875\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   0 | time: 773.55s | valid loss  0.82 accuracy 0.630359682991262 learning rates\n",
      "0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "Start epoch   1 training with lr 0.01 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brian/.conda/envs/pytorch/lib/python3.6/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type SiameseClassifier. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/brian/.conda/envs/pytorch/lib/python3.6/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type LinearClassifier. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|  1000/17167 batches | ms/batch 31.15 | loss 0.7419 acc 0.68596875\n",
      "|  2000/17167 batches | ms/batch 33.79 | loss 0.7443 acc 0.68009375\n",
      "|  3000/17167 batches | ms/batch 35.27 | loss 0.7545 acc 0.6748125\n",
      "|  4000/17167 batches | ms/batch 36.81 | loss 0.7654 acc 0.667875\n",
      "|  5000/17167 batches | ms/batch 37.96 | loss 0.7656 acc 0.6655\n",
      "|  6000/17167 batches | ms/batch 39.01 | loss 0.7684 acc 0.66215625\n",
      "|  7000/17167 batches | ms/batch 40.10 | loss 0.7758 acc 0.65915625\n",
      "|  8000/17167 batches | ms/batch 41.39 | loss 0.7813 acc 0.656125\n",
      "|  9000/17167 batches | ms/batch 42.44 | loss 0.7864 acc 0.65440625\n",
      "| 10000/17167 batches | ms/batch 43.83 | loss 0.7824 acc 0.65353125\n",
      "| 11000/17167 batches | ms/batch 45.40 | loss 0.7829 acc 0.65371875\n",
      "| 12000/17167 batches | ms/batch 47.31 | loss 0.7846 acc 0.65296875\n",
      "| 13000/17167 batches | ms/batch 49.14 | loss 0.7956 acc 0.64825\n",
      "| 14000/17167 batches | ms/batch 51.28 | loss 0.8038 acc 0.64090625\n",
      "| 15000/17167 batches | ms/batch 54.00 | loss 0.8049 acc 0.64071875\n",
      "| 16000/17167 batches | ms/batch 58.26 | loss 0.8221 acc 0.62728125\n",
      "| 17000/17167 batches | ms/batch 66.46 | loss 0.8183 acc 0.63275\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 774.74s | valid loss  0.81 accuracy 0.6429587482219061 learning rates\n",
      "0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "Start epoch   2 training with lr 0.01 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brian/.conda/envs/pytorch/lib/python3.6/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type SiameseClassifier. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/brian/.conda/envs/pytorch/lib/python3.6/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type LinearClassifier. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|  1000/17167 batches | ms/batch 31.19 | loss 0.7016 acc 0.7098125\n",
      "|  2000/17167 batches | ms/batch 33.83 | loss 0.7137 acc 0.69865625\n",
      "|  3000/17167 batches | ms/batch 35.33 | loss 0.7233 acc 0.69090625\n",
      "|  4000/17167 batches | ms/batch 36.89 | loss 0.7279 acc 0.6885625\n",
      "|  5000/17167 batches | ms/batch 37.98 | loss 0.7333 acc 0.68228125\n",
      "|  6000/17167 batches | ms/batch 39.12 | loss 0.7413 acc 0.67853125\n",
      "|  7000/17167 batches | ms/batch 40.10 | loss 0.7450 acc 0.6763125\n",
      "|  8000/17167 batches | ms/batch 41.48 | loss 0.7541 acc 0.67003125\n",
      "|  9000/17167 batches | ms/batch 42.45 | loss 0.7611 acc 0.66671875\n",
      "| 10000/17167 batches | ms/batch 43.93 | loss 0.7528 acc 0.67075\n",
      "| 11000/17167 batches | ms/batch 45.49 | loss 0.7514 acc 0.66953125\n",
      "| 12000/17167 batches | ms/batch 47.37 | loss 0.7606 acc 0.66553125\n",
      "| 13000/17167 batches | ms/batch 49.12 | loss 0.7739 acc 0.6573125\n",
      "| 14000/17167 batches | ms/batch 51.32 | loss 0.7750 acc 0.659125\n",
      "| 15000/17167 batches | ms/batch 54.05 | loss 0.7803 acc 0.653625\n",
      "| 16000/17167 batches | ms/batch 58.33 | loss 0.7921 acc 0.646\n",
      "| 17000/17167 batches | ms/batch 66.65 | loss 0.7973 acc 0.64371875\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 775.82s | valid loss  0.80 accuracy 0.6544401544401545 learning rates\n",
      "0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "Start epoch   3 training with lr 0.01 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brian/.conda/envs/pytorch/lib/python3.6/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type SiameseClassifier. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/brian/.conda/envs/pytorch/lib/python3.6/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type LinearClassifier. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|  1000/17167 batches | ms/batch 31.22 | loss 0.6727 acc 0.7238125\n",
      "|  2000/17167 batches | ms/batch 33.86 | loss 0.6886 acc 0.71209375\n",
      "|  3000/17167 batches | ms/batch 35.38 | loss 0.6998 acc 0.70090625\n",
      "|  4000/17167 batches | ms/batch 36.88 | loss 0.7058 acc 0.700125\n",
      "|  5000/17167 batches | ms/batch 38.05 | loss 0.7068 acc 0.6966875\n",
      "|  6000/17167 batches | ms/batch 39.11 | loss 0.7154 acc 0.69284375\n",
      "|  7000/17167 batches | ms/batch 40.15 | loss 0.7232 acc 0.68696875\n",
      "|  8000/17167 batches | ms/batch 41.47 | loss 0.7263 acc 0.68375\n",
      "|  9000/17167 batches | ms/batch 42.52 | loss 0.7395 acc 0.6788125\n",
      "| 10000/17167 batches | ms/batch 43.91 | loss 0.7347 acc 0.68146875\n",
      "| 11000/17167 batches | ms/batch 45.48 | loss 0.7306 acc 0.6825625\n",
      "| 12000/17167 batches | ms/batch 47.39 | loss 0.7373 acc 0.6795625\n",
      "| 13000/17167 batches | ms/batch 49.24 | loss 0.7465 acc 0.6734375\n",
      "| 14000/17167 batches | ms/batch 51.39 | loss 0.7545 acc 0.67146875\n",
      "| 15000/17167 batches | ms/batch 54.12 | loss 0.7577 acc 0.66528125\n",
      "| 16000/17167 batches | ms/batch 58.39 | loss 0.7760 acc 0.6546875\n",
      "| 17000/17167 batches | ms/batch 66.67 | loss 0.7740 acc 0.658\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 776.34s | valid loss  0.84 accuracy 0.6321885795570006 learning rates\n",
      "0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "Start epoch   4 training with lr 0.01 \n",
      "|  1000/17167 batches | ms/batch 31.23 | loss 0.6472 acc 0.731375\n",
      "|  2000/17167 batches | ms/batch 33.87 | loss 0.6653 acc 0.72046875\n",
      "|  3000/17167 batches | ms/batch 35.39 | loss 0.6738 acc 0.7161875\n",
      "|  4000/17167 batches | ms/batch 36.89 | loss 0.6822 acc 0.7099375\n",
      "|  5000/17167 batches | ms/batch 37.99 | loss 0.6819 acc 0.70834375\n",
      "|  6000/17167 batches | ms/batch 39.09 | loss 0.6902 acc 0.707625\n",
      "|  7000/17167 batches | ms/batch 40.15 | loss 0.7029 acc 0.69803125\n",
      "|  8000/17167 batches | ms/batch 41.40 | loss 0.7067 acc 0.69475\n",
      "|  9000/17167 batches | ms/batch 42.47 | loss 0.7187 acc 0.6895\n",
      "| 10000/17167 batches | ms/batch 43.89 | loss 0.7122 acc 0.69365625\n",
      "| 11000/17167 batches | ms/batch 45.43 | loss 0.7101 acc 0.69453125\n",
      "| 12000/17167 batches | ms/batch 47.30 | loss 0.7152 acc 0.6910625\n",
      "| 13000/17167 batches | ms/batch 49.13 | loss 0.7277 acc 0.68221875\n",
      "| 14000/17167 batches | ms/batch 51.29 | loss 0.7375 acc 0.67809375\n",
      "| 15000/17167 batches | ms/batch 54.02 | loss 0.7412 acc 0.67609375\n",
      "| 16000/17167 batches | ms/batch 58.28 | loss 0.7573 acc 0.6665625\n",
      "| 17000/17167 batches | ms/batch 66.48 | loss 0.7611 acc 0.6631875\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 775.46s | valid loss  0.75 accuracy 0.6788254419833367 learning rates\n",
      "0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "Start epoch   5 training with lr 0.01 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brian/.conda/envs/pytorch/lib/python3.6/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type SiameseClassifier. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/brian/.conda/envs/pytorch/lib/python3.6/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type LinearClassifier. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|  1000/17167 batches | ms/batch 31.18 | loss 0.6294 acc 0.74484375\n",
      "|  2000/17167 batches | ms/batch 33.84 | loss 0.6419 acc 0.73315625\n",
      "|  3000/17167 batches | ms/batch 35.33 | loss 0.6584 acc 0.72403125\n",
      "|  4000/17167 batches | ms/batch 36.84 | loss 0.6645 acc 0.72028125\n",
      "|  5000/17167 batches | ms/batch 38.02 | loss 0.6605 acc 0.7186875\n",
      "|  6000/17167 batches | ms/batch 39.08 | loss 0.6691 acc 0.71540625\n",
      "|  7000/17167 batches | ms/batch 40.15 | loss 0.6784 acc 0.71065625\n",
      "|  8000/17167 batches | ms/batch 41.45 | loss 0.6877 acc 0.70453125\n",
      "|  9000/17167 batches | ms/batch 42.49 | loss 0.6952 acc 0.70153125\n",
      "| 10000/17167 batches | ms/batch 43.89 | loss 0.6927 acc 0.705125\n",
      "| 11000/17167 batches | ms/batch 45.48 | loss 0.6943 acc 0.702375\n",
      "| 12000/17167 batches | ms/batch 47.36 | loss 0.7043 acc 0.6954375\n",
      "| 13000/17167 batches | ms/batch 49.20 | loss 0.7096 acc 0.69225\n",
      "| 14000/17167 batches | ms/batch 51.37 | loss 0.7207 acc 0.68825\n",
      "| 15000/17167 batches | ms/batch 54.10 | loss 0.7205 acc 0.68759375\n",
      "| 16000/17167 batches | ms/batch 58.35 | loss 0.7387 acc 0.67615625\n",
      "| 17000/17167 batches | ms/batch 66.63 | loss 0.7470 acc 0.67078125\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 775.93s | valid loss  0.79 accuracy 0.6574883153830522 learning rates\n",
      "0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "Start epoch   6 training with lr 0.01 \n",
      "|  1000/17167 batches | ms/batch 31.15 | loss 0.6100 acc 0.75021875\n",
      "|  2000/17167 batches | ms/batch 33.78 | loss 0.6275 acc 0.7395\n",
      "|  3000/17167 batches | ms/batch 35.25 | loss 0.6388 acc 0.7321875\n",
      "|  4000/17167 batches | ms/batch 36.78 | loss 0.6438 acc 0.73428125\n",
      "|  5000/17167 batches | ms/batch 37.97 | loss 0.6451 acc 0.72778125\n",
      "|  6000/17167 batches | ms/batch 39.05 | loss 0.6521 acc 0.72575\n",
      "|  7000/17167 batches | ms/batch 40.11 | loss 0.6650 acc 0.71671875\n",
      "|  8000/17167 batches | ms/batch 41.39 | loss 0.6701 acc 0.71359375\n",
      "|  9000/17167 batches | ms/batch 42.48 | loss 0.6819 acc 0.70909375\n",
      "| 10000/17167 batches | ms/batch 43.89 | loss 0.6785 acc 0.70903125\n",
      "| 11000/17167 batches | ms/batch 45.41 | loss 0.6771 acc 0.70940625\n",
      "| 12000/17167 batches | ms/batch 47.31 | loss 0.6862 acc 0.70784375\n",
      "| 13000/17167 batches | ms/batch 49.13 | loss 0.6969 acc 0.6973125\n",
      "| 14000/17167 batches | ms/batch 51.29 | loss 0.7053 acc 0.698875\n",
      "| 15000/17167 batches | ms/batch 54.04 | loss 0.7068 acc 0.69484375\n",
      "| 16000/17167 batches | ms/batch 58.26 | loss 0.7267 acc 0.681375\n",
      "| 17000/17167 batches | ms/batch 66.51 | loss 0.7290 acc 0.68403125\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time: 774.96s | valid loss  0.75 accuracy 0.6770981507823614 learning rates\n",
      "0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "Start epoch   7 training with lr 0.01 \n",
      "|  1000/17167 batches | ms/batch 31.28 | loss 0.5947 acc 0.7586875\n",
      "|  2000/17167 batches | ms/batch 33.95 | loss 0.6093 acc 0.74975\n",
      "|  3000/17167 batches | ms/batch 35.45 | loss 0.6251 acc 0.74065625\n",
      "|  4000/17167 batches | ms/batch 36.92 | loss 0.6340 acc 0.7356875\n",
      "|  5000/17167 batches | ms/batch 38.06 | loss 0.6288 acc 0.7354375\n",
      "|  6000/17167 batches | ms/batch 39.11 | loss 0.6342 acc 0.7341875\n",
      "|  7000/17167 batches | ms/batch 40.10 | loss 0.6475 acc 0.72525\n",
      "|  8000/17167 batches | ms/batch 41.40 | loss 0.6540 acc 0.72075\n",
      "|  9000/17167 batches | ms/batch 42.50 | loss 0.6644 acc 0.719375\n",
      "| 10000/17167 batches | ms/batch 43.87 | loss 0.6633 acc 0.71809375\n",
      "| 11000/17167 batches | ms/batch 45.50 | loss 0.6596 acc 0.7178125\n",
      "| 12000/17167 batches | ms/batch 47.41 | loss 0.6704 acc 0.71259375\n",
      "| 13000/17167 batches | ms/batch 49.23 | loss 0.6793 acc 0.70809375\n",
      "| 14000/17167 batches | ms/batch 51.41 | loss 0.6877 acc 0.7030625\n",
      "| 15000/17167 batches | ms/batch 54.12 | loss 0.6947 acc 0.70075\n",
      "| 16000/17167 batches | ms/batch 58.39 | loss 0.7113 acc 0.6925\n",
      "| 17000/17167 batches | ms/batch 66.61 | loss 0.7189 acc 0.68603125\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time: 776.47s | valid loss  0.76 accuracy 0.6787238366185735 learning rates\n",
      "0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "Start epoch   8 training with lr 0.01 \n",
      "|  1000/17167 batches | ms/batch 31.24 | loss 0.5805 acc 0.76246875\n",
      "|  2000/17167 batches | ms/batch 33.91 | loss 0.5975 acc 0.754875\n",
      "|  3000/17167 batches | ms/batch 35.40 | loss 0.6055 acc 0.7505625\n",
      "|  4000/17167 batches | ms/batch 36.88 | loss 0.6127 acc 0.74553125\n",
      "|  5000/17167 batches | ms/batch 37.98 | loss 0.6141 acc 0.74178125\n",
      "|  6000/17167 batches | ms/batch 39.13 | loss 0.6218 acc 0.7418125\n",
      "|  7000/17167 batches | ms/batch 40.13 | loss 0.6306 acc 0.73446875\n",
      "|  8000/17167 batches | ms/batch 41.40 | loss 0.6362 acc 0.7271875\n",
      "|  9000/17167 batches | ms/batch 42.46 | loss 0.6506 acc 0.7273125\n",
      "| 10000/17167 batches | ms/batch 43.89 | loss 0.6443 acc 0.72646875\n",
      "| 11000/17167 batches | ms/batch 45.47 | loss 0.6477 acc 0.7239375\n",
      "| 12000/17167 batches | ms/batch 47.37 | loss 0.6563 acc 0.72059375\n",
      "| 13000/17167 batches | ms/batch 49.35 | loss 0.6680 acc 0.7159375\n",
      "| 14000/17167 batches | ms/batch 51.45 | loss 0.6742 acc 0.71278125\n",
      "| 15000/17167 batches | ms/batch 54.05 | loss 0.6781 acc 0.70759375\n",
      "| 16000/17167 batches | ms/batch 58.27 | loss 0.6976 acc 0.69915625\n",
      "| 17000/17167 batches | ms/batch 66.54 | loss 0.7044 acc 0.69428125\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time: 776.06s | valid loss  0.78 accuracy 0.662873399715505 learning rates\n",
      "0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "Start epoch   9 training with lr 0.01 \n",
      "|  1000/17167 batches | ms/batch 31.30 | loss 0.5690 acc 0.76815625\n",
      "|  2000/17167 batches | ms/batch 33.95 | loss 0.5834 acc 0.7585\n",
      "|  3000/17167 batches | ms/batch 35.45 | loss 0.5937 acc 0.75453125\n",
      "|  4000/17167 batches | ms/batch 36.82 | loss 0.5981 acc 0.75246875\n",
      "|  5000/17167 batches | ms/batch 37.93 | loss 0.6011 acc 0.7473125\n",
      "|  6000/17167 batches | ms/batch 39.04 | loss 0.6115 acc 0.745875\n",
      "|  7000/17167 batches | ms/batch 40.06 | loss 0.6167 acc 0.74428125\n",
      "|  8000/17167 batches | ms/batch 41.38 | loss 0.6192 acc 0.738\n",
      "|  9000/17167 batches | ms/batch 42.44 | loss 0.6356 acc 0.73303125\n",
      "| 10000/17167 batches | ms/batch 43.85 | loss 0.6323 acc 0.7346875\n",
      "| 11000/17167 batches | ms/batch 45.52 | loss 0.6370 acc 0.727625\n",
      "| 12000/17167 batches | ms/batch 47.41 | loss 0.6417 acc 0.72990625\n",
      "| 13000/17167 batches | ms/batch 49.20 | loss 0.6525 acc 0.720375\n",
      "| 14000/17167 batches | ms/batch 51.35 | loss 0.6630 acc 0.71634375\n",
      "| 15000/17167 batches | ms/batch 54.08 | loss 0.6659 acc 0.71603125\n",
      "| 16000/17167 batches | ms/batch 58.33 | loss 0.6840 acc 0.70575\n",
      "| 17000/17167 batches | ms/batch 66.55 | loss 0.6918 acc 0.7011875\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time: 775.84s | valid loss  0.78 accuracy 0.6606380816907133 learning rates\n",
      "0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "Start epoch  10 training with lr 0.01 \n",
      "|  1000/17167 batches | ms/batch 31.31 | loss 0.5554 acc 0.7748125\n",
      "|  2000/17167 batches | ms/batch 33.97 | loss 0.5711 acc 0.7655\n",
      "|  3000/17167 batches | ms/batch 35.50 | loss 0.5812 acc 0.76109375\n",
      "|  4000/17167 batches | ms/batch 36.94 | loss 0.5850 acc 0.7600625\n",
      "|  5000/17167 batches | ms/batch 38.02 | loss 0.5850 acc 0.75621875\n",
      "|  6000/17167 batches | ms/batch 39.11 | loss 0.5889 acc 0.75509375\n",
      "|  7000/17167 batches | ms/batch 40.16 | loss 0.6079 acc 0.74559375\n",
      "|  8000/17167 batches | ms/batch 41.55 | loss 0.6152 acc 0.7420625\n",
      "|  9000/17167 batches | ms/batch 42.51 | loss 0.6209 acc 0.7415625\n",
      "| 10000/17167 batches | ms/batch 43.98 | loss 0.6186 acc 0.7400625\n",
      "| 11000/17167 batches | ms/batch 45.59 | loss 0.6244 acc 0.7405\n",
      "| 12000/17167 batches | ms/batch 47.52 | loss 0.6282 acc 0.73478125\n",
      "| 13000/17167 batches | ms/batch 49.38 | loss 0.6375 acc 0.72796875\n",
      "| 14000/17167 batches | ms/batch 51.47 | loss 0.6510 acc 0.72259375\n",
      "| 15000/17167 batches | ms/batch 54.18 | loss 0.6511 acc 0.7251875\n",
      "| 16000/17167 batches | ms/batch 58.44 | loss 0.6724 acc 0.7125625\n",
      "| 17000/17167 batches | ms/batch 66.67 | loss 0.6814 acc 0.7065\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | time: 777.48s | valid loss  0.77 accuracy 0.6693761430603535 learning rates\n",
      "0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "Start epoch  11 training with lr 0.01 \n",
      "|  1000/17167 batches | ms/batch 31.19 | loss 0.5456 acc 0.779875\n",
      "|  2000/17167 batches | ms/batch 33.85 | loss 0.5528 acc 0.77628125\n",
      "|  3000/17167 batches | ms/batch 35.33 | loss 0.5692 acc 0.76634375\n",
      "|  4000/17167 batches | ms/batch 36.85 | loss 0.5799 acc 0.76371875\n",
      "|  5000/17167 batches | ms/batch 37.99 | loss 0.5727 acc 0.7604375\n",
      "|  6000/17167 batches | ms/batch 39.09 | loss 0.5836 acc 0.7576875\n",
      "|  7000/17167 batches | ms/batch 40.13 | loss 0.5947 acc 0.753\n",
      "|  8000/17167 batches | ms/batch 41.42 | loss 0.5996 acc 0.74796875\n",
      "|  9000/17167 batches | ms/batch 42.48 | loss 0.6079 acc 0.7465625\n",
      "| 10000/17167 batches | ms/batch 43.89 | loss 0.6072 acc 0.74775\n",
      "| 11000/17167 batches | ms/batch 45.45 | loss 0.6110 acc 0.741125\n",
      "| 12000/17167 batches | ms/batch 47.42 | loss 0.6210 acc 0.7409375\n",
      "| 13000/17167 batches | ms/batch 49.21 | loss 0.6266 acc 0.73425\n",
      "| 14000/17167 batches | ms/batch 51.36 | loss 0.6352 acc 0.73053125\n",
      "| 15000/17167 batches | ms/batch 54.07 | loss 0.6427 acc 0.72609375\n",
      "| 16000/17167 batches | ms/batch 58.37 | loss 0.6572 acc 0.72115625\n",
      "| 17000/17167 batches | ms/batch 66.53 | loss 0.6719 acc 0.7093125\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  11 | time: 775.83s | valid loss  0.73 accuracy 0.6882747409063199 learning rates\n",
      "0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "Start epoch  12 training with lr 0.01 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brian/.conda/envs/pytorch/lib/python3.6/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type SiameseClassifier. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/brian/.conda/envs/pytorch/lib/python3.6/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type LinearClassifier. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|  1000/17167 batches | ms/batch 31.22 | loss 0.5350 acc 0.7873125\n",
      "|  2000/17167 batches | ms/batch 33.84 | loss 0.5457 acc 0.7795625\n",
      "|  3000/17167 batches | ms/batch 35.33 | loss 0.5596 acc 0.770875\n",
      "|  4000/17167 batches | ms/batch 36.85 | loss 0.5603 acc 0.7710625\n",
      "|  5000/17167 batches | ms/batch 38.06 | loss 0.5659 acc 0.76559375\n",
      "|  6000/17167 batches | ms/batch 39.14 | loss 0.5665 acc 0.76803125\n",
      "|  7000/17167 batches | ms/batch 40.16 | loss 0.5814 acc 0.75990625\n",
      "|  8000/17167 batches | ms/batch 41.50 | loss 0.5839 acc 0.7569375\n",
      "|  9000/17167 batches | ms/batch 42.61 | loss 0.5985 acc 0.75140625\n",
      "| 10000/17167 batches | ms/batch 44.02 | loss 0.5945 acc 0.75275\n",
      "| 11000/17167 batches | ms/batch 45.63 | loss 0.5988 acc 0.7475\n",
      "| 12000/17167 batches | ms/batch 47.26 | loss 0.6037 acc 0.74709375\n",
      "| 13000/17167 batches | ms/batch 49.25 | loss 0.6103 acc 0.74240625\n",
      "| 14000/17167 batches | ms/batch 51.38 | loss 0.6296 acc 0.7379375\n",
      "| 15000/17167 batches | ms/batch 54.06 | loss 0.6336 acc 0.72996875\n",
      "| 16000/17167 batches | ms/batch 58.34 | loss 0.6494 acc 0.72128125\n",
      "| 17000/17167 batches | ms/batch 66.56 | loss 0.6557 acc 0.718375\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  12 | time: 776.39s | valid loss  0.72 accuracy 0.7033123348912823 learning rates\n",
      "0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "Start epoch  13 training with lr 0.01 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brian/.conda/envs/pytorch/lib/python3.6/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type SiameseClassifier. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/brian/.conda/envs/pytorch/lib/python3.6/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type LinearClassifier. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|  1000/17167 batches | ms/batch 31.23 | loss 0.5266 acc 0.78984375\n",
      "|  2000/17167 batches | ms/batch 33.92 | loss 0.5413 acc 0.78084375\n",
      "|  3000/17167 batches | ms/batch 35.41 | loss 0.5496 acc 0.77634375\n",
      "|  4000/17167 batches | ms/batch 36.93 | loss 0.5505 acc 0.77153125\n",
      "|  5000/17167 batches | ms/batch 38.05 | loss 0.5496 acc 0.7720625\n",
      "|  6000/17167 batches | ms/batch 39.11 | loss 0.5591 acc 0.771125\n",
      "|  7000/17167 batches | ms/batch 40.15 | loss 0.5714 acc 0.76240625\n",
      "|  8000/17167 batches | ms/batch 41.47 | loss 0.5731 acc 0.7624375\n",
      "|  9000/17167 batches | ms/batch 42.52 | loss 0.5860 acc 0.75675\n",
      "| 10000/17167 batches | ms/batch 43.89 | loss 0.5841 acc 0.7561875\n",
      "| 11000/17167 batches | ms/batch 45.47 | loss 0.5883 acc 0.75475\n",
      "| 12000/17167 batches | ms/batch 47.39 | loss 0.5928 acc 0.7533125\n",
      "| 13000/17167 batches | ms/batch 49.29 | loss 0.6013 acc 0.74671875\n",
      "| 14000/17167 batches | ms/batch 51.49 | loss 0.6118 acc 0.7441875\n",
      "| 15000/17167 batches | ms/batch 54.21 | loss 0.6127 acc 0.74278125\n",
      "| 16000/17167 batches | ms/batch 58.46 | loss 0.6352 acc 0.7306875\n",
      "| 17000/17167 batches | ms/batch 66.73 | loss 0.6446 acc 0.72653125\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  13 | time: 776.88s | valid loss  0.71 accuracy 0.7041251778093883 learning rates\n",
      "0.01\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brian/.conda/envs/pytorch/lib/python3.6/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type SiameseClassifier. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/brian/.conda/envs/pytorch/lib/python3.6/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type LinearClassifier. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start epoch  14 training with lr 0.01 \n",
      "|  1000/17167 batches | ms/batch 31.23 | loss 0.5172 acc 0.79334375\n",
      "|  2000/17167 batches | ms/batch 33.89 | loss 0.5272 acc 0.7868125\n",
      "|  3000/17167 batches | ms/batch 35.37 | loss 0.5404 acc 0.7786875\n",
      "|  4000/17167 batches | ms/batch 36.79 | loss 0.5426 acc 0.77984375\n",
      "|  5000/17167 batches | ms/batch 37.92 | loss 0.5405 acc 0.7803125\n",
      "|  6000/17167 batches | ms/batch 39.03 | loss 0.5457 acc 0.7756875\n",
      "|  7000/17167 batches | ms/batch 40.05 | loss 0.5560 acc 0.771875\n",
      "|  8000/17167 batches | ms/batch 41.39 | loss 0.5626 acc 0.7638125\n",
      "|  9000/17167 batches | ms/batch 42.44 | loss 0.5777 acc 0.76084375\n",
      "| 10000/17167 batches | ms/batch 43.85 | loss 0.5704 acc 0.76421875\n",
      "| 11000/17167 batches | ms/batch 45.35 | loss 0.5718 acc 0.7640625\n",
      "| 12000/17167 batches | ms/batch 47.38 | loss 0.5828 acc 0.7566875\n",
      "| 13000/17167 batches | ms/batch 49.20 | loss 0.5916 acc 0.7504375\n",
      "| 14000/17167 batches | ms/batch 51.38 | loss 0.5991 acc 0.750875\n",
      "| 15000/17167 batches | ms/batch 54.05 | loss 0.6062 acc 0.74428125\n",
      "| 16000/17167 batches | ms/batch 58.34 | loss 0.6295 acc 0.732875\n",
      "| 17000/17167 batches | ms/batch 66.58 | loss 0.6348 acc 0.7280625\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  14 | time: 775.41s | valid loss  0.78 accuracy 0.6936598252387726 learning rates\n",
      "0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "Start epoch  15 training with lr 0.01 \n",
      "|  1000/17167 batches | ms/batch 31.20 | loss 0.5015 acc 0.8011875\n",
      "|  2000/17167 batches | ms/batch 33.87 | loss 0.5144 acc 0.793\n",
      "|  3000/17167 batches | ms/batch 35.35 | loss 0.5298 acc 0.7834375\n",
      "|  4000/17167 batches | ms/batch 36.84 | loss 0.5288 acc 0.78440625\n",
      "|  5000/17167 batches | ms/batch 38.00 | loss 0.5324 acc 0.782375\n",
      "|  6000/17167 batches | ms/batch 39.15 | loss 0.5382 acc 0.77796875\n",
      "|  7000/17167 batches | ms/batch 40.18 | loss 0.5422 acc 0.77775\n",
      "|  8000/17167 batches | ms/batch 41.47 | loss 0.5505 acc 0.7711875\n",
      "|  9000/17167 batches | ms/batch 42.54 | loss 0.5652 acc 0.7665625\n",
      "| 10000/17167 batches | ms/batch 43.96 | loss 0.5668 acc 0.76503125\n",
      "| 11000/17167 batches | ms/batch 45.55 | loss 0.5669 acc 0.76490625\n",
      "| 12000/17167 batches | ms/batch 47.46 | loss 0.5736 acc 0.76296875\n",
      "| 13000/17167 batches | ms/batch 49.31 | loss 0.5800 acc 0.75878125\n",
      "| 14000/17167 batches | ms/batch 51.47 | loss 0.5929 acc 0.75221875\n",
      "| 15000/17167 batches | ms/batch 54.18 | loss 0.6050 acc 0.74671875\n",
      "| 16000/17167 batches | ms/batch 58.50 | loss 0.6194 acc 0.7378125\n",
      "| 17000/17167 batches | ms/batch 66.70 | loss 0.6259 acc 0.73178125\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  15 | time: 776.90s | valid loss  0.76 accuracy 0.6946758788864053 learning rates\n",
      "0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "Start epoch  16 training with lr 0.01 \n",
      "|  1000/17167 batches | ms/batch 31.29 | loss 0.4962 acc 0.8025625\n",
      "|  2000/17167 batches | ms/batch 33.98 | loss 0.5066 acc 0.79759375\n",
      "|  3000/17167 batches | ms/batch 35.51 | loss 0.5177 acc 0.78853125\n",
      "|  4000/17167 batches | ms/batch 36.95 | loss 0.5227 acc 0.788125\n",
      "|  5000/17167 batches | ms/batch 37.95 | loss 0.5208 acc 0.787625\n",
      "|  6000/17167 batches | ms/batch 39.08 | loss 0.5284 acc 0.78275\n",
      "|  7000/17167 batches | ms/batch 40.08 | loss 0.5330 acc 0.77928125\n",
      "|  8000/17167 batches | ms/batch 41.41 | loss 0.5416 acc 0.776625\n",
      "|  9000/17167 batches | ms/batch 42.43 | loss 0.5520 acc 0.77378125\n",
      "| 10000/17167 batches | ms/batch 43.97 | loss 0.5523 acc 0.77071875\n",
      "| 11000/17167 batches | ms/batch 45.58 | loss 0.5560 acc 0.77184375\n",
      "| 12000/17167 batches | ms/batch 47.54 | loss 0.5597 acc 0.7684375\n",
      "| 13000/17167 batches | ms/batch 49.40 | loss 0.5755 acc 0.75890625\n",
      "| 14000/17167 batches | ms/batch 51.51 | loss 0.5820 acc 0.7573125\n",
      "| 15000/17167 batches | ms/batch 54.19 | loss 0.5939 acc 0.7505625\n",
      "| 16000/17167 batches | ms/batch 58.44 | loss 0.6064 acc 0.7424375\n",
      "| 17000/17167 batches | ms/batch 66.70 | loss 0.6090 acc 0.74384375\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  16 | time: 777.17s | valid loss  0.78 accuracy 0.6961999593578541 learning rates\n",
      "0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "Start epoch  17 training with lr 0.01 \n",
      "|  1000/17167 batches | ms/batch 31.34 | loss 0.4872 acc 0.8058125\n",
      "|  2000/17167 batches | ms/batch 33.98 | loss 0.4939 acc 0.802125\n",
      "|  3000/17167 batches | ms/batch 35.39 | loss 0.5077 acc 0.796875\n",
      "|  4000/17167 batches | ms/batch 36.87 | loss 0.5143 acc 0.7925\n",
      "|  5000/17167 batches | ms/batch 38.01 | loss 0.5122 acc 0.7904375\n",
      "|  6000/17167 batches | ms/batch 39.12 | loss 0.5159 acc 0.78965625\n",
      "|  7000/17167 batches | ms/batch 40.14 | loss 0.5273 acc 0.7839375\n",
      "|  8000/17167 batches | ms/batch 41.45 | loss 0.5300 acc 0.77740625\n",
      "|  9000/17167 batches | ms/batch 42.52 | loss 0.5385 acc 0.7805\n",
      "| 10000/17167 batches | ms/batch 43.82 | loss 0.5420 acc 0.77578125\n",
      "| 11000/17167 batches | ms/batch 45.37 | loss 0.5461 acc 0.77428125\n",
      "| 12000/17167 batches | ms/batch 47.33 | loss 0.5536 acc 0.77\n",
      "| 13000/17167 batches | ms/batch 49.23 | loss 0.5638 acc 0.7645625\n",
      "| 14000/17167 batches | ms/batch 51.38 | loss 0.5668 acc 0.7639375\n",
      "| 15000/17167 batches | ms/batch 54.12 | loss 0.5810 acc 0.75953125\n",
      "| 16000/17167 batches | ms/batch 58.42 | loss 0.6006 acc 0.75078125\n",
      "| 17000/17167 batches | ms/batch 66.65 | loss 0.6042 acc 0.744625\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  17 | time: 776.31s | valid loss  0.80 accuracy 0.6940662466978257 learning rates\n",
      "0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "Start epoch  18 training with lr 0.01 \n",
      "|  1000/17167 batches | ms/batch 31.24 | loss 0.4757 acc 0.812\n",
      "|  2000/17167 batches | ms/batch 33.89 | loss 0.4870 acc 0.80621875\n",
      "|  3000/17167 batches | ms/batch 35.40 | loss 0.5008 acc 0.7946875\n",
      "|  4000/17167 batches | ms/batch 36.84 | loss 0.5003 acc 0.797375\n",
      "|  5000/17167 batches | ms/batch 37.94 | loss 0.5015 acc 0.796625\n",
      "|  6000/17167 batches | ms/batch 39.11 | loss 0.5068 acc 0.792375\n",
      "|  7000/17167 batches | ms/batch 40.13 | loss 0.5172 acc 0.78846875\n",
      "|  8000/17167 batches | ms/batch 41.43 | loss 0.5189 acc 0.78565625\n",
      "|  9000/17167 batches | ms/batch 42.48 | loss 0.5329 acc 0.7815625\n",
      "| 10000/17167 batches | ms/batch 43.90 | loss 0.5337 acc 0.7805625\n",
      "| 11000/17167 batches | ms/batch 45.43 | loss 0.5359 acc 0.7780625\n",
      "| 12000/17167 batches | ms/batch 47.32 | loss 0.5410 acc 0.77759375\n",
      "| 13000/17167 batches | ms/batch 49.15 | loss 0.5537 acc 0.77090625\n",
      "| 14000/17167 batches | ms/batch 51.31 | loss 0.5648 acc 0.76640625\n",
      "| 15000/17167 batches | ms/batch 54.08 | loss 0.5689 acc 0.76190625\n",
      "| 16000/17167 batches | ms/batch 58.31 | loss 0.5883 acc 0.75328125\n",
      "| 17000/17167 batches | ms/batch 66.57 | loss 0.5994 acc 0.74553125\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  18 | time: 775.67s | valid loss  0.74 accuracy 0.7097134728713677 learning rates\n",
      "0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "Start epoch  19 training with lr 0.01 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brian/.conda/envs/pytorch/lib/python3.6/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type SiameseClassifier. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/brian/.conda/envs/pytorch/lib/python3.6/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type LinearClassifier. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|  1000/17167 batches | ms/batch 31.16 | loss 0.4697 acc 0.81365625\n",
      "|  2000/17167 batches | ms/batch 33.81 | loss 0.4829 acc 0.80678125\n",
      "|  3000/17167 batches | ms/batch 35.29 | loss 0.4911 acc 0.8004375\n",
      "|  4000/17167 batches | ms/batch 36.79 | loss 0.4973 acc 0.80084375\n",
      "|  5000/17167 batches | ms/batch 37.94 | loss 0.4947 acc 0.79840625\n",
      "|  6000/17167 batches | ms/batch 39.10 | loss 0.5016 acc 0.79421875\n",
      "|  7000/17167 batches | ms/batch 40.17 | loss 0.5064 acc 0.79109375\n",
      "|  8000/17167 batches | ms/batch 41.38 | loss 0.5138 acc 0.78878125\n",
      "|  9000/17167 batches | ms/batch 42.43 | loss 0.5229 acc 0.784125\n",
      "| 10000/17167 batches | ms/batch 43.82 | loss 0.5265 acc 0.78553125\n",
      "| 11000/17167 batches | ms/batch 45.50 | loss 0.5298 acc 0.7818125\n",
      "| 12000/17167 batches | ms/batch 47.30 | loss 0.5344 acc 0.7796875\n",
      "| 13000/17167 batches | ms/batch 49.14 | loss 0.5418 acc 0.776\n",
      "| 14000/17167 batches | ms/batch 51.33 | loss 0.5532 acc 0.77009375\n",
      "| 15000/17167 batches | ms/batch 54.08 | loss 0.5623 acc 0.767\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-0a937540008b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.03\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.05\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.07\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.09\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.02\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.03\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.04\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.05\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.06\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msiamese_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mtraining_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msiamese_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-af3a1aaf1f2a>\u001b[0m in \u001b[0;36mtraining_loop\u001b[0;34m(model, epochs, optimizer, scheduler)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mepoch_start_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msiamese_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mscheduler\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-9a3ccc5bff4a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, data_loader, optimizer)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mnum_correct\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pytorch/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pytorch/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for param in siamese_model.encoder.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "for lr in [0.01, 0.03, 0.05, 0.07, 0.09, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06]:\n",
    "    optimizer = optim.SGD(siamese_model.parameters(), lr=lr)\n",
    "    training_loop(siamese_model, 20, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start epoch   0 training with lr 0.07 \n",
      "|  1000/17167 batches | ms/batch 30.97 | loss 0.3957 acc 0.84734375\n",
      "|  2000/17167 batches | ms/batch 33.69 | loss 0.4212 acc 0.833875\n",
      "|  3000/17167 batches | ms/batch 35.29 | loss 0.4405 acc 0.82371875\n",
      "|  4000/17167 batches | ms/batch 36.72 | loss 0.4443 acc 0.8230625\n",
      "|  5000/17167 batches | ms/batch 37.84 | loss 0.4452 acc 0.82325\n",
      "|  6000/17167 batches | ms/batch 38.95 | loss 0.4522 acc 0.81725\n",
      "|  7000/17167 batches | ms/batch 40.00 | loss 0.4623 acc 0.81278125\n",
      "|  8000/17167 batches | ms/batch 41.23 | loss 0.4709 acc 0.80628125\n",
      "|  9000/17167 batches | ms/batch 42.29 | loss 0.4825 acc 0.80440625\n",
      "| 10000/17167 batches | ms/batch 43.69 | loss 0.4764 acc 0.806875\n",
      "| 11000/17167 batches | ms/batch 45.28 | loss 0.4846 acc 0.8019375\n",
      "| 12000/17167 batches | ms/batch 47.21 | loss 0.4954 acc 0.79825\n",
      "| 13000/17167 batches | ms/batch 49.10 | loss 0.4975 acc 0.79640625\n",
      "| 14000/17167 batches | ms/batch 51.29 | loss 0.5128 acc 0.78984375\n",
      "| 15000/17167 batches | ms/batch 54.01 | loss 0.5217 acc 0.786\n",
      "| 16000/17167 batches | ms/batch 58.26 | loss 0.5303 acc 0.78190625\n",
      "| 17000/17167 batches | ms/batch 66.45 | loss 0.5471 acc 0.77125\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   0 | time: 773.39s | valid loss  0.73 accuracy 0.7320666531192846 learning rates\n",
      "0.07\n",
      "-----------------------------------------------------------------------------------------\n",
      "Start epoch   1 training with lr 0.07 \n",
      "|  1000/17167 batches | ms/batch 31.19 | loss 0.4146 acc 0.84253125\n",
      "|  2000/17167 batches | ms/batch 33.82 | loss 0.4284 acc 0.82878125\n",
      "|  3000/17167 batches | ms/batch 35.29 | loss 0.4393 acc 0.8243125\n",
      "|  4000/17167 batches | ms/batch 36.82 | loss 0.4467 acc 0.82259375\n",
      "|  5000/17167 batches | ms/batch 37.95 | loss 0.4410 acc 0.82453125\n",
      "|  6000/17167 batches | ms/batch 39.07 | loss 0.4448 acc 0.82128125\n",
      "|  7000/17167 batches | ms/batch 40.08 | loss 0.4526 acc 0.81853125\n",
      "|  8000/17167 batches | ms/batch 41.35 | loss 0.4599 acc 0.81428125\n",
      "|  9000/17167 batches | ms/batch 42.41 | loss 0.4703 acc 0.8096875\n",
      "| 10000/17167 batches | ms/batch 43.82 | loss 0.4662 acc 0.81025\n",
      "| 11000/17167 batches | ms/batch 45.39 | loss 0.4723 acc 0.80984375\n",
      "| 12000/17167 batches | ms/batch 47.33 | loss 0.4762 acc 0.80684375\n",
      "| 13000/17167 batches | ms/batch 49.23 | loss 0.4861 acc 0.80440625\n",
      "| 14000/17167 batches | ms/batch 51.44 | loss 0.5016 acc 0.7966875\n",
      "| 15000/17167 batches | ms/batch 54.18 | loss 0.5040 acc 0.7915\n",
      "| 16000/17167 batches | ms/batch 58.51 | loss 0.5184 acc 0.78684375\n",
      "| 17000/17167 batches | ms/batch 66.72 | loss 0.5285 acc 0.780875\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 775.80s | valid loss  0.71 accuracy 0.73846779109937 learning rates\n",
      "0.0683114498121828\n",
      "-----------------------------------------------------------------------------------------\n",
      "Start epoch   2 training with lr 0.0683114498121828 \n",
      "|  1000/17167 batches | ms/batch 31.14 | loss 0.4052 acc 0.844125\n",
      "|  2000/17167 batches | ms/batch 33.77 | loss 0.4155 acc 0.83575\n",
      "|  3000/17167 batches | ms/batch 35.25 | loss 0.4213 acc 0.8320625\n",
      "|  4000/17167 batches | ms/batch 36.75 | loss 0.4324 acc 0.82978125\n",
      "|  5000/17167 batches | ms/batch 37.87 | loss 0.4302 acc 0.82734375\n",
      "|  6000/17167 batches | ms/batch 38.99 | loss 0.4397 acc 0.82453125\n",
      "|  7000/17167 batches | ms/batch 40.00 | loss 0.4421 acc 0.8238125\n",
      "|  8000/17167 batches | ms/batch 41.28 | loss 0.4498 acc 0.81796875\n",
      "|  9000/17167 batches | ms/batch 42.35 | loss 0.4614 acc 0.815875\n",
      "| 10000/17167 batches | ms/batch 43.74 | loss 0.4567 acc 0.8166875\n",
      "| 11000/17167 batches | ms/batch 45.33 | loss 0.4620 acc 0.81425\n",
      "| 12000/17167 batches | ms/batch 47.30 | loss 0.4674 acc 0.8111875\n",
      "| 13000/17167 batches | ms/batch 49.15 | loss 0.4764 acc 0.80509375\n",
      "| 14000/17167 batches | ms/batch 51.29 | loss 0.4844 acc 0.80421875\n",
      "| 15000/17167 batches | ms/batch 54.02 | loss 0.4907 acc 0.7976875\n",
      "| 16000/17167 batches | ms/batch 58.23 | loss 0.5029 acc 0.794125\n",
      "| 17000/17167 batches | ms/batch 66.38 | loss 0.5126 acc 0.78959375\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 773.97s | valid loss  0.74 accuracy 0.732168258484048 learning rates\n",
      "0.06341108630593568\n",
      "-----------------------------------------------------------------------------------------\n",
      "Start epoch   3 training with lr 0.06341108630593568 \n",
      "|  1000/17167 batches | ms/batch 31.28 | loss 0.3915 acc 0.8488125\n",
      "|  2000/17167 batches | ms/batch 33.93 | loss 0.4030 acc 0.84209375\n",
      "|  3000/17167 batches | ms/batch 35.42 | loss 0.4142 acc 0.83396875\n",
      "|  4000/17167 batches | ms/batch 36.87 | loss 0.4207 acc 0.8331875\n",
      "|  5000/17167 batches | ms/batch 37.89 | loss 0.4209 acc 0.8323125\n",
      "|  6000/17167 batches | ms/batch 39.02 | loss 0.4197 acc 0.83203125\n",
      "|  7000/17167 batches | ms/batch 40.03 | loss 0.4302 acc 0.828\n",
      "|  8000/17167 batches | ms/batch 41.35 | loss 0.4312 acc 0.824125\n",
      "|  9000/17167 batches | ms/batch 42.42 | loss 0.4388 acc 0.82478125\n",
      "| 10000/17167 batches | ms/batch 43.82 | loss 0.4389 acc 0.8235625\n",
      "| 11000/17167 batches | ms/batch 45.41 | loss 0.4482 acc 0.8171875\n",
      "| 12000/17167 batches | ms/batch 47.31 | loss 0.4438 acc 0.8226875\n",
      "| 13000/17167 batches | ms/batch 49.16 | loss 0.4607 acc 0.8141875\n",
      "| 14000/17167 batches | ms/batch 51.36 | loss 0.4652 acc 0.812875\n",
      "| 15000/17167 batches | ms/batch 54.07 | loss 0.4738 acc 0.80678125\n",
      "| 16000/17167 batches | ms/batch 58.36 | loss 0.4857 acc 0.80334375\n",
      "| 17000/17167 batches | ms/batch 66.60 | loss 0.4903 acc 0.799375\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 775.50s | valid loss  0.73 accuracy 0.7404998983946353 learning rates\n",
      "0.05577859120409033\n",
      "-----------------------------------------------------------------------------------------\n",
      "Start epoch   4 training with lr 0.05577859120409033 \n",
      "|  1000/17167 batches | ms/batch 31.29 | loss 0.3845 acc 0.8535\n",
      "|  2000/17167 batches | ms/batch 33.94 | loss 0.3904 acc 0.84903125\n",
      "|  3000/17167 batches | ms/batch 35.43 | loss 0.4038 acc 0.839625\n",
      "|  4000/17167 batches | ms/batch 36.91 | loss 0.3986 acc 0.84371875\n",
      "|  5000/17167 batches | ms/batch 38.03 | loss 0.4064 acc 0.84084375\n",
      "|  6000/17167 batches | ms/batch 39.05 | loss 0.3993 acc 0.84340625\n",
      "|  7000/17167 batches | ms/batch 40.07 | loss 0.4085 acc 0.8403125\n",
      "|  8000/17167 batches | ms/batch 41.35 | loss 0.4143 acc 0.83575\n",
      "|  9000/17167 batches | ms/batch 42.39 | loss 0.4238 acc 0.833375\n",
      "| 10000/17167 batches | ms/batch 43.88 | loss 0.4110 acc 0.83709375\n",
      "| 11000/17167 batches | ms/batch 45.52 | loss 0.4281 acc 0.82840625\n",
      "| 12000/17167 batches | ms/batch 47.35 | loss 0.4281 acc 0.8288125\n",
      "| 13000/17167 batches | ms/batch 49.16 | loss 0.4416 acc 0.820875\n",
      "| 14000/17167 batches | ms/batch 51.34 | loss 0.4468 acc 0.81903125\n",
      "| 15000/17167 batches | ms/batch 54.06 | loss 0.4453 acc 0.8206875\n",
      "| 16000/17167 batches | ms/batch 58.33 | loss 0.4606 acc 0.81165625\n",
      "| 17000/17167 batches | ms/batch 66.51 | loss 0.4746 acc 0.80784375\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 775.75s | valid loss  0.75 accuracy 0.7403982930298719 learning rates\n",
      "0.04616108630593569\n",
      "-----------------------------------------------------------------------------------------\n",
      "Start epoch   5 training with lr 0.04616108630593569 \n",
      "|  1000/17167 batches | ms/batch 31.17 | loss 0.3702 acc 0.8590625\n",
      "|  2000/17167 batches | ms/batch 33.77 | loss 0.3809 acc 0.8513125\n",
      "|  3000/17167 batches | ms/batch 35.25 | loss 0.3919 acc 0.84734375\n",
      "|  4000/17167 batches | ms/batch 36.84 | loss 0.3851 acc 0.8485\n",
      "|  5000/17167 batches | ms/batch 37.99 | loss 0.3839 acc 0.84784375\n",
      "|  6000/17167 batches | ms/batch 39.11 | loss 0.3863 acc 0.8460625\n",
      "|  7000/17167 batches | ms/batch 40.09 | loss 0.3902 acc 0.8450625\n",
      "|  8000/17167 batches | ms/batch 41.38 | loss 0.3920 acc 0.8444375\n",
      "|  9000/17167 batches | ms/batch 42.51 | loss 0.4027 acc 0.841875\n",
      "| 10000/17167 batches | ms/batch 43.96 | loss 0.4002 acc 0.84190625\n",
      "| 11000/17167 batches | ms/batch 45.49 | loss 0.4078 acc 0.83615625\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-8d8407dec7d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msiamese_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.07\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mscheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCosineAnnealingLR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meta_min\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtraining_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msiamese_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-af3a1aaf1f2a>\u001b[0m in \u001b[0;36mtraining_loop\u001b[0;34m(model, epochs, optimizer, scheduler)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mepoch_start_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msiamese_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mscheduler\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-9a3ccc5bff4a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, data_loader, optimizer)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-80c9ddedfd9b>\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'reset'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mLinearClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/HDD/brian/blog_examples/SiameseULMFiT/fastai/lm_rnn.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqrnn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/HDD/brian/blog_examples/SiameseULMFiT/fastai/lm_rnn.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqrnn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/HDD/brian/blog_examples/SiameseULMFiT/fastai/lm_rnn.py\u001b[0m in \u001b[0;36mone_hidden\u001b[0;34m(self, l)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mone_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0mnh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_hid\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0ml\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_layers\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0memb_sz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mIS_TORCH_04\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvolatile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 10 \n",
    "optimizer = optim.SGD(siamese_model.parameters(), lr=0.07)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs, eta_min=0.001)\n",
    "training_loop(siamese_model, epochs, optimizer, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "entailed_a = []\n",
    "entailed_b = []\n",
    "contra_a = []\n",
    "contra_b = []\n",
    "netural_a = []\n",
    "netural_b = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def list2arr(l):\n",
    "    \"Convert list into pytorch Variable.\"\n",
    "    return V(np.expand_dims(np.array(l), -1))\n",
    "\n",
    "def make_prediction_from_list(model, l):\n",
    "    \"\"\"\n",
    "    Encode a list of integers that represent a sequence of tokens.  The\n",
    "    purpose is to encode a sentence or phrase.\n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "    model : fastai language model\n",
    "    l : list\n",
    "        list of integers, representing a sequence of tokens that you want to encode`\n",
    "\n",
    "    \"\"\"\n",
    "    arr = list2arr(l)# turn list into pytorch Variable with bs=1\n",
    "    model.reset()  # language model is stateful, so you must reset upon each prediction\n",
    "    hidden_states = model(arr)[-1][-1] # RNN Hidden Layer output is last output, and only need the last layer\n",
    "\n",
    "    #return avg-pooling, max-pooling, and last hidden state\n",
    "    return hidden_states.mean(0), hidden_states.max(0)[0], hidden_states[-1]\n",
    "\n",
    "\n",
    "def get_embeddings(encoder, list_list_int):\n",
    "    \"\"\"\n",
    "    Vectorize a list of sequences List[List[int]] using a fast.ai language model.\n",
    "\n",
    "    Paramters\n",
    "    ---------\n",
    "    encoder : sentence_encoder\n",
    "    list_list_int : List[List[int]]\n",
    "        A list of sequences to encode\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple: (avg, mean, last)\n",
    "        A tuple that returns the average-pooling, max-pooling over time steps as well as the last time step.\n",
    "    \"\"\"\n",
    "    n_rows = len(list_list_int)\n",
    "    n_dim = encoder.nhid\n",
    "    avgarr = np.empty((n_rows, n_dim))\n",
    "    maxarr = np.empty((n_rows, n_dim))\n",
    "    lastarr = np.empty((n_rows, n_dim))\n",
    "\n",
    "    for i in tqdm_notebook(range(len(list_list_int))):\n",
    "        avg_, max_, last_ = make_prediction_from_list(encoder, list_list_int[i])\n",
    "        avgarr[i,:] = avg_.data.cpu().numpy()\n",
    "        maxarr[i,:] = max_.data.cpu().numpy()\n",
    "        lastarr[i,:] = last_.data.cpu().numpy()\n",
    "\n",
    "    return avgarr, maxarr, lastarr\n",
    "\n",
    "def encode_sentences(sentences, encoding_dict):\n",
    "    sentences_enc = []\n",
    "    for sent in sentences:\n",
    "        sent_enc = []\n",
    "        for word in f'{BOS} ' +fixup(sent):\n",
    "            sent_enc.append(encoding_dict[word])\n",
    "        sentences_enc.append(sent_enc)\n",
    "        \n",
    "    return sentences_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_model = torch.load('siamese_model0.73.pt')\n",
    "siamese_model.encoder.nhid = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entailed_a_vec = get_embeddings(siamese_model.encoder, encode_sentences(entailed_a, stoi))\n",
    "entailed_b_vec = get_embeddings(siamese_model.encoder, encode_sentences(entailed_b, stoi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nmslib\n",
    "\n",
    "def create_nmslib_search_index(numpy_vectors):\n",
    "    \"\"\"Create search index using nmslib.\n",
    "    Parameters\n",
    "    ==========\n",
    "    numpy_vectors : numpy.array\n",
    "        The matrix of vectors\n",
    "    Returns\n",
    "    =======\n",
    "    nmslib object that has index of numpy_vectors\n",
    "    \"\"\"\n",
    "\n",
    "    search_index = nmslib.init(method='hnsw', space='cosinesimil')\n",
    "    search_index.addDataPointBatch(numpy_vectors)\n",
    "    search_index.createIndex({'post': 2}, print_progress=True)\n",
    "    return search_index\n",
    "\n",
    "def percent_matching(query_vec, searchindex, k=10):\n",
    "    num_found = 0\n",
    "    num_total = len(query_vec)\n",
    "    for i in range(num_total):\n",
    "        query = query_vec[i]\n",
    "        idxs, dists = searchindex.knnQuery(query, k=k)\n",
    "        if i in idxs:\n",
    "            num_found += 1\n",
    "\n",
    "    return 100 * num_found/num_total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entailed_b_max_searchindex = create_nmslib_search_index(entailed_b_vec[0])\n",
    "percent_matching(entailed_a_vec[0], entailed_b_max_searchindex, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entailed_b_max_searchindex = create_nmslib_search_index(entailed_b_vec[1])\n",
    "percent_matching(entailed_a_vec[1], entailed_b_max_searchindex, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entailed_b_last_searchindex = create_nmslib_search_index(entailed_b_vec[2])\n",
    "percent_matching(entailed_a_vec[2], entailed_b_last_searchindex, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entailed_a_concat = np.concatenate((entailed_a_vec[0], entailed_a_vec[1], entailed_a_vec[2]), axis=1)\n",
    "entailed_b_concat = np.concatenate((entailed_b_vec[0], entailed_b_vec[1], entailed_b_vec[2]), axis=1)\n",
    "entailed_b_concat_searchindex = create_nmslib_search_index(entailed_b_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_matching(entailed_a_concat, entailed_b_concat_searchindex, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_similar(query, query_idx, searchindex):\n",
    "    idx, _ = searchindex.knnQuery(query[query_idx], k=10)\n",
    "    matched = []\n",
    "    for i in idx:\n",
    "        matched.append(entailed_b[i])\n",
    "    \n",
    "    match = query_idx in idx\n",
    "    \n",
    "    return match, entailed_a[query_idx], entailed_b[query_idx], matched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    results = show_similar(entailed_a_vec[1], i, entailed_b_max_searchindex)\n",
    "    if results[0]:\n",
    "        print(results[1])\n",
    "        print(results[2])\n",
    "        for result in results[3]:\n",
    "            print(\"\\t\"+result)\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "_draft": {
   "nbviewer_url": "https://gist.github.com/0dd0df21cf404cf2bb51d0148c8b7d8b"
  },
  "gist": {
   "data": {
    "description": "fastai.text imdb example",
    "public": true
   },
   "id": "0dd0df21cf404cf2bb51d0148c8b7d8b"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "86px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
