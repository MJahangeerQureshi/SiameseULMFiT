{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ULMFiT + Siamese Network for Sentence Vectors\n",
    "## Part Two: Classifying\n",
    "\n",
    "The first notebook created a new language model from the SNLI dataset.\n",
    "This notebook will adapt that model to predicting the SNLI category for sentence pairs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text import *\n",
    "import html\n",
    "\n",
    "import json\n",
    "import html\n",
    "import re\n",
    "import pickle\n",
    "from collections import Counter\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import sklearn\n",
    "from sklearn import model_selection\n",
    "from functools import partial\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils \n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import dataset, dataloader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import time\n",
    "import math\n",
    "import sys\n",
    "import data\n",
    "\n",
    "snli_root = './data/SNLI/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8842"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load the tokens\n",
    "itos = pickle.load(open(f'{snli_root}itos.pkl', 'rb'))\n",
    "trn_lm = np.load(f'{snli_root}trn_lm.npy')\n",
    "val_lm = np.load(f'{snli_root}val_lm.npy')\n",
    "\n",
    "stoi = defaultdict(lambda:0, {v:k for k,v in enumerate(itos)})\n",
    "vocab_size = len(itos)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a new dataloader to create sentence pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "class Entail(Enum):\n",
    "    entailment = 0\n",
    "    contradiction = 1\n",
    "    neutral = 2\n",
    "       \n",
    "class SiameseDataset(dataset.Dataset):\n",
    "    def __init__(self, json_file):\n",
    "        \n",
    "        content = None\n",
    "        with open(json_file) as fp:\n",
    "            content = json.load(fp)\n",
    "\n",
    "        self.items = []\n",
    "        for item in content:\n",
    "            s0 = item[0]\n",
    "            s1 = item[1]\n",
    "            label = Entail[item[2]].value\n",
    "            self.items.append((s0, s1, label))\n",
    "            \n",
    "    def shuffle(self):\n",
    "        random.shuffle(self.items)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.items[index]\n",
    "       \n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "    \n",
    "class SiameseDataLoader():\n",
    "    def __init__(self, dataset, stoi, pad_val, batch_size=32):\n",
    "        self.dataset = dataset\n",
    "        dataset.shuffle()\n",
    "        self.batch_size = batch_size\n",
    "        self.stoi = stoi\n",
    "        self.index = 0\n",
    "        self.pad_val = pad_val\n",
    "      \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def fill_tensor(self, sentences, max_len):\n",
    "        data = np.zeros((max_len, self.batch_size), dtype=np.long)\n",
    "        data.fill(self.pad_val)\n",
    "        \n",
    "        for i, s in enumerate(sentences): \n",
    "            start_idx = max_len - len(s)\n",
    "            for j, p in enumerate(s):\n",
    "                data[:,i][start_idx+j] = stoi[p]\n",
    "            \n",
    "        return torch.LongTensor([data.tolist()]).cuda()\n",
    "     \n",
    "    def batch(self):\n",
    "        return self.index//self.batch_size\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)//self.batch_size\n",
    "    \n",
    "    def __next__(self):\n",
    "        #how many examples to ananlyise for this round\n",
    "        num = min(self.batch_size, len(self.dataset) - self.index)\n",
    "        \n",
    "        if num < 1:\n",
    "            raise StopIteration  # signals \"the end\"\n",
    "            \n",
    "        #collect the sentences\n",
    "        max_len = 0\n",
    "        first = []\n",
    "        second = []\n",
    "        labels = np.zeros((self.batch_size), dtype=np.long)\n",
    "        \n",
    "        for i in range(self.index, self.index+num):\n",
    "            a, b, l = self.dataset[i]\n",
    "            \n",
    "            if len(a) > max_len:\n",
    "                max_len = len(a)\n",
    "            \n",
    "            if len(b) > max_len:\n",
    "                max_len = len(b)\n",
    "            \n",
    "            first.append(a)\n",
    "            second.append(b)\n",
    "            labels[i - self.index] = l\n",
    "            \n",
    "        self.index += num\n",
    "             \n",
    "        return (self.fill_tensor(first, max_len),\n",
    "                self.fill_tensor(second, max_len),\n",
    "                torch.LongTensor([labels.tolist()]).cuda()\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_dataset_dev = SiameseDataset(f'{snli_root}/snli_dev.json')\n",
    "siamese_dataset_test = SiameseDataset(f'{snli_root}snli_test.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Siamese network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, encoder, classifier):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.classifier = classifier\n",
    "    \n",
    "    def pool(self, x, bs, is_max):\n",
    "        f = F.adaptive_max_pool1d if is_max else F.adaptive_avg_pool1d\n",
    "        return f(x.permute(1,2,0), (1,)).view(bs,-1)\n",
    "\n",
    "    def pool_outputs(self, output):\n",
    "        sl, bs,_ = output.size()\n",
    "        avgpool = self.pool(output, bs, False)\n",
    "        maxpool = self.pool(output, bs, True)\n",
    "        return torch.cat([output[-1], maxpool, avgpool], 1)\n",
    "        \n",
    "    def forward(self, input1, input2):\n",
    "\n",
    "        raw_outputs1, outputs1 = self.encoder(input1)\n",
    "        raw_outputs2, outputs2 = self.encoder(input2)\n",
    "        \n",
    "        out1 = self.pool_outputs(outputs1[-1])\n",
    "        out2 = self.pool_outputs(outputs2[-1])\n",
    "        \n",
    "        out = torch.cat([out1, out2], 1)\n",
    "        \n",
    "        return self.classifier(out)\n",
    "        \n",
    "    def reset(self):\n",
    "        for c in self.children():\n",
    "            if hasattr(c, 'reset'): c.reset()\n",
    "                \n",
    "    def freeze_encoder(self):\n",
    "        for param in self.encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "    def unfreeze_encoder(self):\n",
    "        for param in self.encoder.parameters():\n",
    "            param.requires_grad = True\n",
    "                \n",
    "class LinearClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden, num_categories, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layers = nn.ModuleList([\n",
    "            LinearBlock(input_size, hidden, dropout),\n",
    "            LinearBlock(hidden, hidden, dropout),\n",
    "            LinearBlock(hidden, num_categories, dropout)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for l in self.layers:\n",
    "            x = F.relu(l(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load our pretrained model then build the Siamese network from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#these are the values used for the original LM\n",
    "em_sz, nh = 400, 1150\n",
    "\n",
    "SNLI_encoder = torch.load(\"SNLI_Encoder.pt\")\n",
    "\n",
    "#2 pooled vectors, of 3 times the embedding size\n",
    "classifier = LinearClassifier(3*2*em_sz, nh,  3, dropout=0.05)\n",
    "siamese_model = SiameseClassifier(SNLI_encoder, classifier).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the new network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_interval = 50\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "def evaluate(model, data_loader):\n",
    "    # Turn on evaluation mode which disables dropout.\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    num_correct = 0\n",
    "    total = 0 \n",
    "    for a, b, l in data_loader:\n",
    "\n",
    "        a, b, l = Variable(a), Variable(b), Variable(l)\n",
    "        a.requires_grad = False\n",
    "        b.requires_grad = False\n",
    "        l.requires_grad = False\n",
    "        out = model(a.squeeze(), b.squeeze())\n",
    "        num_correct += np.sum(l.data.cpu().numpy() == np.argmax(out.data.cpu().numpy(), 1))\n",
    "        total += out.shape[0]\n",
    "        loss = criterion(out, l.squeeze())\n",
    "        total_loss += out.shape[0] * loss.data.cpu()[0]\n",
    "\n",
    "    return (total_loss / total, num_correct / total)\n",
    "\n",
    "def train(model, data_loader, optimizer):\n",
    "    # Turn on training mode which enables dropout.\n",
    "    model.train()\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    \n",
    "    num_correct = 0\n",
    "    total = 0 \n",
    "        \n",
    "    for a, b, l in data_loader:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        a, b, l = Variable(a), Variable(b), Variable(l)\n",
    "\n",
    "        out = model(a.squeeze(), b.squeeze())\n",
    "        loss = criterion(out, l.squeeze())\n",
    "        total_loss += out.shape[0] * loss.data.cpu()[0]\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        num_correct += np.sum(l.data.cpu().numpy() == np.argmax(out.data.cpu().numpy(), 1))\n",
    "        total += out.shape[0]\n",
    "\n",
    "        batch = data_loader.batch()\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / total\n",
    "            elapsed = time.time() - start_time\n",
    "            batches = len(data_loader)\n",
    "            ms = elapsed * 1000 / log_interval\n",
    "            print(f'| epoch {epoch:3d} | {batch:5d}/{batches:5d} batches', end=\" \")\n",
    "            print(f'| ms/batch {ms:5.2f} | loss {cur_loss:5.4f} acc {num_correct / total}')\n",
    "            total_loss = 0\n",
    "            total = 0\n",
    "            num_correct = 0\n",
    "            start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training with lr 0.001\n",
      "| epoch   0 |    50/  307 batches | ms/batch 270.44 | loss 1.1969 acc 0.343125\n",
      "| epoch   0 |   100/  307 batches | ms/batch 257.95 | loss 1.1029 acc 0.35\n",
      "| epoch   0 |   150/  307 batches | ms/batch 231.96 | loss 1.0986 acc 0.34125\n",
      "| epoch   0 |   200/  307 batches | ms/batch 251.36 | loss 1.0987 acc 0.34375\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-62d3a12e79c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mepoch_start_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msiamese_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSiameseDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msiamese_dataset_test\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mstoi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstoi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_pad_\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-52-24f34b0ddfb6>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, data_loader, optimizer)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/fastai/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    165\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \"\"\"\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/fastai/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 99\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lrs = [0.001, 0.0005, 0.0001, 0.0001, 0.0001, 0.00005, 0.00001]\n",
    "\n",
    "for epoch, lr in enumerate(lrs):\n",
    "\n",
    "    print(f'training with lr {lr}')\n",
    "    optimizer = optim.Adam(siamese_model.parameters(), lr=lr)\n",
    "\n",
    "    training_data = SiameseDataLoader(siamese_dataset_dev, stoi, stoi[\"_pad_\"], batch_size=32)\n",
    "\n",
    "    epoch_start_time = time.time()\n",
    "    train(siamese_model, training_data, optimizer)\n",
    "\n",
    "    validation_data = SiameseDataLoader(siamese_dataset_test , stoi, stoi[\"_pad_\"], batch_size=32)\n",
    "    val_loss, accuracy = evaluate(siamese_model, validation_data)\n",
    "\n",
    "    delta_t = (time.time() - epoch_start_time)\n",
    "    print('-' * 89)\n",
    "    print(f'| end of epoch {epoch:3d} | time: {delta_t:5.2f}s | valid loss {val_loss:5.2f} accuracy {accuracy} learning rate {lr}')\n",
    "    print('-' * 89)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{data_root}siamese_model.pt', 'wb') as f:\n",
    "    torch.save(siamese_model, f)"
   ]
  }
 ],
 "metadata": {
  "_draft": {
   "nbviewer_url": "https://gist.github.com/0dd0df21cf404cf2bb51d0148c8b7d8b"
  },
  "gist": {
   "data": {
    "description": "fastai.text imdb example",
    "public": true
   },
   "id": "0dd0df21cf404cf2bb51d0148c8b7d8b"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "86px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
