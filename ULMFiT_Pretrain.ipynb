{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ULMFiT + Siamese Network for Sentence Vectors\n",
    "## Part One: Pretraining\n",
    "This notebook will take a language model from lesson 10 of the Fast ai course on deeplearning and add a siamese network to create sentence vectors. We will be using the SNLI dataset. The first task will be to make a network that predicts entailment. Then we will create sentence vectors and determine suitability for use as a similarity metric.\n",
    "\n",
    "### You must have the fastai library installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fastai.text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-bc05dcc5e241>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mfastai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhtml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhtml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'fastai.text'"
     ]
    }
   ],
   "source": [
    "from fastai.text import *\n",
    "import html\n",
    "\n",
    "import json\n",
    "import html\n",
    "import re\n",
    "import pickle\n",
    "from collections import Counter\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import sklearn\n",
    "from sklearn import model_selection\n",
    "from functools import partial\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils \n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import dataset, dataloader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import time\n",
    "import math\n",
    "import sys\n",
    "import data\n",
    "\n",
    "data_root = './data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the tokens from the SNLI data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_trn = np.load(f'{data_root}tok_trn.npy')\n",
    "tok_val = np.load(f'{data_root}tok_val.npy')\n",
    "freq = Counter(np.concatenate([tok_trn, tok_val]))\n",
    "\n",
    "max_vocab = 60000\n",
    "min_freq = 2\n",
    "itos = [o for o, c in freq.most_common(max_vocab) if c>min_freq]\n",
    "itos.insert(0, '_pad_')\n",
    "itos.insert(0, '_unk_')\n",
    "stoi = defaultdict(lambda:0, {v:k for k,v in enumerate(itos)})\n",
    "\n",
    "trn_lm = np.array([stoi[p] for p in tok_trn])\n",
    "val_lm = np.array([stoi[p] for p in tok_val])\n",
    "\n",
    "#save results\n",
    "pickle.dump(itos, open(f'{data_root}itos.pkl', 'wb'))\n",
    "np.save(f'{data_root}trn_lm.npy', trn_lm)\n",
    "np.save(f'{data_root}val_lm.npy', val_lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21434"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load the results so we can pick it up from here \n",
    "itos = pickle.load(open(f'{data_root}itos.pkl', 'rb'))\n",
    "trn_lm = np.load(f'{data_root}trn_lm.npy')\n",
    "val_lm = np.load(f'{data_root}val_lm.npy')\n",
    "\n",
    "stoi = defaultdict(lambda:0, {v:k for k,v in enumerate(itos)})\n",
    "vocab_size = len(itos)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " xsos a man on a phone and a woman are eating candy . \n",
      " xsos a bicycler rides his bike on the road next to rocks with snow . \n",
      " xsos the bench is outdoors \n",
      " xsos a woman is with her kid \n",
      " xsos two little boys play football on green grass . \n",
      " xsos four elderly people sitting under a white tent playing musical instruments . \n",
      " xsos there are people sking . \n",
      " xsos some people following a ball . \n",
      " xsos two dogs play together . \n",
      " xsos a very young girl is holding food "
     ]
    }
   ],
   "source": [
    "# check to make sure that the data looks ok\n",
    "for word in trn_lm[:100]:\n",
    "    print(itos[word], end=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Wikitext LM and fix the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download the wikitext LM\n",
    "# ! wget -nH -r -np -P ./data/aclImdb/ http://files.fast.ai/models/wt103/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#these are the values used for the original LM\n",
    "em_sz,nh,nl = 400,1150,3\n",
    "\n",
    "PRE_PATH = f'{data_root}aclImdb/models/wt103'\n",
    "PRE_LM_PATH = PRE_PATH+'/fwd_wt103.h5'\n",
    "wgts = torch.load(PRE_LM_PATH, map_location=lambda storage, loc: storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the mean weight value for any new vocab\n",
    "enc_wgts = to_np(wgts['0.encoder.weight'])\n",
    "row_m = enc_wgts.mean(0)\n",
    "\n",
    "itos2 = pickle.load(Path(PRE_PATH+'/itos_wt103.pkl').open('rb'))\n",
    "stoi2 = defaultdict(lambda:-1, {v:k for k,v in enumerate(itos2)})\n",
    "\n",
    "#fill in the missing values from the old vocab\n",
    "new_w = np.zeros((vocab_size, em_sz), dtype=np.float32)\n",
    "for i,w in enumerate(itos):\n",
    "    r = stoi2[w]\n",
    "    new_w[i] = enc_wgts[r] if r>=0 else row_m\n",
    "    \n",
    "#fix up the wgts with the new values\n",
    "wgts['0.encoder.weight'] = T(new_w)\n",
    "wgts['0.encoder_with_dropout.embed.weight'] = T(np.copy(new_w))\n",
    "wgts['1.decoder.weight'] = T(np.copy(new_w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LanguageModelLoader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-2e7060e345c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m52\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrn_dl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLanguageModelLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrn_lm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbptt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mval_dl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLanguageModelLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_lm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbptt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLanguageModelData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrn_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbptt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbptt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LanguageModelLoader' is not defined"
     ]
    }
   ],
   "source": [
    "max_seq = 20*70\n",
    "wd = 1e-7\n",
    "bptt = 70\n",
    "batch_size = 52\n",
    "\n",
    "trn_dl = LanguageModelLoader(np.concatenate(trn_lm), batch_size, bptt)\n",
    "val_dl = LanguageModelLoader(np.concatenate(val_lm), batch_size, bptt)\n",
    "md = LanguageModelData(PATH, 1, vs, trn_dl, val_dl, bs=batch_size, bptt=bptt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the language model and load the weigths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "drops = np.array([0.25, 0.1, 0.2, 0.02, 0.15])*0.7\n",
    "\n",
    "dropouti, dropout, wdrop, dropoute, dropouth = drops[0], drops[1], drops[2], drops[3], drops[4]\n",
    "rnn_enc = RNN_Encoder(vocab_size, em_sz, n_hid=nh, n_layers=nl, pad_token=stoi['_pad_'],\n",
    "                 dropouth=dropouth, dropouti=dropouti, dropoute=dropoute, wdrop=wdrop, qrnn=False)\n",
    "enc = rnn_enc.encoder\n",
    "language_model = SequentialRNN(rnn_enc, LinearDecoder(vocab_size, em_sz, dropout, tie_encoder=enc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_model.load_state_dict(wgts)\n",
    "language_model = language_model.cuda()\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_interval = 50\n",
    "\n",
    "def evaluate(model, data_source, ntokens, batch_size, bptt):\n",
    "    # Turn on evaluation mode which disables dropout.\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    #with torch.no_grad():\n",
    "    for i in range(0, data_source.size(0) - 1, bptt):\n",
    "        data, targets = get_batch(data_source, i, bptt)\n",
    "        data = Variable(data)\n",
    "        data.requires_grad = False\n",
    "        \n",
    "        targets = Variable(targets)\n",
    "        targets.requires_grad = False\n",
    "        result, raw_outputs, outputs = model(data)\n",
    "        loss = criterion(result, targets)\n",
    "\n",
    "        total_loss += data.shape[0] * loss.data.cpu()[0]\n",
    "    \n",
    "    return total_loss / len(data_source)\n",
    "\n",
    "def train(model, data_source, ntokens, batch_size, bptt, optimizer):\n",
    "    # Turn on training mode which enables dropout.\n",
    "    model.train()\n",
    "    total_loss = 0.\n",
    "    total_items = 0.\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for batch, i in enumerate(range(0, data_source.size(0) - 1, bptt)):\n",
    "        data, targets = get_batch(data_source, i, bptt)\n",
    "        \n",
    "        # Starting each batch, we detach the hidden state from how it was previously produced.\n",
    "        # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        result, raw_outputs, outputs = model(Variable(data))\n",
    "        \n",
    "        loss = criterion(result, Variable(targets))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_items += data.shape[0]\n",
    "        total_loss += data.shape[0] * loss.data.cpu()[0]\n",
    "\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / total_items\n",
    "            elapsed = time.time() - start_time\n",
    "            batches = len(data_source) // bptt\n",
    "            ms = elapsed * 1000 / log_interval\n",
    "            print(f'| epoch {epoch:3d} | {batch:5d}/{batches:5d} batches', end=\" \")\n",
    "            #lr {scheduler.get_lr()[0]:02.5f}\n",
    "            print(f'| ms/batch {ms:5.2f} | loss {cur_loss:5.4f} | ppl {math.exp(cur_loss):8.2f}')\n",
    "            total_loss = 0\n",
    "            total_items = 0\n",
    "            start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |    50/ 3662 batches | ms/batch 119.78 | loss 2.7412 | ppl    15.51\n",
      "| epoch   1 |   100/ 3662 batches | ms/batch 117.33 | loss 2.7254 | ppl    15.26\n",
      "| epoch   1 |   150/ 3662 batches | ms/batch 117.52 | loss 2.7423 | ppl    15.52\n",
      "| epoch   1 |   200/ 3662 batches | ms/batch 117.62 | loss 2.7327 | ppl    15.37\n",
      "| epoch   1 |   250/ 3662 batches | ms/batch 117.54 | loss 2.7352 | ppl    15.41\n",
      "| epoch   1 |   300/ 3662 batches | ms/batch 117.33 | loss 2.7364 | ppl    15.43\n",
      "| epoch   1 |   350/ 3662 batches | ms/batch 117.55 | loss 2.7434 | ppl    15.54\n",
      "| epoch   1 |   400/ 3662 batches | ms/batch 117.41 | loss 2.7359 | ppl    15.42\n",
      "| epoch   1 |   450/ 3662 batches | ms/batch 117.77 | loss 2.7372 | ppl    15.44\n",
      "| epoch   1 |   500/ 3662 batches | ms/batch 117.41 | loss 2.7425 | ppl    15.53\n",
      "| epoch   1 |   550/ 3662 batches | ms/batch 117.69 | loss 2.7303 | ppl    15.34\n",
      "| epoch   1 |   600/ 3662 batches | ms/batch 117.51 | loss 2.7462 | ppl    15.58\n",
      "| epoch   1 |   650/ 3662 batches | ms/batch 117.54 | loss 2.7446 | ppl    15.56\n",
      "| epoch   1 |   700/ 3662 batches | ms/batch 117.50 | loss 2.7507 | ppl    15.65\n",
      "| epoch   1 |   750/ 3662 batches | ms/batch 117.43 | loss 2.7410 | ppl    15.50\n",
      "| epoch   1 |   800/ 3662 batches | ms/batch 117.65 | loss 2.7534 | ppl    15.70\n",
      "| epoch   1 |   850/ 3662 batches | ms/batch 117.56 | loss 2.7450 | ppl    15.56\n",
      "| epoch   1 |   900/ 3662 batches | ms/batch 117.61 | loss 2.7386 | ppl    15.47\n",
      "| epoch   1 |   950/ 3662 batches | ms/batch 117.71 | loss 2.7486 | ppl    15.62\n",
      "| epoch   1 |  1000/ 3662 batches | ms/batch 117.76 | loss 2.7297 | ppl    15.33\n",
      "| epoch   1 |  1050/ 3662 batches | ms/batch 117.69 | loss 2.7272 | ppl    15.29\n",
      "| epoch   1 |  1100/ 3662 batches | ms/batch 117.95 | loss 2.7425 | ppl    15.53\n",
      "| epoch   1 |  1150/ 3662 batches | ms/batch 117.66 | loss 2.7307 | ppl    15.34\n",
      "| epoch   1 |  1200/ 3662 batches | ms/batch 117.68 | loss 2.7428 | ppl    15.53\n",
      "| epoch   1 |  1250/ 3662 batches | ms/batch 117.74 | loss 2.7254 | ppl    15.26\n",
      "| epoch   1 |  1300/ 3662 batches | ms/batch 117.78 | loss 2.7289 | ppl    15.32\n",
      "| epoch   1 |  1350/ 3662 batches | ms/batch 117.95 | loss 2.7339 | ppl    15.39\n",
      "| epoch   1 |  1400/ 3662 batches | ms/batch 117.72 | loss 2.7244 | ppl    15.25\n",
      "| epoch   1 |  1450/ 3662 batches | ms/batch 117.89 | loss 2.7296 | ppl    15.33\n",
      "| epoch   1 |  1500/ 3662 batches | ms/batch 117.98 | loss 2.7161 | ppl    15.12\n",
      "| epoch   1 |  1550/ 3662 batches | ms/batch 117.90 | loss 2.7319 | ppl    15.36\n",
      "| epoch   1 |  1600/ 3662 batches | ms/batch 117.53 | loss 2.7179 | ppl    15.15\n",
      "| epoch   1 |  1650/ 3662 batches | ms/batch 117.73 | loss 2.7488 | ppl    15.62\n",
      "| epoch   1 |  1700/ 3662 batches | ms/batch 117.96 | loss 2.7372 | ppl    15.44\n",
      "| epoch   1 |  1750/ 3662 batches | ms/batch 117.82 | loss 2.7290 | ppl    15.32\n",
      "| epoch   1 |  1800/ 3662 batches | ms/batch 117.63 | loss 2.7306 | ppl    15.34\n",
      "| epoch   1 |  1850/ 3662 batches | ms/batch 117.92 | loss 2.7213 | ppl    15.20\n",
      "| epoch   1 |  1900/ 3662 batches | ms/batch 117.81 | loss 2.7292 | ppl    15.32\n",
      "| epoch   1 |  1950/ 3662 batches | ms/batch 117.72 | loss 2.7358 | ppl    15.42\n",
      "| epoch   1 |  2000/ 3662 batches | ms/batch 117.85 | loss 2.7189 | ppl    15.16\n",
      "| epoch   1 |  2050/ 3662 batches | ms/batch 118.01 | loss 2.7187 | ppl    15.16\n",
      "| epoch   1 |  2100/ 3662 batches | ms/batch 117.82 | loss 2.7265 | ppl    15.28\n",
      "| epoch   1 |  2150/ 3662 batches | ms/batch 117.85 | loss 2.7057 | ppl    14.96\n",
      "| epoch   1 |  2200/ 3662 batches | ms/batch 117.75 | loss 2.7111 | ppl    15.05\n",
      "| epoch   1 |  2250/ 3662 batches | ms/batch 117.91 | loss 2.7137 | ppl    15.09\n",
      "| epoch   1 |  2300/ 3662 batches | ms/batch 117.94 | loss 2.7426 | ppl    15.53\n",
      "| epoch   1 |  2350/ 3662 batches | ms/batch 117.71 | loss 2.7185 | ppl    15.16\n",
      "| epoch   1 |  2400/ 3662 batches | ms/batch 117.66 | loss 2.6953 | ppl    14.81\n",
      "| epoch   1 |  2450/ 3662 batches | ms/batch 117.97 | loss 2.7057 | ppl    14.97\n",
      "| epoch   1 |  2500/ 3662 batches | ms/batch 117.69 | loss 2.7169 | ppl    15.13\n",
      "| epoch   1 |  2550/ 3662 batches | ms/batch 117.58 | loss 2.7251 | ppl    15.26\n",
      "| epoch   1 |  2600/ 3662 batches | ms/batch 117.96 | loss 2.7065 | ppl    14.98\n",
      "| epoch   1 |  2650/ 3662 batches | ms/batch 117.80 | loss 2.7197 | ppl    15.18\n",
      "| epoch   1 |  2700/ 3662 batches | ms/batch 118.01 | loss 2.6976 | ppl    14.84\n",
      "| epoch   1 |  2750/ 3662 batches | ms/batch 118.04 | loss 2.7242 | ppl    15.24\n",
      "| epoch   1 |  2800/ 3662 batches | ms/batch 118.01 | loss 2.7123 | ppl    15.06\n",
      "| epoch   1 |  2850/ 3662 batches | ms/batch 117.87 | loss 2.7033 | ppl    14.93\n",
      "| epoch   1 |  2900/ 3662 batches | ms/batch 117.76 | loss 2.7108 | ppl    15.04\n",
      "| epoch   1 |  2950/ 3662 batches | ms/batch 118.25 | loss 2.6985 | ppl    14.86\n",
      "| epoch   1 |  3000/ 3662 batches | ms/batch 117.83 | loss 2.7011 | ppl    14.90\n",
      "| epoch   1 |  3050/ 3662 batches | ms/batch 117.87 | loss 2.6965 | ppl    14.83\n",
      "| epoch   1 |  3100/ 3662 batches | ms/batch 117.77 | loss 2.7137 | ppl    15.08\n",
      "| epoch   1 |  3150/ 3662 batches | ms/batch 118.03 | loss 2.6973 | ppl    14.84\n",
      "| epoch   1 |  3200/ 3662 batches | ms/batch 118.05 | loss 2.7003 | ppl    14.88\n",
      "| epoch   1 |  3250/ 3662 batches | ms/batch 117.94 | loss 2.6954 | ppl    14.81\n",
      "| epoch   1 |  3300/ 3662 batches | ms/batch 117.67 | loss 2.6940 | ppl    14.79\n",
      "| epoch   1 |  3350/ 3662 batches | ms/batch 117.77 | loss 2.6914 | ppl    14.75\n",
      "| epoch   1 |  3400/ 3662 batches | ms/batch 117.99 | loss 2.7047 | ppl    14.95\n",
      "| epoch   1 |  3450/ 3662 batches | ms/batch 117.81 | loss 2.7073 | ppl    14.99\n",
      "| epoch   1 |  3500/ 3662 batches | ms/batch 118.11 | loss 2.6872 | ppl    14.69\n",
      "| epoch   1 |  3550/ 3662 batches | ms/batch 118.19 | loss 2.6864 | ppl    14.68\n",
      "| epoch   1 |  3600/ 3662 batches | ms/batch 117.88 | loss 2.6966 | ppl    14.83\n",
      "| epoch   1 |  3650/ 3662 batches | ms/batch 117.77 | loss 2.6927 | ppl    14.77\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 448.24s | valid loss  2.57 | valid ppl    13.13\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |    50/ 3662 batches | ms/batch 120.33 | loss 2.6831 | ppl    14.63\n",
      "| epoch   2 |   100/ 3662 batches | ms/batch 117.99 | loss 2.6678 | ppl    14.41\n",
      "| epoch   2 |   150/ 3662 batches | ms/batch 117.95 | loss 2.6783 | ppl    14.56\n",
      "| epoch   2 |   200/ 3662 batches | ms/batch 117.80 | loss 2.6674 | ppl    14.40\n",
      "| epoch   2 |   250/ 3662 batches | ms/batch 117.95 | loss 2.6750 | ppl    14.51\n",
      "| epoch   2 |   300/ 3662 batches | ms/batch 117.63 | loss 2.6750 | ppl    14.51\n",
      "| epoch   2 |   350/ 3662 batches | ms/batch 117.70 | loss 2.6812 | ppl    14.60\n",
      "| epoch   2 |   400/ 3662 batches | ms/batch 118.12 | loss 2.6811 | ppl    14.60\n",
      "| epoch   2 |   450/ 3662 batches | ms/batch 117.86 | loss 2.6742 | ppl    14.50\n",
      "| epoch   2 |   500/ 3662 batches | ms/batch 117.76 | loss 2.6799 | ppl    14.58\n",
      "| epoch   2 |   550/ 3662 batches | ms/batch 117.67 | loss 2.6678 | ppl    14.41\n",
      "| epoch   2 |   600/ 3662 batches | ms/batch 117.84 | loss 2.6781 | ppl    14.56\n",
      "| epoch   2 |   650/ 3662 batches | ms/batch 117.66 | loss 2.6753 | ppl    14.52\n",
      "| epoch   2 |   700/ 3662 batches | ms/batch 118.25 | loss 2.6851 | ppl    14.66\n",
      "| epoch   2 |   750/ 3662 batches | ms/batch 117.80 | loss 2.6716 | ppl    14.46\n",
      "| epoch   2 |   800/ 3662 batches | ms/batch 117.71 | loss 2.6831 | ppl    14.63\n",
      "| epoch   2 |   850/ 3662 batches | ms/batch 117.69 | loss 2.6741 | ppl    14.50\n",
      "| epoch   2 |   900/ 3662 batches | ms/batch 117.85 | loss 2.6757 | ppl    14.52\n",
      "| epoch   2 |   950/ 3662 batches | ms/batch 117.84 | loss 2.6803 | ppl    14.59\n",
      "| epoch   2 |  1000/ 3662 batches | ms/batch 117.81 | loss 2.6627 | ppl    14.33\n",
      "| epoch   2 |  1050/ 3662 batches | ms/batch 117.92 | loss 2.6611 | ppl    14.31\n",
      "| epoch   2 |  1100/ 3662 batches | ms/batch 117.67 | loss 2.6748 | ppl    14.51\n",
      "| epoch   2 |  1150/ 3662 batches | ms/batch 117.85 | loss 2.6686 | ppl    14.42\n",
      "| epoch   2 |  1200/ 3662 batches | ms/batch 117.81 | loss 2.6730 | ppl    14.48\n",
      "| epoch   2 |  1250/ 3662 batches | ms/batch 117.86 | loss 2.6623 | ppl    14.33\n",
      "| epoch   2 |  1300/ 3662 batches | ms/batch 117.74 | loss 2.6614 | ppl    14.32\n",
      "| epoch   2 |  1350/ 3662 batches | ms/batch 117.58 | loss 2.6670 | ppl    14.40\n",
      "| epoch   2 |  1400/ 3662 batches | ms/batch 117.90 | loss 2.6648 | ppl    14.37\n",
      "| epoch   2 |  1450/ 3662 batches | ms/batch 117.89 | loss 2.6729 | ppl    14.48\n",
      "| epoch   2 |  1500/ 3662 batches | ms/batch 118.00 | loss 2.6557 | ppl    14.23\n",
      "| epoch   2 |  1550/ 3662 batches | ms/batch 118.02 | loss 2.6705 | ppl    14.45\n",
      "| epoch   2 |  1600/ 3662 batches | ms/batch 117.98 | loss 2.6578 | ppl    14.27\n",
      "| epoch   2 |  1650/ 3662 batches | ms/batch 117.89 | loss 2.6873 | ppl    14.69\n",
      "| epoch   2 |  1700/ 3662 batches | ms/batch 117.79 | loss 2.6795 | ppl    14.58\n",
      "| epoch   2 |  1750/ 3662 batches | ms/batch 117.94 | loss 2.6614 | ppl    14.32\n",
      "| epoch   2 |  1800/ 3662 batches | ms/batch 117.86 | loss 2.6740 | ppl    14.50\n",
      "| epoch   2 |  1850/ 3662 batches | ms/batch 117.70 | loss 2.6570 | ppl    14.25\n",
      "| epoch   2 |  1900/ 3662 batches | ms/batch 117.96 | loss 2.6701 | ppl    14.44\n",
      "| epoch   2 |  1950/ 3662 batches | ms/batch 117.77 | loss 2.6697 | ppl    14.44\n",
      "| epoch   2 |  2000/ 3662 batches | ms/batch 117.94 | loss 2.6620 | ppl    14.33\n",
      "| epoch   2 |  2050/ 3662 batches | ms/batch 117.79 | loss 2.6558 | ppl    14.24\n",
      "| epoch   2 |  2100/ 3662 batches | ms/batch 118.03 | loss 2.6666 | ppl    14.39\n",
      "| epoch   2 |  2150/ 3662 batches | ms/batch 118.00 | loss 2.6438 | ppl    14.07\n",
      "| epoch   2 |  2200/ 3662 batches | ms/batch 117.82 | loss 2.6592 | ppl    14.28\n",
      "| epoch   2 |  2250/ 3662 batches | ms/batch 118.04 | loss 2.6551 | ppl    14.23\n",
      "| epoch   2 |  2300/ 3662 batches | ms/batch 117.92 | loss 2.6798 | ppl    14.58\n",
      "| epoch   2 |  2350/ 3662 batches | ms/batch 118.01 | loss 2.6552 | ppl    14.23\n",
      "| epoch   2 |  2400/ 3662 batches | ms/batch 117.64 | loss 2.6297 | ppl    13.87\n",
      "| epoch   2 |  2450/ 3662 batches | ms/batch 117.82 | loss 2.6518 | ppl    14.18\n",
      "| epoch   2 |  2500/ 3662 batches | ms/batch 117.79 | loss 2.6576 | ppl    14.26\n",
      "| epoch   2 |  2550/ 3662 batches | ms/batch 118.18 | loss 2.6664 | ppl    14.39\n",
      "| epoch   2 |  2600/ 3662 batches | ms/batch 117.85 | loss 2.6515 | ppl    14.17\n",
      "| epoch   2 |  2650/ 3662 batches | ms/batch 117.93 | loss 2.6623 | ppl    14.33\n",
      "| epoch   2 |  2700/ 3662 batches | ms/batch 117.84 | loss 2.6383 | ppl    13.99\n",
      "| epoch   2 |  2750/ 3662 batches | ms/batch 117.89 | loss 2.6634 | ppl    14.34\n",
      "| epoch   2 |  2800/ 3662 batches | ms/batch 117.73 | loss 2.6543 | ppl    14.22\n",
      "| epoch   2 |  2850/ 3662 batches | ms/batch 117.65 | loss 2.6437 | ppl    14.06\n",
      "| epoch   2 |  2900/ 3662 batches | ms/batch 117.74 | loss 2.6524 | ppl    14.19\n",
      "| epoch   2 |  2950/ 3662 batches | ms/batch 118.09 | loss 2.6449 | ppl    14.08\n",
      "| epoch   2 |  3000/ 3662 batches | ms/batch 118.11 | loss 2.6486 | ppl    14.13\n",
      "| epoch   2 |  3050/ 3662 batches | ms/batch 117.85 | loss 2.6431 | ppl    14.06\n",
      "| epoch   2 |  3100/ 3662 batches | ms/batch 117.87 | loss 2.6507 | ppl    14.16\n",
      "| epoch   2 |  3150/ 3662 batches | ms/batch 117.77 | loss 2.6404 | ppl    14.02\n",
      "| epoch   2 |  3200/ 3662 batches | ms/batch 118.66 | loss 2.6436 | ppl    14.06\n",
      "| epoch   2 |  3250/ 3662 batches | ms/batch 117.69 | loss 2.6382 | ppl    13.99\n",
      "| epoch   2 |  3300/ 3662 batches | ms/batch 117.77 | loss 2.6347 | ppl    13.94\n",
      "| epoch   2 |  3350/ 3662 batches | ms/batch 117.88 | loss 2.6418 | ppl    14.04\n",
      "| epoch   2 |  3400/ 3662 batches | ms/batch 117.47 | loss 2.6481 | ppl    14.13\n",
      "| epoch   2 |  3450/ 3662 batches | ms/batch 117.94 | loss 2.6489 | ppl    14.14\n",
      "| epoch   2 |  3500/ 3662 batches | ms/batch 117.71 | loss 2.6357 | ppl    13.95\n",
      "| epoch   2 |  3550/ 3662 batches | ms/batch 117.67 | loss 2.6385 | ppl    13.99\n",
      "| epoch   2 |  3600/ 3662 batches | ms/batch 117.82 | loss 2.6380 | ppl    13.99\n",
      "| epoch   2 |  3650/ 3662 batches | ms/batch 117.83 | loss 2.6459 | ppl    14.10\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 448.63s | valid loss  2.52 | valid ppl    12.45\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |    50/ 3662 batches | ms/batch 120.14 | loss 2.6332 | ppl    13.92\n",
      "| epoch   3 |   100/ 3662 batches | ms/batch 117.68 | loss 2.6227 | ppl    13.77\n",
      "| epoch   3 |   150/ 3662 batches | ms/batch 117.68 | loss 2.6268 | ppl    13.83\n",
      "| epoch   3 |   200/ 3662 batches | ms/batch 117.81 | loss 2.6216 | ppl    13.76\n",
      "| epoch   3 |   250/ 3662 batches | ms/batch 117.55 | loss 2.6295 | ppl    13.87\n",
      "| epoch   3 |   300/ 3662 batches | ms/batch 117.68 | loss 2.6315 | ppl    13.89\n",
      "| epoch   3 |   350/ 3662 batches | ms/batch 117.44 | loss 2.6311 | ppl    13.89\n",
      "| epoch   3 |   400/ 3662 batches | ms/batch 117.70 | loss 2.6287 | ppl    13.86\n",
      "| epoch   3 |   450/ 3662 batches | ms/batch 118.12 | loss 2.6199 | ppl    13.73\n",
      "| epoch   3 |   500/ 3662 batches | ms/batch 117.70 | loss 2.6359 | ppl    13.96\n",
      "| epoch   3 |   550/ 3662 batches | ms/batch 117.60 | loss 2.6204 | ppl    13.74\n",
      "| epoch   3 |   600/ 3662 batches | ms/batch 117.62 | loss 2.6309 | ppl    13.89\n",
      "| epoch   3 |   650/ 3662 batches | ms/batch 117.83 | loss 2.6250 | ppl    13.80\n",
      "| epoch   3 |   700/ 3662 batches | ms/batch 117.55 | loss 2.6408 | ppl    14.02\n",
      "| epoch   3 |   750/ 3662 batches | ms/batch 117.61 | loss 2.6242 | ppl    13.79\n",
      "| epoch   3 |   800/ 3662 batches | ms/batch 117.78 | loss 2.6274 | ppl    13.84\n",
      "| epoch   3 |   850/ 3662 batches | ms/batch 117.77 | loss 2.6244 | ppl    13.80\n",
      "| epoch   3 |   900/ 3662 batches | ms/batch 117.61 | loss 2.6199 | ppl    13.73\n",
      "| epoch   3 |   950/ 3662 batches | ms/batch 117.63 | loss 2.6280 | ppl    13.85\n",
      "| epoch   3 |  1000/ 3662 batches | ms/batch 117.61 | loss 2.6116 | ppl    13.62\n",
      "| epoch   3 |  1050/ 3662 batches | ms/batch 117.76 | loss 2.6048 | ppl    13.53\n",
      "| epoch   3 |  1100/ 3662 batches | ms/batch 117.55 | loss 2.6262 | ppl    13.82\n",
      "| epoch   3 |  1150/ 3662 batches | ms/batch 117.64 | loss 2.6257 | ppl    13.81\n",
      "| epoch   3 |  1200/ 3662 batches | ms/batch 117.51 | loss 2.6240 | ppl    13.79\n",
      "| epoch   3 |  1250/ 3662 batches | ms/batch 117.87 | loss 2.6133 | ppl    13.64\n",
      "| epoch   3 |  1300/ 3662 batches | ms/batch 117.77 | loss 2.6168 | ppl    13.69\n",
      "| epoch   3 |  1350/ 3662 batches | ms/batch 117.77 | loss 2.6209 | ppl    13.75\n",
      "| epoch   3 |  1400/ 3662 batches | ms/batch 117.67 | loss 2.6131 | ppl    13.64\n",
      "| epoch   3 |  1450/ 3662 batches | ms/batch 117.66 | loss 2.6147 | ppl    13.66\n",
      "| epoch   3 |  1500/ 3662 batches | ms/batch 117.76 | loss 2.5978 | ppl    13.43\n",
      "| epoch   3 |  1550/ 3662 batches | ms/batch 117.74 | loss 2.6245 | ppl    13.80\n",
      "| epoch   3 |  1600/ 3662 batches | ms/batch 117.48 | loss 2.6054 | ppl    13.54\n",
      "| epoch   3 |  1650/ 3662 batches | ms/batch 117.77 | loss 2.6364 | ppl    13.96\n",
      "| epoch   3 |  1700/ 3662 batches | ms/batch 117.57 | loss 2.6214 | ppl    13.76\n",
      "| epoch   3 |  1750/ 3662 batches | ms/batch 117.78 | loss 2.6169 | ppl    13.69\n",
      "| epoch   3 |  1800/ 3662 batches | ms/batch 117.78 | loss 2.6180 | ppl    13.71\n",
      "| epoch   3 |  1850/ 3662 batches | ms/batch 117.66 | loss 2.6054 | ppl    13.54\n",
      "| epoch   3 |  1900/ 3662 batches | ms/batch 117.47 | loss 2.6192 | ppl    13.72\n",
      "| epoch   3 |  1950/ 3662 batches | ms/batch 117.88 | loss 2.6192 | ppl    13.72\n",
      "| epoch   3 |  2000/ 3662 batches | ms/batch 117.88 | loss 2.6117 | ppl    13.62\n",
      "| epoch   3 |  2050/ 3662 batches | ms/batch 117.82 | loss 2.6089 | ppl    13.58\n",
      "| epoch   3 |  2100/ 3662 batches | ms/batch 117.58 | loss 2.6220 | ppl    13.76\n",
      "| epoch   3 |  2150/ 3662 batches | ms/batch 117.57 | loss 2.5912 | ppl    13.35\n",
      "| epoch   3 |  2200/ 3662 batches | ms/batch 117.93 | loss 2.6119 | ppl    13.62\n",
      "| epoch   3 |  2250/ 3662 batches | ms/batch 117.86 | loss 2.6106 | ppl    13.61\n",
      "| epoch   3 |  2300/ 3662 batches | ms/batch 117.86 | loss 2.6342 | ppl    13.93\n",
      "| epoch   3 |  2350/ 3662 batches | ms/batch 117.40 | loss 2.6102 | ppl    13.60\n",
      "| epoch   3 |  2400/ 3662 batches | ms/batch 117.43 | loss 2.5905 | ppl    13.34\n",
      "| epoch   3 |  2450/ 3662 batches | ms/batch 117.61 | loss 2.5977 | ppl    13.43\n",
      "| epoch   3 |  2500/ 3662 batches | ms/batch 117.51 | loss 2.6069 | ppl    13.56\n",
      "| epoch   3 |  2550/ 3662 batches | ms/batch 117.60 | loss 2.6186 | ppl    13.72\n",
      "| epoch   3 |  2600/ 3662 batches | ms/batch 117.68 | loss 2.6100 | ppl    13.60\n",
      "| epoch   3 |  2650/ 3662 batches | ms/batch 117.71 | loss 2.6109 | ppl    13.61\n",
      "| epoch   3 |  2700/ 3662 batches | ms/batch 117.62 | loss 2.5905 | ppl    13.34\n",
      "| epoch   3 |  2750/ 3662 batches | ms/batch 117.76 | loss 2.6076 | ppl    13.57\n",
      "| epoch   3 |  2800/ 3662 batches | ms/batch 117.74 | loss 2.6037 | ppl    13.51\n",
      "| epoch   3 |  2850/ 3662 batches | ms/batch 117.60 | loss 2.5975 | ppl    13.43\n",
      "| epoch   3 |  2900/ 3662 batches | ms/batch 117.59 | loss 2.6132 | ppl    13.64\n",
      "| epoch   3 |  2950/ 3662 batches | ms/batch 117.75 | loss 2.5936 | ppl    13.38\n",
      "| epoch   3 |  3000/ 3662 batches | ms/batch 117.75 | loss 2.6059 | ppl    13.54\n",
      "| epoch   3 |  3050/ 3662 batches | ms/batch 117.64 | loss 2.5985 | ppl    13.44\n",
      "| epoch   3 |  3100/ 3662 batches | ms/batch 117.59 | loss 2.6059 | ppl    13.54\n",
      "| epoch   3 |  3150/ 3662 batches | ms/batch 117.83 | loss 2.5999 | ppl    13.46\n",
      "| epoch   3 |  3200/ 3662 batches | ms/batch 117.55 | loss 2.5977 | ppl    13.43\n",
      "| epoch   3 |  3250/ 3662 batches | ms/batch 117.74 | loss 2.5934 | ppl    13.38\n",
      "| epoch   3 |  3300/ 3662 batches | ms/batch 117.54 | loss 2.6007 | ppl    13.47\n",
      "| epoch   3 |  3350/ 3662 batches | ms/batch 117.88 | loss 2.6042 | ppl    13.52\n",
      "| epoch   3 |  3400/ 3662 batches | ms/batch 117.75 | loss 2.6036 | ppl    13.51\n",
      "| epoch   3 |  3450/ 3662 batches | ms/batch 117.46 | loss 2.6060 | ppl    13.54\n",
      "| epoch   3 |  3500/ 3662 batches | ms/batch 118.00 | loss 2.5861 | ppl    13.28\n",
      "| epoch   3 |  3550/ 3662 batches | ms/batch 117.96 | loss 2.5892 | ppl    13.32\n",
      "| epoch   3 |  3600/ 3662 batches | ms/batch 117.77 | loss 2.5941 | ppl    13.39\n",
      "| epoch   3 |  3650/ 3662 batches | ms/batch 117.86 | loss 2.6007 | ppl    13.47\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 448.02s | valid loss  2.48 | valid ppl    11.91\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "lr = 1e-4\n",
    "\n",
    "epochs = 3\n",
    "optimizer = optim.Adam(language_model.parameters(), lr=lr,  betas=(0.8, 0.99))\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    epoch_start_time = time.time()\n",
    "    \n",
    "    train(language_model, training_set, vocab_size, batch_size, bptt, optimizer)\n",
    "    val_loss = evaluate(language_model, validation_set, vocab_size, batch_size, bptt)\n",
    "\n",
    "    delta_t = (time.time() - epoch_start_time)\n",
    "    print('-' * 89)\n",
    "    print(f'| end of epoch {epoch:3d} | time: {delta_t:5.2f}s | valid loss {val_loss:5.2f} | valid ppl {math.exp(val_loss):8.2f}')\n",
    "    print('-' * 89)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{data_root}language_model.pt', 'wb') as f:\n",
    "    torch.save(language_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roll(tensor, shift, axis):\n",
    "    if shift == 0:\n",
    "        return tensor\n",
    "\n",
    "    if axis < 0:\n",
    "        axis += tensor.dim()\n",
    "\n",
    "    dim_size = tensor.size(axis)\n",
    "    after_start = dim_size - shift\n",
    "    if shift < 0:\n",
    "        after_start = -shift\n",
    "        shift = dim_size - abs(shift)\n",
    "\n",
    "    before = tensor.narrow(axis, 0, dim_size - shift)\n",
    "    after = tensor.narrow(axis, after_start, shift)\n",
    "    return torch.cat([after, before], axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". \n",
      " xsos a man is sitting on a bench . \n",
      " xsos a man is sitting on a bench . \n",
      " xsos a man is sitting on a bench "
     ]
    }
   ],
   "source": [
    "sm = nn.LogSoftmax(0)\n",
    "test_batch_size = 1\n",
    "sentence = torch.LongTensor(np.ones((30,1))).cuda()\n",
    "sentence[-3] = stoi[\"xsos\"]\n",
    "sentence[-2] = stoi[\"a\"]\n",
    "sentence[-1] = stoi[\"ball\"]\n",
    "\n",
    "sentence = Variable(sentence)\n",
    "\n",
    "result, raw_outputs, outputs = language_model(sentence)\n",
    "for i in range(30):\n",
    "    result, raw_outputs, outputs = language_model(sentence)\n",
    "    out = sm(result[29]).data.cpu().numpy()\n",
    "    word = np.argmax(out)\n",
    "    print(itos[word], end=\" \")\n",
    "    sentence = roll(sentence, -1, 0)\n",
    "    sentence[29] = torch.LongTensor([word.tolist()]).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "_draft": {
   "nbviewer_url": "https://gist.github.com/0dd0df21cf404cf2bb51d0148c8b7d8b"
  },
  "gist": {
   "data": {
    "description": "fastai.text imdb example",
    "public": true
   },
   "id": "0dd0df21cf404cf2bb51d0148c8b7d8b"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "86px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
